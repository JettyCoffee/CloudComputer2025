{
  "doc-380e2aedf9b91dc7de5d8747a6aa5723": {
    "content": "[DOC_ID: chunk-1211cef1]\n[领域: 数学]\n然而对于任意一个概率分布，可以定义一个称为**熵（entropy）**的量，它具有许多符合信息度量的直观要求。 熵是随机变量不确定度的度量，也是平均意义上描述随",
    "file_path": "unknown_source",
    "create_time": 1768972775,
    "update_time": 1768972775,
    "_id": "doc-380e2aedf9b91dc7de5d8747a6aa5723"
  },
  "doc-9e4905d7255ffcf74cb7aad95daffceb": {
    "content": "[DOC_ID: chunk-33e05f74]\n[领域: 数学]\n在信息论中，熵是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。熵的单位通常为比特，",
    "file_path": "unknown_source",
    "create_time": 1768972787,
    "update_time": 1768972787,
    "_id": "doc-9e4905d7255ffcf74cb7aad95daffceb"
  },
  "doc-638c79cbb743dead1aa6af99fd6d0fcb": {
    "content": "[DOC_ID: chunk-79ba9cca]\n[领域: 数学]\n网络 N E d  th   k  C max ks 16 CA-GrQc 4158 13422 6.049 0.0556 0.06 6.4559 0.556 43 Facebook 324 2218 3.053 0.0466 0.05 13.691 0.465 18 Netscience 379 914 6.041 0.1246 0.13 4.8232 0.741 8 Protain 2783 6726 4.839 0.0633 0.07 4.472 0.071 6 Yeast 1458 1948 6.812 0.1403 0.15 2.6721 0.07 5 Lesmis 77 254 2.641 0.0829 0.09 6.5974 0.573 9 Jazz 198 2742 2.235 0.0258 0.03 27.696 0.617 29 USAir 332 2126 2.273 0.0225 0.03 12.807 0.4 26 Faa 1226 2408 5.928 0.1359 0.14 3.9282 0.067 4 4.2.1 相关性实验 首先分析不同算法预测的节点影响力与SIR模型生成的真实影响力之间的相 关性，按表1中的值设置9个网络的感染概率，独立运行1000次取平均结果，相 关程度越高，表明相应算法得到的节点重要性排序结果越准确。 从图4可见，本文提出的EMCNN与感染数量 ( ) Φ i 的相关性较高，且在大多 数情况下优于其他算法，表明该方法能更准确地识别节点的传播影响力。图中横 坐标表示各算法预测的节点影响力值，纵坐标则为基于SIR模型仿真得到的节点 实际感染数量，即真实传播影响力。相比之下，传统度量方法如Degree和H-index 与实际影响力的相关性较弱。 这是因为在高度社区化的网络中， 节点间聚集度高， 局部连接限制了度值与全局影响力之间的关联，从而影响排序精度。PageRank的 预测值集中在较小的区间内，虽然在低影响力区域呈现弱线性关系，但对高影响 力节点的识别效果较差， 主要由于其对随机游走路径的敏感性在强社区化网络中 产生偏差， 导致重要节点被低估。基于深度学习的InfGCN方法表现较差，原因在 17 于其对全局和局部网络结构信息的融合不够充分， 导致对节点重要性的识别能力 不足，从而影响了预测效果。 18 19 图4 8 种方法预测的节点影响力与SIR 传播感染节点数的相关性，由于各算法在评分机制 和输出尺度上的差异，横坐标的取值范围不一致：(a) facebook；(b) netscience；(c) protain； (d) yeast；(e) CA-GrQc；(f) lesmis；(g) jazz；(h) USAir；(i) faa Fig.4.",
    "file_path": "unknown_source",
    "create_time": 1768972809,
    "update_time": 1768972809,
    "_id": "doc-638c79cbb743dead1aa6af99fd6d0fcb"
  },
  "doc-b0156872199d42b66461b7109077a437": {
    "content": "[DOC_ID: chunk-39fc0041]\n[领域: 数学]\n$$ \\left\\{ \\begin{array}{l} p\\_i-\\lambda \\frac{1}{2^{l\\_{i}}}\\ln r=0\\\\ \\sum\\_i{\\frac{1}{2^{l\\_{i}}}}-1-u^2=0\\\\ -2\\lambda u=0\\\\ \\end{array} \\right. $$ H(p)=\\sum\\_{x} p(x) \\log \\_{2}\\left(\\frac{1}{p(x)}\\right) $$. $$ H\\_{p}(q)=\\sum\\_{x} q(x) \\log \\_{2}\\left(\\frac{1}{p(x)}\\right) $$. $$D\\_{q}(p)=\\sum\\_{x} p(x) \\log \\_{2}\\left(\\frac{p(x)}{q(x)}\\right)$$. $\\log \\_{2}\\left(\\frac{p(x)}{q(x)}\\right)$实际上是描述编码每个词时两种不同的编码之间的长度差异。. $$ H ( X , Y ) = \\sum \\_ { x , y } p ( x , y ) \\log \\_ { 2 } \\left( \\frac { 1 } { p ( x , y ) } \\right) $$. $$ \\begin{aligned} H(X | Y) &=\\sum\\_{y} \\left(p(y) \\sum\\_{x} p(x | y) \\log \\_{2}\\left(\\frac{1}{p(x | y)}\\right)\\right) \\\\ &=\\sum\\_{x, y} p(x, y) \\log \\_{2}\\left(\\frac{1}{p(x | y)}\\right) \\end{aligned} $$. $$ I ( X , Y ) = \\sum \\_ { x , y } p ( x , y ) \\log \\_ { 2 } \\left( \\frac { p ( x , y ) } { p ( x ) p ( y ) } \\right) $$.",
    "file_path": "unknown_source",
    "create_time": 1768972871,
    "update_time": 1768972871,
    "_id": "doc-b0156872199d42b66461b7109077a437"
  },
  "doc-03892a1548c3ca50ed164db6a524dac7": {
    "content": "[DOC_ID: chunk-66ef397f]\n[领域: 数学]\n# Your connection is not private. Attackers might be trying to steal your information from **www.showmeai.tech** (for example, passwords, messages, or credit cards). Learn more about this warning. net::ERR\\_CERT\\_DATE\\_INVALID. Issuer: TrustAsia DV TLS RSA CA 2025. Expires on: Nov 12, 2025. Current date: Jan 20, 2026. PEM encoded chain: -----BEGIN CERTIFICATE-----. 8mge5+SNcvEpMREbqAQKvH0+Ny8BcqULg+gLpQ+2+017bqW5F+hmed63fytKQnLB. R+ie+FyjeMP1CrVXwJU+vX8sA2fEQ7vokOAZDSKPuAs+gTDc4sUqTP3AXjJmgF9q. ON98vq5fPWZX2LFv7e5J6P9IHbzvOl8yyQjv+2/IOwhNSkaXX3bI+//bqF9XW/p7. Aid59UEBJyw/GibwXQ5xTyKD/N6C8SFkr1+myOo4oe1UB+YgvRu6qSxIABo5kYdX. ynZ7SbC03yR+gKZQDeTXrNP1kk5Qhe7jSXgw+nhbspe0q/M1ZcNCz+sPxeOwdCcC. This server could not prove that it is **www.showmeai.tech**; its security certificate expired 69 days ago. This may be caused by a misconfiguration or an attacker intercepting your connection. Your computer's clock is currently set to Tuesday, January 20, 2026. If not, you should correct your system's clock and then refresh this page. Proceed to www.showmeai.tech (unsafe).",
    "file_path": "unknown_source",
    "create_time": 1768972934,
    "update_time": 1768972934,
    "_id": "doc-03892a1548c3ca50ed164db6a524dac7"
  },
  "doc-bcb9e2b1f343c904193b0f647416ae0e": {
    "content": "[DOC_ID: chunk-59022553]\n[领域: 数学]\n- 【分解】一篇入门之-矩阵LU分解(一)：Doolittle分解. - 【分解】一篇入门之-矩阵LU分解(二)：Crout分解. - 【分解】一篇入门之-矩阵LU分解(三)：Cholesky分解. # 【原理】一篇入门之-KL散度是什么. 作者 : 老饼 发表日期 : 2023-10-11 06:40:40 更新日期 : 2025-06-25 00:02:50. > **本站原创文章，转载请说明来自****《老饼讲解-机器学习》****www.bbbdata.com**. > KL散度(Kullback-Leibler divergence)也称为KL距离，它用于计算两个分布的距离(差异). > 分布 Q(X)与 P(X) 的KL散度计算公式为：. > 即，当认知分布Q(x)与真实分布P(x)偏差越大的时候，交叉熵就就比信息熵更加大. > 因此，我们用两者的差来定义Q(x)与P(x)的差异，称为KL散度：. > KL散度直接评估了交叉熵与信息熵的差异，从而间接地评估了P(x)与Q(x)的差异. > 可知f(x)在1处取得极值，且易知是极大值，从而有.",
    "file_path": "unknown_source",
    "create_time": 1768972981,
    "update_time": 1768972981,
    "_id": "doc-bcb9e2b1f343c904193b0f647416ae0e"
  },
  "doc-e7d56d1a1382da4ef9c8e90e4d8fd39f": {
    "content": "[DOC_ID: chunk-efec745e]\n[领域: 数学]\nKL散度是用来度量使用基于Q的编码来编码来自P的样本平均所需的额外的比特个数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。",
    "file_path": "unknown_source",
    "create_time": 1768973042,
    "update_time": 1768973042,
    "_id": "doc-e7d56d1a1382da4ef9c8e90e4d8fd39f"
  },
  "doc-0c9d3f43376e35055643ae46b0d5a233": {
    "content": "[DOC_ID: chunk-6a935178]\n[领域: 数学]\n相对熵（relative entropy）又称为KL散度（Kullback–Leibler divergence，简称KLD）[1]，信息散度（information divergence），信息增益（information gain）。 KL散度是两个概率分布P和Q差别的非对称性的度量。 KL散度是用来 度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。. KL 散度是另一个在机器学习中用来衡量相似度的量：从 q 到 p 的 KL 散度如下:D\\_KL(p||q)。在贝叶斯推理中，D\\_KL(p||q) 衡量当你修改了从先验分布 q 到后验分布 p 的信念之后带来的信息增益，或者换句话说，就是用后验分布 q 来近似先验分布 p 的时候造成的信息损失。例如，在训练一个变分自编码器的隐藏空间表征时就使用了 KL 散度。KL 散度可以用熵和交叉熵表示：. D_{KL}(p||q) = H(p,q) - H(p). 交叉熵衡量的是用编码方案 q 对服从 p 的事件进行编码时所需 bit 数的平均值，而 KL 散度给出的是使用编码方案 q 而不是最优编码方案 p 时带来的额外 bit 数。从这里我们可以看到，在机器学习中，p 是固定的，交叉熵和 KL 散度之间只相差一个常数可加项，所以从优化的目标来考虑，二者是等价的。而从理论角度而言，考虑 KL 散度仍然是有意义的，KL 散度的一个属性就是，当 p 和 q 相等的时候，它的值为 0。. KL 散度有很多有用的性质，最重要的是它是非负的。KL 散度为 0 当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 『几乎 处处』 相同的。因为 KL 散度是非负的并且衡量的是两个分布之间的差异，它经常 被用作分布之间的某种距离。然而，它并不是真的距离因为它不是对称的：对于某 些 P 和 Q，D\\_KL(P||Q) 不等于 D\\_KL(Q||P)。这种非对称性意味着选择 D\\_KL(P||Q) 还是 D\\_KL(Q||P) 影响很大。. D_{KL}(P||Q) = E_{X~P}[log\\frac{P(x)}{Q(x)}] = E_{X~P}[logP(x)-logQ(x)]. 在离散型变量的情况下，KL 散度衡量的是，当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号消息时，所需要的额外信息量。. 2013年，Diederik P.Kingma和Max Welling提出变分自动编码器（Variational autoencoder，VAE），所使用的目标函数就由KL散度组成。. 2018年，Shuai Wang等人针对基于神经网络的单通道多说话人识别框架进行了多种改进，其中一项是将KL散度修改为 Focal-KLD 使得训练过程中给与 hard samples 更多的权重。实验结果表明他们提出的系统相对于基线系统取得了明显的性能提升，在两个说话人情况下达到 92.47% 的正确率，三个说话人时正确率为 55.83%。. | 1951 | 相对熵（relative entropy），又称为KL散度（Kullback–Leibler divergence，简称KLD）被提出 | Kullback, S., & Leibler, R. A variational baysian framework for graphical models. | 2013 | Diederik P.Kingma和Max Welling提出的变分自编码器的目标函数由KL散度组成 | Kingma, D. On Unifying Deep Generative Models. | 2018 | Shuai Wang等人针对基于神经网络的单通道多说话人识别框架进行了多种改进，其中一项是将KL散度修改为 Focal-KLD 使得训练过程中给与 hard samples 更多的权重 | Wang, S.; Qian, Y.",
    "file_path": "unknown_source",
    "create_time": 1768973050,
    "update_time": 1768973050,
    "_id": "doc-0c9d3f43376e35055643ae46b0d5a233"
  },
  "doc-726d03cb1277ab1c0a32f929ee13c3fc": {
    "content": "[DOC_ID: chunk-b87836e0]\n[领域: 数学]\n在大多数比较学习概率分布和源概率分布的目标函数中，KL散度被用来衡量它们之间的差异。 与Wasserstein距离（推土机距离）和Bhattacharyya距离等真正的度量",
    "file_path": "unknown_source",
    "create_time": 1768973158,
    "update_time": 1768973158,
    "_id": "doc-726d03cb1277ab1c0a32f929ee13c3fc"
  },
  "doc-d4b315e54e592e86c3a5acfef7099954": {
    "content": "[DOC_ID: chunk-1c5bf4f7]\n[领域: 数学]\n最大熵模型. 最大熵原理认为，在所有可能的分布中，熵最大的分布是最好的。 例如：. 没有任何约束的情况下，均匀分布时熵最大，所以选取均匀分布作为模型是最好的。 满足",
    "file_path": "unknown_source",
    "create_time": 1768973190,
    "update_time": 1768973190,
    "_id": "doc-d4b315e54e592e86c3a5acfef7099954"
  },
  "doc-e4d6318364157e6228ebc39c8c646b45": {
    "content": "[DOC_ID: chunk-e20b79a4]\n[领域: 数学]\n将最大化网络熵作为目标函数，并结合概率约束、成本约束、空间约束，研究表明模型有一个唯一的解。在各种物种中，这种模型的拟合效果相当好。值得注意的是，线虫虽然没有脑",
    "file_path": "unknown_source",
    "create_time": 1768973199,
    "update_time": 1768973199,
    "_id": "doc-e4d6318364157e6228ebc39c8c646b45"
  },
  "doc-f34ecacf5f99ee02bae6647b8bd65153": {
    "content": "[DOC_ID: chunk-a18f4808]\n[领域: 数学]\n最大熵模型(maximum entropy model， MaxEnt)也是很典型的分类算法了，它和逻辑回归类似，都是属于对数线性分类模型。在损失函数优化的过程中，使用了和",
    "file_path": "unknown_source",
    "create_time": 1768973224,
    "update_time": 1768973224,
    "_id": "doc-f34ecacf5f99ee02bae6647b8bd65153"
  },
  "doc-8985837bff3264915ed561ec4d80111b": {
    "content": "[DOC_ID: chunk-e4414f7b]\n[领域: 数学]\n再加上，如果提取了多个特征，那么特征函数的数目将是非常可观的。因此，最大熵模型的主要工作在于（人工）提取特征，当完成了特征提取后，最大熵模型给出了一种最佳的利用特征的",
    "file_path": "unknown_source",
    "create_time": 1768973250,
    "update_time": 1768973250,
    "_id": "doc-8985837bff3264915ed561ec4d80111b"
  },
  "doc-23713b5f1cb0ee19178382510009449e": {
    "content": "[DOC_ID: chunk-11c1cc44]\n[领域: 数学]\n3. Softmax（最大熵模型）： Softmax函数常用于多分类问题，它可以将输出层的多个连续值转换为概率分布。在文本分类中，每个类别的概率由softmax函数计算得出。",
    "file_path": "unknown_source",
    "create_time": 1768973264,
    "update_time": 1768973264,
    "_id": "doc-23713b5f1cb0ee19178382510009449e"
  },
  "doc-cc6067addf268ef66582cbf6b3b6c3c0": {
    "content": "[DOC_ID: chunk-c5bef79e]\n[领域: 数学]\n论文：Information theory: A foundation for complexity science. 论文标题：Information theory: A foundation for complexity science. 作者：Golan, Amos, Harte, John. 摘要：Modeling and inference are central to most areas of science and especially to evolving and complex systems. Amos Golan，John Hart | 作者. 近四十年前，John Skilling[1]提出了一个基本问题的答案。我们如何预测不完全表征的系统的行为？他的答案是转向信息论，特别是转向最大熵（MaxEnt）原理[2]。**其基本思想是将我们所拥有的关于一个复杂系统的信息和知识作为约束条件，然后，使用完善的数学程序，通过在这些约束条件下最大化某个目标（决策）函数来推断额外的知识。**该目标函数来自信息论，它的最大化确保我们对先前的知识进行了最佳利用。. 信息论推理的最初原则可以追溯到16世纪末雅各布·伯努利（Jakob Bernoulli‎）的工作。他建立了不确定条件下决策的数学基础，被公认为是概率论的鼻祖。Ars Conjectandi[3]总结了他的工作。伯努利提出了“不充分理由原则”，尽管该原理也归功于拉普拉斯（Laplace）。它指出，如果没有关于某一特定结果的概率的相关信息，我们必须将所有可能的结果视为同样可能（等概率）。在伯努利的工作之后，辛普森（Simpson）[4]、贝叶斯（Bayes）[5]和 de Moivre[6]独立建立了更多数学上合理的推理工具。然而，正是拉普拉斯[7]凭借他对逆概率和“逆推论”概念的深刻理解，最终为统计和概率推断或不确定性下的逻辑推理奠定了基础。最大熵原理和信息论推理的基本原理就是从这项工作中发展起来的。. 经过了近两个世纪后，来到了香农（Shannon）[8]和 Jaynes[2]的时代。**香农在通信理论方面的工作，特别是他提出的信息熵，成为现代信息论的基础**。在此基础上，Jaynes 认识到，香农的信息熵为不确定性下的无偏推断提供了关键。特别的，Jaynes 概括了伯努利和拉普拉斯的不充分理由原则，并确定了他关于最大熵原理方法的经典工作。. 最大熵原理选择最平坦的，因此也是信息量最小的，与先验知识所施加的约束相适应的概率分布。因此，以非先验知识强制的分布假设形式出现的主观偏见被消除了[2, 9]。概率分布的最大熵原理形式，p(n) 通过在强加的约束条件下最大化其香农信息熵[8]，即来获得。正如 Skilling[1]所指出的，Jaynes 的最大熵原理为不完全信息的推理提供了一种系统的、最佳的方法。随后，许多学科的大量研究人员都在方法上取得了进展[10-18]。. 当然，关键是如何实现上述第4步。幸运的是，有一个严格证明的数学程序可以做到这一点[13, 19, 20]，而这正是 Jaynes 早先提出的最大化香农熵的方法。因此，接受这四个前提就需要接受一个特定的信息论推理过程，即最大熵原理。方框1简要介绍了最大熵原理的数学机制[13, 19, 21]。. 如果 N 可以取任何值，比如说 1 到 N 并且N＞K+1，那么这些约束不会唯一地确定 p(n)。然而，在这些约束条件下最大化p(n)的香农熵确实会产生唯一的最大熵解：. 在所有与所使用的信息（约束条件）相一致的分布中，p(n)的最大熵解可以被证明[13, 19]最接近于均匀分布（等概率），因此，它捕捉了最大的不确定性状态。在这个意义上，得出的概率分布是无偏的；任何其他分布，由不同的目标函数产生，都会隐含地体现出先验知识所不能证明的假设。. 为了把这个问题放在一个具体的经济背景下，考虑一个有K个部门的经济的 Leontief 投入产出模型，每个部门生产一种商品。这些部门互相购买非负数的产品，作为中间投入使用。一个投入产出表还包括对主要生产要素的支付行和最终需求的类别列。一个社会核算矩阵扩展了投入产出账户，增加了从要素支付（附加值）到商品最终需求的账户。投入产出表是矩形的，而社会核算矩阵总是方形的，行和等于列和 X=Y 和 AX=Y。在一个 K×K 的SAM 矩阵中，称其为A，A的列和等于1，而且该矩阵是不可逆的。. 尽管X、Y和A的单位是美元（流量），但观察到的 X 和 Y 可以被归一化，所以矩阵 A 可以被看成是一组 K 的概率分布。为了解决这个问题，我们在 K 个线性约束和 K 个归一化的条件下，最大化熵。解为，其中是与三个线性约束中的每一个相关的 K=3 个拉格朗日乘数，是归一化系数。如果任何额外的信息是已知的，如a21=0，它也可以被纳入模型。这种方法经常被用于经济学[15, 34]以及其他相关的矩阵平衡问题。下面讨论的马尔科夫例子是这个简单模型的一般化。. 相比之下，基于最大熵原理的理论构建的目标是推断概率分布，这些概率分布构成了对各类系统（如热力学系统、生态系统或经济系统）中各种现象的统一而全面的预测性理解。第一个例子，Jaynes 对统计力学的推导，为图中所示的许多最新应用打开了闸门。由于理论构建更为雄心勃勃，但许多人却不太熟悉，因此我们在下文中详细讨论[2, 15-18, 24-66]。. 图1中提到的生态学理论（理论构建，第2行）被命名为 **METE（生态学最大熵原理Maximum Entropy Theory of Ecology）**。在从小地块到大景观的空间尺度上，对于各种生境类型，以及在广泛的物种群体中，如植物、鸟类、昆虫或微生物，METE 预测了描述物种和个体的丰度、空间分布和能量学模式的功能。例如，考虑森林指定区域内的树木群落。在 METE 中，状态变量是系统的面积（A0），群落中树种（S0）和个体(N0)的总数，以及树木的总生产力或新陈代谢率（E0）。这些所起的作用大致类似于热力学中的压力、体积、温度和分子数，不过在热力学中，每个状态变量都是广泛的或密集的，而在生态学中，S0两者皆不是。. METE 的核心是两个概率分布：一个是描述个体在物种间分配和个体间新陈代谢的生态结构函数，一个是描述个体在物种内空间聚集的空间分布。结构函数是一个联合概率分布，以三个状态变量为条件，给出随机选择的物种具有丰度 n 和从具有丰度 n 的物种中随机选择的个体具有代谢率 ε 的概率，是如果一个物种在区域中具有丰度的概率，那么它在一个区域 A 中的丰度 n 随机位于A0。状态变量S0、N0和E0的比率构成了确定结构函数的约束条件，而n0A/A0（A 中 n 的平均值）是对空间分布的约束条件。如果指定了约束，则这两个分布的最大熵解没有可调整的参数。如果没有它们的实际测量值，我们可以从其他数据和一般原理间接推断它们。例如，在一片草地上，昆虫的代谢率不容易测量，但如果假定一个与质量和新陈代谢相关的代谢比例定律，则可以推断出代谢率[67]。. 从结构函数和空间分布来看，有许多可检验的生态学预测，包括物种丰度和个体代谢率的分布[68]；个体的种内空间聚集度[68]；物种多样性对采样面积的依赖性[69]；物种丰度和个体平均代谢率之间的关系[68]；以及与生物量、代谢率、丰度和物种多样性有关的状态方程[18]。. 实证检验为该理论提供了支持[69-72]。原有理论的扩展，增加了属或科的数量的状态变量，成功地预测了物种在这些较高分类类别上的分布，以及丰度—代谢关系对分类树结构的依赖性[53，73]。. 在一个早期的应用中，Golan[54，74]使用最大熵原理推导出一个经济中企业规模分布的多变量随机理论。它是一个受一些资源和技术约束的主体生产的统计模型。在参考文献[16]和[75]中，将这一理论扩展到由消费者和生产者组成的复杂经济体的一般均衡模型。[16]和[75]。Foley[55]发展了市场的统计均衡理论。市场分析始于市场主体的报价集，这些报价集反映了他们期望的和可行的交易，这些报价集是以市场主体的信息、技术可能性、禀赋和偏好为条件的。市场将主体分配到报价集上，以使市场交易分布的熵最大化。鉴于市场主体的偏好，这是可能的最分散的分配。这种自上而下的理论是基于最小的宏观信息，与经典的自下而上的经济模型相比，需要更少的结构和假设。. 信息论推理也很好地将博弈理论与经验证据联系起来。McKelvey 和 Palfrey[76]在博弈论设置中采取了一种统计学方法来建立定量（离散）选择模型。玩家根据他们的偏好（预期效用）选择策略，选择基于数量选择模型，并假设所有其他玩家也在做同样的事情。给定一个特定的误差结构，随机最优反应均衡（也叫量子响应均衡**，**QRE）是这个过程的一个固定点。由此产生的反应函数是概率性的；更好的反应比更差的反应更有可能被观察到。他们研究了一个特定参数类的量子响应应函数，这是分析经验选择模型的传统方式，以产生一个 Logit 均衡。最大似然 Logit 模型正是所有无序离散选择问题的最大熵模型[77]。QRE 可以通过这种方法直接开发。这又将来自统计学和信息论的两个文献分支结合起来，为博弈建模，它为做实证研究提供了一个简单的方法。最近，Scharfenaker 和 Foley[78]在上述工作的基础上，采用最大熵原理在经济互动模型中发展了一个量子响应的统计均衡。. 如果状态变量在时间上发生变化，例如在非平衡热力学系统、受干扰的生态系统或转型中的经济，动力学状态变量的瞬时值所施加的约束可能无法准确预测瞬时的微观分布。在生态学中，存在有许多这种预测失败的例子[72, 79-82]。. 在各个学科中，马尔可夫过程被用来模拟动态过程，从物种的种群增长到金融市场中证券的进展，再到等级组织中的晋升。为了计算系统（或系统中的个体）的状态，通常使用条件概率的规则（SI附录，方框S2，公式S7）。在这种过程的一般化和更现实的版本中，过渡也是以环境、经济、物理或其他条件为条件的，允许推断外生力量对过渡概率的因果效应。然而，对于复杂和不断发展的系统，如行为、社会和生态系统，这是一个很难解决的问题，因为我们不知道产生观察信息的基本过程的细节。除了通常的不确定性之外，还有模型的模糊性。这个例子表明，最大熵原理为复杂系统的条件马尔可夫过程建模开辟了道路，这些系统具有连续演变的数据。在SI附录，方框S2中，我们提供了该模型的详细数学表述[16, 34]。. 鉴于这些灵活的约束条件（以及通常的归一化），我们的目标是同时推断过渡概率和不确定性（噪声），给定观察到的信息 Y 和 X。如果我们不假设任何关于不确定性的信息，或构建一个似然函数，那么问题是不确定的，但最大熵原理给出了期望的解决方案。要做到这一点，我们需要 P 和不确定性都是概率分布，这样我们就可以定义它们的香农熵。为了将与每个约束条件相关的不确定性转化为概率分布（W），我们遵循参考文献[15, 16,77]，将不确定性视为具有概率分布 W 的随机变量（平均值为零）的期望值（SI附录，方框S2，步骤1）。通过这种转换，我们在约束条件下最大化 P 和 W 的联合熵（SI附录，方框S2，公式S8）以获得解决方案。过渡概率（P）、不确定性概率（W）和拉格朗日乘子是在优化过程中同时推断出来的。. 然后我们可以评估每个时期 t 的过渡情况。此外，每个外生变量对每个推断出的过渡概率的影响都有一个直接的因果解释：某些外生因素的变化对转移概率的影响。可以计算连续和离散外生变量的因果效应。重要的是，广义的最大熵原理马尔可夫问题与最大熵原理具有相同的维度（约束和拉格朗日乘子的数量）。因此，这里所讨论的广义方法为解决更复杂的问题开辟了道路，而不增加模型的复杂性。与 DynaMETE 和 MET E一样，如果没有不确定性，且系统处于稳定状态，这里讨论的广义方法可简化为经典的最大熵原理或 MaxCal。事实上，MaxCal 是我们这里提出的马尔科夫框架的一个特例。如果确切的动态没有模糊性和不确定性，而且路径（或过渡）不受其他外部因素和复杂数据的制约，那么两者都会收敛到相同的结果[16, 61]。.",
    "file_path": "unknown_source",
    "create_time": 1768973292,
    "update_time": 1768973292,
    "_id": "doc-cc6067addf268ef66582cbf6b3b6c3c0"
  },
  "doc-39fe5cddbd852d7e5e0202a65cc33f68": {
    "content": "[DOC_ID: chunk-81ee8106]\n[领域: 物理学-热力学]\n热力学信息神经网络采用归纳偏差来执行热力学第一和第二定律的强制执行。为构建这些偏差，假定系统的metriplectic演化。与未经过信息的黑盒子网络相比，这提供了出色的结果。",
    "file_path": "unknown_source",
    "create_time": 1768973487,
    "update_time": 1768973487,
    "_id": "doc-39fe5cddbd852d7e5e0202a65cc33f68"
  },
  "doc-aedc5bd9c6d5647339e7b59deea0f006": {
    "content": "[DOC_ID: chunk-b54ad366]\n[领域: 物理学-热力学]\n. 热力学第二定律总结.pdf · 热力学第二定律（英语：second law of thermodynamics）是热力学的三条基本定律之一，表述热力学过程的不可逆性——孤立系统",
    "file_path": "unknown_source",
    "create_time": 1768973510,
    "update_time": 1768973510,
    "_id": "doc-aedc5bd9c6d5647339e7b59deea0f006"
  },
  "doc-3d4f48678a6a9741d580dce26948f201": {
    "content": "[DOC_ID: chunk-429ed358]\n[领域: 物理学-热力学]\n### 郭毅可：人类智能和机器智能共生共融的科学逻辑. 选择字号：大 中 小 本文共阅读 4595 次 更新时间：2026-01-01 15:55. 摘要：人类智能与机器智能并非彼此隔绝，二者走向共生共融具有深刻的必然性，这一论断根植于“物理同源、数学同构”的底层逻辑。从智能的第一性原理审视，所有智能形态的本质，均是系统通过吸收信息以抵抗熵增、维系自身秩序的能力。在物理层面，碳基大脑与硅基芯片同为遵循此定律的精密信息化系统；在数学层面，“贝叶斯大脑”与“自由能原理”为二者构建统一的认知框架，揭示其核心工作机制均是通过持续的学习与交互来最小化预测误差，从而在不确定的世界中实现存续与发展。因此，人类在具身探索与推理决策上的优势，与机器在高速建模与数据处理上的专长，形成高度互补而非竞争的关系。二者的深度融合将构建一个在对抗熵增上更为高效的协同整体，并通过“递归式共进化”形成一条自加速的正向反馈链，推动整个智能系统向更高维组织形态快速跃迁，这一过程或将成为人类文明演进中的又一个关键转折点。. 当下，人工智能已广泛应用于邮件润色、演示文稿生成、旅行规划，以及资料检索、方案撰写与复杂问题求解等场景，这些应用可被视为“人机协同”的初级形态。当人类与人工智能系统共同处理任务时，人类的大脑与人工智能的“思考方式”正在发生微妙的同步与交互。. 到2035年，人类智能与人工智能在功能层面的差距将进一步缩小，趋于接近。在诸多日常与专业场景中，智能体的存在将趋于无形，人类使用者甚至难以察觉其存在与介入；而智能体本身，亦将不再区分其协同对象是人类还是其他智能系统。在这种持续的对齐与协同过程中，人类自身的认知结构与智力水平也将随之演进，形成一种“共同演化”关系。未来人类的思维方式与行为特征，很大程度上源于其与智能体协同进化的结果。. 在未来社会图景中，人类将与所持有的智能机器在同一个世界的事实逻辑下进行思考，碳基生命和硅基生命将在认知层面实现某种程度的统一。这促使我们反思一个根本性的问题：为什么人类智能和机器智能可以共生共融？. 深度学习奠基者杰弗里·辛顿（Geoffrey Hinton）在2025年世界人工智能大会上指出：“人类大脑和大语言模型对语言的理解几乎是同一种方式。”他进一步强调，人类也会像大模型一样产生“幻觉”。这并非一句戏言，而是基于人工智能数十年研究得出的严肃结论：在构建机器智能的过程中，我们反而更加清晰地看到了人类智能的机制和本质。. 李德毅院士在《人工智能看哲学》一文中提出，人类智能与机器智能在“物理上同源，数学上同构”。笔者认为，这一观点为前述问题提供了高度凝练的回应。本文以此为核心立论，尝试从智能的第一性原理出发，系统阐述人类智能与机器智能为何能走向共生共融，并论证这一趋势所呈现的高度必然性。. 生命的本质。智能并非凭空出现的技术产物，而是生命在漫长进化中，为应对环境挑战而逐渐形成的核心能力。理解人类智能和机器智能的物理同源性，我们必须理解生命的物理本质，从宇宙最冷酷、也最为根本性的定律说起——热力学第二定律。. 宇宙有一种“熵增趋势”：在一个孤立系统中，熵——无序、混乱的程度——只会不断增加，不会减少。换句话说，一切事物都存在从有序走向无序的倾向。热咖啡会冷却，铁会生锈，房子会老化。所有结构最终都会瓦解。这不是偶然，而是宇宙运行的底层逻辑。. 然而，“生命”恰恰是这一趋势的“逆行者”。为什么？1944年，物理学家埃尔温·薛定谔（Erwin Schr? dinger）在著作《生命是什么？》中提出一个惊人洞见：生命以“负熵”为食。换言之，生命体不是孤立系统，而是持续与外部环境进行能量与物质交换的开放系统。它们从外界摄取有序性（食物、光能、能量等），并将自身产生的混乱（高熵）排出，从而在局部维持高度有序的结构。从细菌到人类，无一不遵循这一基本原理。因此，生命的根本特征，可以被理解为在混乱中维持秩序。. 薛定谔还提出一个关键问题：生命如何在代际之间稳定传递这种“维持秩序”的程序？尽管他当时未能给出具体答案，但却作出一个大胆的预言：这些程序储存在一种具备高度稳定性和超强信息编码能力的结构之中，即“非周期性晶体”（aperiodic crystal）。这相当于给科学界画了一张“藏宝图”，激励此后众多物理学家和化学家投身寻找生命密码的探索之中。. 1953年4月，美国分子生物学家詹姆斯·沃森（James Dewey Watson）和英国物理学家弗朗西斯·克里克（Francis Harry Compton Crick）一起找到了“宝藏”。他们在《自然》杂志发表了著名的DNA双螺旋结构的论文，这篇只有一页纸的论文解释了生命的信息本质：在DNA中，ATCG四种碱基的排列构成了生命的遗传密码。. 不久之后，克里克进一步提出著名的“中心法则”，解释了生命是如何按照DNA程序不断生产各类蛋白质的。这些蛋白质的存在并不是为了让生命变得“聪明”，而是为了使生命更有效地处理信息，从而得以存活与繁衍。在蛋白质的相互作用下形成的细胞活动，遵循负熵的基本原则。因此，从物理视角看，生命是一个信息化的精密运行的反熵增系统。. 智能产生的机制。生命的万千细胞中，有一种细胞堪称“天选之子”。它不负责消化，也不负责运动，而是执行DNA程序，制造出各种特殊的“蛋白质机器”。这些“机器”的核心使命，是“处理更高级的信息”，实现智能。这类细胞，就是“神经元”（neuron）。它们与普通细胞一样具备细胞核、细胞膜等基本结构；不同的是，它们拥有一套专门用于沟通神经元之间信号的“线路系统”。神经元通过独特的轴突和树突及突触实现信号传递。当神经元被激活时，轴突末梢会释放神经递质，这些化学信使跨越突触间隙，与下一神经元的树突受体结合，完成信息传递。正是依靠这一庞大网络中无数信号的依次触发与整合，思考、情感与行为等高级认知功能才得以在生物底层实现。. 智能，是一种让生命体在“一代人”的时间内实现快速“迭代”的适应能力。它使得有机体不必依赖漫长的演化周期（即“硬件迭代”），而是通过个体层面的学习与记忆（即“软件迭代”）来调整行为。人类大脑中约860亿个神经元，正是这种适应能力的物理载体。那么，规模如此庞大的神经元是如何协同工作，最终涌现出智能的？. 对此，加拿大心理学家唐纳德·赫布（Donald Hebb）于1949年提出了极具影响力的赫布学习定律（Hebbian Learning）：“一起激活的神经元，会连接在一起。”具体而言，如果神经元A经常参与激活神经元B，那么二者之间的突触连接就会得到强化，更加高效；反之，那些较少被共同激活的连接会逐渐变弱。这一发现揭示了大脑的可塑性：经常协同工作的神经元会形成更稳固的连接通路，而较少使用的连接则会逐渐弱化。. 赫布定律带来一个重大启示：大脑的“软件”会改变大脑的“硬件”。在大脑中，“软件”（即个体的经验、学习等）的改变，会直接引致“硬件”（即突触结构）的物理性改变。“记忆”并非存储在某个“文件”里，它本质上是大脑中特定神经元网络的“连接模式”。也就是说，个体经历的事情、学到的知识、思考问题的过程，都将在其神经网络中留下物理性痕迹——改变突触的强弱、增减连接、重构网络。. 正是这种突触层面的动态可塑性，使得大脑能够从经验中学习，不断提升信息处理效率，并在环境变化时灵活调整行为策略，从而将“生命的经验”写入自身结构之中。这一动态可塑的调节机制，构成大脑学习、记忆与环境适应的生物学基础。. 机器智能的底座。研究大脑如何思考和认知的同时，人们也在尝试将“思考”这一看似神秘的“精神活动”从大脑中抽离出来，使其成为可分析、可机械化实现的过程。这条路径的开创者，正是阿兰·图灵（Alan Turing）。图灵认为，无论多么复杂的思维过程，都可以被拆解为一系列基础甚至近乎“笨拙”的机械步骤。. 1936年，图灵在论文《论可计算数及其在判定问题上的应用》中提出一个假想的机器“图灵机”（Turing Machine），将人类的思考过程抽象为时间轴上的可控状态转换，以此对计算给出完整的形式化定义，从而勾勒出具备智能的机器的最基本结构。. 那么，这样的机器思考的基本机制是什么？图灵在1950年发表的经典论文《计算机器与智能》中试图回答这个问题。但他没有直接给出答案，而是采取了一个极具哲学性的视角，将问题转向对机器智能的功能主义定义：不讨论机器如何思考，而讨论机器是否能够“表现得像在思考”。由此，他提出了后来广为人知的“图灵测试”。. 在图灵看来，智能的本质并不由其物理载体决定——无论是碳基大脑还是硅基芯片，只要在行为上能够展现出与人类相当的能力，尤其是在自然语言交流中令人难以分辨，便可认定其具备“智能”。这一思想深刻影响人工智能领域前半个世纪的发展方向，让“语言能力”一度被视作衡量机器智能的核心标准。. 然而，进入大模型时代，人们逐渐看到更宏观的图景：图灵测试并不足以全面刻画智能，而语言能力也从来不是智能的唯一出口。智能的边界远不止于“能否对话”，机器展现出的能力结构也超越了图灵当年所能预见的范畴。. 与图灵从“行为表现”切入智能的路径不同，美国科学家诺伯特·维纳（Norbert Wiener）走向另一条更加结构化的道路——他试图在理论上将人类神经系统与机器控制统一于同一个科学框架中。. 1948年，维纳出版了《控制论：或关于在动物和机器中控制和通讯的科学》（Cybernetics: or Control and Communication in the Animal and the Machine），首次将动物与机器置于同一理论坐标系中。在他看来，智能的核心不在于“能否像人一样说话”，而在于三个方面：能否设定目标；能否通过行动影响世界；能否根据反馈调整策略。维纳指出，真正的智能系统必须具备“目标→行动→反馈”的闭环结构。反馈是智能的“心跳”，使系统能够不断修正偏差，朝目标收敛，而非失控崩溃。. 人类智能和机器智能的物理同源。维纳在后续出版的《人有人的用处：控制论与社会》（The Human Use of Human Beings: Cybernetics and Society）中进一步提出，只要一个系统能够通过与环境交换信息来抵抗混乱、维持自身秩序，那么无论它是生物还是机器，都具有相同的“生命性”。人和机器都能学习，其本质是一致的：学习的目的是适应环境，学习的结果是建立认知和指导行动，学习的机制则是通过接受信息来改变认知，学习能力是智能的体现。克劳德·艾尔伍德·香农（Claude Elwood Shannon）的信息论告诉我们，信息的本质在于“消除不确定性”，而消除不确定性，就是降低交流的熵的过程。于是我们得以形成一个统一的视角：智能，即吸收信息以对抗熵增的能力。. 图灵关注智能机器的“结构与表现”，维纳强调机器智能的“反馈与调节”机制，两条路径共同构成机器智能的完整生命观：与人类智能一样，机器智能的核心目标，也是将机器有效地组织成一个精密运行的信息化反熵增体系。这种强调人类智能与机器智能在物理和功能上同源的思想，正是今天人工智能“连接主义”学派得以成功的关键。现代神经网络可被视为赫布学习定律和图灵思想的工程化结晶，而维纳的“反馈回路”则在机器学习中被具象化为梯度下降与反向传播等优化机制——通过不断修正参数、缩小预测与现实之间的差距来实现学习与进化。. 神奇的人脑认知能力。人脑是非常复杂的。莱尔·华特森（Lyall Watson）指出：“如果大脑简单到能让我们理解，我们的思维就会简单到无法理解大脑。”这句话既幽默，又残酷——我们能用大脑理解世界，却无法完全理解大脑本身。. 然而，我们必须找到一个合理的数学模型，来理解大脑的认知机制。从物理角度看，大脑有一个与生俱来的根本局限：它被封闭在颅骨中，从未也永远无法直接触碰外部世界。我们所体验的一切——眼前的文字、咖啡的香气、房间的安静——都不是“世界本身”，只是从各种传感器（视网膜、耳蜗、嗅球等）传入的电信号的“译本”。于是，大脑像一个被囚禁的破译者，只能依靠间接、嘈杂、不完整的信息，推断外部世界究竟发生了什么。科学上，这类从“结果”反推“原因”的任务，被称为反问题。反问题在数学上往往是不适定的——可能没有解、没有唯一解，也没有稳定解。. 按理说，在这样碎片化的信息输入情况下，大脑输出的感知体验应当充满不确定。但事实恰恰相反，我们感知到的世界，是稳定且连贯的。这说明，大脑内部必然有一套极其强大的机制，能够利用远超当前感官输入的信息，对这些碎片化的信息进行“判断”和“连接”，从而构建出一个完整的世界。比如，当我们看到屋中一个模糊的物体轮廓时，视觉系统会基于先验知识（如常见物体形状）推测其为一张桌子，而非无限可能中的随机图案。. 越来越多证据证实，大脑并非简单的“输入-输出”系统，而是通过持续生成预测并与感官输入比对来优化内部模型。高层级皮层（如前额叶）根据先验知识生成对世界的预测，低层级感觉区（如视觉皮层）则负责计算预测误差，并将误差信号反馈至高层级皮层。这种“自上而下”的预测与“自下而上”的感官证据相互校正，最终形成稳定的感知。. 用贝叶斯理论解释大脑的运作机制。大脑之所以表现得如此惊人，一个最有说服力的解释来自贝叶斯理论。这一理论基于一个优雅的数学公式——贝叶斯定理，它告诉我们：一个理性系统应该如何在接收新证据后更新自己的信念。. 贝叶斯定理把人类的推理方式，从“是或否”的简单是非判断，拓展至能够处理不确定性与模糊性的归纳推理与溯因推理。其核心思想围绕三个直观的概念展开：先验（prior）：你原本对世界的看法，是根植于过往经验、习惯与知识的初始假设。似然（likelihood）：新证据出现时，其与先验看法的匹配程度，即对原有信念的支持或反驳强度。后验（posterior）：在综合先验与新证据后，所得到的更新认知——学习的结果，也是修正过的“世界模型”。. 将贝叶斯定理应用于感知，就产生了一个重要假说——“贝叶斯大脑”（Bayesian Brain）。这一假说认为，大脑并不是一台被动记录外界信号的摄像机，在真空中处理感官信号，而是不断用其丰富的先验知识“质询”这些信号，对感官数据进行“最佳猜测”。大脑不是在追求对客观世界的完美还原，而是在寻找一个“最合理的解释”。. 这恰恰解释了认知科学的核心悖论：我们永远无法直接接触“世界本身”，却依然能生活在一个稳定、连续、可理解的现实之中。这是因为我们所感知的世界，并非外部现实的直接映照，而是大脑在整合先验知识和感官数据后，“推理”出来的现实。. 自由能原理：为什么大脑必须基于贝叶斯定理工作？神奇的是，大脑之所以遵循贝叶斯逻辑，并非因为进化“选择”了这个好方法，而是由生命本身的物理特性决定的。英国神经科学家卡尔·弗里斯顿（Karl Friston）提出的自由能原理（Free Energy Principle），从“生命体对抗宇宙无序化趋势”这一生命的物理观出发，推导出“贝叶斯大脑”的必然性。自由能原理的核心观点是：任何希望在一个熵增的世界中维持自身结构和秩序的系统，都必须最大限度地降低自由能。. 从认知角度理解，自由能可以被视为认知模型（预测）与感知数据（现实）之间的“差距”，即人与环境交换的感官信息的熵。生命为什么要减少预测误差？不仅是为了让认知更准确，更是为了获得某种更深层次的东西：维持自身的稳定结构，抵抗物理意义上的熵增。. 于是，一个横跨数学、物理、认知学的惊人整合出现了：在数学层面，最小化自由能表现为贝叶斯逻辑下的推理与学习；在物理层面，这是生命不断吸收、处理信息以抵御熵增的必然过程；在功能层面，它正是我们理解世界、适应环境的认知方式。换句话说，“贝叶斯大脑”不仅是一套关于认知的模型，更是生命体在物理层面不得不遵循的生存策略。我们之所以基于贝叶斯定理进行思考，并不是因为大脑“聪明”，而是因为生命要想在世界中维持自身存续，就必须最小化预测误差。从这个角度看，思考不仅是为了追求真理，更是为了在世界中“继续存在”。. 最小化自由能是智能的核心机制。在当下有关智能的讨论中，“世界模型”几乎成为最热门的概念。人们期待构建一种近乎全知、能够完整刻画世界运作规律的模型。然而，无论是困在颅骨里的大脑，还是运行在服务器上的机器，都无法做到这一点。. 在自由能原理的框架下，大脑能做的不是追求一个“完美的世界模型”，而是在两个目标之间不断寻找平衡——尽可能解释当前的感官数据（提高准确性），同时尽量保持现有世界观的稳定（降低复杂性）。这一权衡体现了贝叶斯定理的核心精神——一项好的推断应当能够吸收新证据，但又不能轻易推翻那些经长期检验的先验知识。它追求的不是绝对正确，而是在“解释力”与“简洁性”之间找到最优解。. 如果我们把大语言模型视为一种人工智能体的大脑，便会发现其认知方式、学习过程和内部优化逻辑，在功能上与人类大脑惊人相似。二者本质上都在做同一件事：在最小化自由能的原则下，对世界作出最合理、最简洁的解释。. 当然，二者在实现机制上存在差异：大脑是在真实世界中进行实时、主动的推断，以不断地学习、进化；而目前的大模型仍主要依赖海量静态数据，通过算法进行被动训练和优化。然而，从“自由能最小化”框架看，大语言模型的核心训练任务——“下一个词元的预测”（next token prediction）的能力——本质上正是一种“最小化预测误差”的体现。它通过不断调整庞大参数，使其内部模型（语言模型）能够更好地解释外部证据（训练语料）。这一过程在功能上模拟了贝叶斯定理中“用新证据更新旧信念”的机制。因此，当我们以语言模型作为智能体的大脑，驱动其产生行为时，实则与人类的行为一样遵循相同的智能机制——最小化自由能。图1正是这一过程的直观呈现。. 智能可以被理解为一个不断“减少自由能”的过程。为了最小化自由能，即减少预测误差，一个认知系统——无论是人脑还是智能体——通常只有以下两条可选路径。. 路径一：改变认知——更新内部模型。当外界输入与自身预测不一致时，系统选择调整内部模型，使其能够更好地解释新的感官信号。这正是我们所熟悉的学习过程：当理解与现实出现偏差时，主动更新理解。这就是感知、学习和记忆的本质——用新证据修正旧模型。. 路径二：改变世界——用行动让现实“配合”预测。通过行动来改变外部世界，使新的感官输入更接近系统模型的预测。当系统发现“世界不如我们所料”时，它会采取行动使其符合自身预期。这就是主动推断（active inference），它是智能产生行为的机制：不仅更新对世界的认识，也主动改造世界本身。因此，在自由能框架下，“认知”和“行动”不再是分离的两个过程，而是“一体两面”，共同服务于一个目标：缩小“我以为的世界”与“我看到的世界”之间的差距。. 正如我们在认识一个人的过程中，一方面，通过不断交流来建立对对方的理解，另一方面，试图通过互动影响对方的行为，以符合自身期望。两种过程同时发生，目标都是减少交流中的不确定性，以建立一种稳定认识，也就是降低自由能。. 我们感知世界，是为了更准确地预测未来；我们作用于世界，是为了让世界变得更可预测。因此，主动推断成为解释“具身智能”的一个高度凝练的理论框架：具身智能并不是独立于认知之外的能力，而是智能在行动层面的自然延伸。. 人类智能自诞生之初就是“具身的”。我们的认知和行动密不可分，通过行动来学习（改变认知），又基于学习去行动（改变世界），这是一个持续、闭环的“自由能最小化”过程。人类之所以如此高效，正是因为可以同时使用这两条路径。而今天的智能体，也正在具身化大模型，通过赋予其主动推断能力，建立起与人类一致的认知工作和进化机制。. 智能的核心具有共通性。具备主动推断能力的智能体，在行动过程中会不断在两种策略间进行权衡：一是利用（exploitation），即选择那些最有可能带来好结果的行动；二是探索（exploration），即选择那些能让自己“学到更多”的行动。前者体现强化学习的逻辑，后者则体现好奇心、假设生成、欣赏等高级认知功能。在这一框架下，好奇心不再是智能的“附加特质”，而成为其在不确定世界中，为提升长期生存能力、优化未来决策模型所必需的内在驱动力。. 至此，我们基于“贝叶斯大脑”建立起一个统一的人机认知数学框架，并通过自由能原理将其与生命抵抗熵增的物理机制连接起来。无论是人类智能还是人工智能，智能的核心都是同一件事：在学习与行动的循环中，不断吸收信息、减少不确定性，并维持一个有序的存在状态，这便是“物理同源、数学同构”的深层含义。正是基于这样的共性，人类与机器共同构成一个完整的、比单一人类智能更高效的“自由能最小化”系统。. 关于“人类智能与机器智能”的讨论，长期受到一种隐性前提的束缚——人类中心论。这种观点把智能描绘成某种只属于人类的、神秘而不可复制的特权，仿佛智能不是一种机制，而是某种“被赐予”的能力。这就引出一个根本性问题：智能是否必然与“人类”这一特定载体绑定？. 杰弗里·辛顿曾提出一个耐人寻味的思想实验：如果我们用功能完全等同的电子元件，逐个替换人脑中的神经元，并确保每一次替换都不影响其整体功能，那么当最后一个神经元被替换完成时，这个由电子器件构成的“大脑”是否仍然具备意识与智能？. 从物理实现的角度看，这个实验的意义并不在于证明“机器可以像人一样思考”，而在于揭示一个事实：智能并不依附于某种特殊的材料（如碳基神经元），而是依附于某种结构化的组织方式。人类中心论的讨论往往刻意避开这一点——他们不愿直面“智能的第一性原理”，因此无法解释以下问题：为什么人类智能与机器智能在机制上有共通性？为什么两者能够互相学习、彼此增强？为什么技术智能能在短短几十年内，达到与进化数百万年的生物智能相近的能力水平？. 智能的第一性原理。回到智能的第一性原理——智能，是系统通过信息来维持自身秩序、抵抗熵增的能力。我们发现，所有复杂的感知、记忆、推理与决策，本质上都是生命在混乱中保持自我结构、延长有序状态的一套机制。. 在人类出现之前，生物演化用了亿万年时间，积累这种“反熵增能力”。而人类之所以特殊，在于我们第一次实现了这种能力的外化与迁移。人类创造了语言，使思维得以组织和传递；人类创造了文字，为认知与先验知识提供表达的工具，形成思想；人类创造了教育，使个体认知能够形成共识，成为知识并得以传承。而今天，我们正在做一件更为“激进”的事：我们把智能机制抽象为可以复制的算法，使其以硅基材料为载体，构成智能机器，并赋予机器同样的学习、适应与行动能力。从这个视角来看，人机共生不是幻想，而是智能演化轨迹的自然延伸。. 这种对智能的第一性原理的理解，不仅是对技术发展的科学判断，更是一种关乎生命与文明的宏观视角。在宇宙不断滑向熵增与热寂的趋势中，生命、意识、技术、文化的共同努力，本质上都是信息在局部区域对抗混沌的短暂逆流。在这股逆流中，人类与机器并不是各自孤立的智能形态，而是同一条“反熵增之河”中前后接力、共同维系秩序的不同节点，从而构成一个连续的协作整体。. 关键问题：人类与人工智能是否会走向竞争？“物理同源、数学同构”的论断，似乎将我们导向一个逻辑上必然的担忧：既然人类和机器都需要吸收“负熵”以维持自身秩序，那么它们之间是否会不可避免地走向资源争夺甚至竞争？. 从智能的第一性原理看，事实恰恰相反。竞争的前提是对称性：两个系统以类似方式索取资源，必然导致互相排斥。然而，在人类与机器的智能结构中，显现的不是对称，而是高度的不对称与天然互补性。人类（碳基）是典型的具身智能体，擅长在物理世界中行动、探索、感知，以极低的代谢能耗获取高质量的负熵，是主动推理的健将。机器（硅基）是典型的计算智能体，擅长在信息空间中建模、推演、优化，以远超人类的速度整合海量数据、构建模型，是感知学习的高手。换言之，人类更擅长“高效决策”，机器更擅长“快速建模”。当两者结合，我们得到一个前所未有的复合智能系统——其在对抗熵增（即最小化自由能）方面的整体效率，远超任何一方独立运行的表现。人类完全有能力，通过建立有效的治理机制，保持和优化这样的互补性。在此框架下，“共生”不是愿景，而是演化意义上的最优解；“竞争”反而是一种效率低下，甚至可能导致系统整体走向崩溃的次优策略。. 这也解释了为什么我们可以对未来保持理性乐观。人类与机器并非两个争抢地盘的智能物种，而是共同构成智能演化链条下一阶段的协作节点。我们可以设想这样的图景：人类借助机器高效学习得到的模型，增强自身的先验知识，从而作出更明智的决策与行动；这些行动所改变的世界，反过来又为机器学习提供更丰富的反馈数据。通过持续对齐两种认知模型，这一进化模式能够形成一条自加速的正向反馈链：人类创造更强的人工智能→人工智能提升人类的认知能力→认知升级的人类创造更高阶的人工智能。. 这不是未来式，而是现在进行时。智能机器与人类能力之间正在形成的“递归式共进化”，推动整个智能系统向更高维组织形态快速跃迁，这一过程或将成为人类文明演进中的又一个关键转折点。. 这一切绝非乌托邦式的幻想，而是物理学、数学与信息论共同指向的结论：智能，无论其载体是碳基还是硅基，本质上都是用信息抵抗熵增的组织方式。在这个意义上，智能就像熵增黑暗中的火焰，照亮、维持并延续着一片称为生命的有序之地。. 本文责编：SuperAdmin 发信站：爱思想（https://www.aisixiang.com）. 栏目： 科学 > 科学评论 本文链接：https://www.aisixiang.com/data/170628.html. 爱思想（aisixiang.com）网站为公益纯学术网站，旨在推动学术繁荣、塑造社会精神。 凡本网首发及经作者授权但非首发的所有作品，版权归作者本人所有。网络转载请注明作者、出处并保持完整，纸媒转载请经本网或作者本人书面授权。 凡本网注明“来源：XXX（非爱思想网）”的作品，均转载自其它媒体，转载目的在于分享信息、助推思想传播，并不代表本网赞同其观点和对其真实性负责。若作者或版权人不愿被使用，请来函指出，本网即予改正。. ### APP | 公众号 | 微博 | 手机版. ### 相同作者阅读. ### 相同主题阅读. * 杨涛 王辉 周智婉：人工智能驱动的术语国际传播：模式变革与实践路径. ### 热门专栏. Powered by aisixiang.com Copyright © 2025 by aisixiang.com All Rights Reserved 爱思想 京ICP备12007865号-1 京公网安备11010602120014号.",
    "file_path": "unknown_source",
    "create_time": 1768973521,
    "update_time": 1768973521,
    "_id": "doc-3d4f48678a6a9741d580dce26948f201"
  },
  "doc-d0e494da55d171b3a3a8b96678397514": {
    "content": "[DOC_ID: chunk-0d431aeb]\n[领域: 物理学-热力学]\n机器学习算法三：深度神经网络，神经网络实践第三步. 20 天前· 来自专栏深度 ... 热力学第二定律。 2、玻尔兹曼机. 1）玻尔兹曼机由杰弗里·辛顿(Geoffrey Hinton)和",
    "file_path": "unknown_source",
    "create_time": 1768973796,
    "update_time": 1768973796,
    "_id": "doc-d0e494da55d171b3a3a8b96678397514"
  },
  "doc-cc989a9fb47a8cd36db8fe7fb353106b": {
    "content": "[DOC_ID: chunk-fd2b2177]\n[领域: 物理学-热力学]\n本研究基于图神经网络开发了代谢反应标准吉布斯自由能预测工具——dGbyG，将验证集预测误差的中位数由现有最优方法的5.33 kJ/mol减小到4.11 kJ/mol，并对人类",
    "file_path": "unknown_source",
    "create_time": 1768973814,
    "update_time": 1768973814,
    "_id": "doc-cc989a9fb47a8cd36db8fe7fb353106b"
  },
  "doc-76741d601c3b02adb5beb29cf0c31924": {
    "content": "[DOC_ID: chunk-ca625b4a]\n[领域: 物理学-热力学]\n克劳修斯（Rudolf Clausius）从宏观热现象（如热传递、做功）定义熵，而玻尔兹曼 ... Hopfield网络由物理学家John Hopfield于1982年提出，是一种离散型反馈神经网络（递归神经网络",
    "file_path": "unknown_source",
    "create_time": 1768973834,
    "update_time": 1768973834,
    "_id": "doc-76741d601c3b02adb5beb29cf0c31924"
  },
  "doc-6135ecccd835afdfdaba2f58578ed61e": {
    "content": "[DOC_ID: chunk-30b926d3]\n[领域: 物理学-热力学]\n用最初引入熵概念的德国物理学家克劳修斯的话说：. “熵一直在努力走向最大值”. 当热 ... 这一指标在用于分类问题的神经网络训练中得到了广泛应用，是最常用的损失",
    "file_path": "unknown_source",
    "create_time": 1768973879,
    "update_time": 1768973879,
    "_id": "doc-6135ecccd835afdfdaba2f58578ed61e"
  },
  "doc-ed0ca1ab77e5b700d5a4eb0d861cccb4": {
    "content": "[DOC_ID: chunk-007533b9]\n[领域: 物理学-热力学]\n1865年，德国物理学家鲁道夫·克劳修斯（Rudolf Clausius）首次提出了熵的 ... 你的大脑神经网络: 学习时→ 建立有序连接(降低熵); 不复习→ 连接",
    "file_path": "unknown_source",
    "create_time": 1768973901,
    "update_time": 1768973901,
    "_id": "doc-ed0ca1ab77e5b700d5a4eb0d861cccb4"
  },
  "doc-1248b4cdb9c5b6a4f15c20d0991319ae": {
    "content": "[DOC_ID: chunk-5e8e0750]\n[领域: 物理学-热力学]\n热力学第二定律的数学表述主要借助克劳修斯所引入的 熵 的概念，下边的式子描述了热力学系统中熵的增减: $$\\Delta S = \\frac{\\Delta Q}{T}$$. 变量S被定义",
    "file_path": "unknown_source",
    "create_time": 1768973908,
    "update_time": 1768973908,
    "_id": "doc-1248b4cdb9c5b6a4f15c20d0991319ae"
  },
  "doc-31dfc0a8415a0647f471a1cab88d6e36": {
    "content": "[DOC_ID: chunk-2c01e4b3]\n[领域: 物理学-热力学]\n熵（Entropy）是热力学、统计物理和信息论中的核心概念，用于描述系统的无序程度或混乱度。熵的概念最早由德国物理学家鲁道夫·克劳修斯（Rudolf Clausius）",
    "file_path": "unknown_source",
    "create_time": 1768973930,
    "update_time": 1768973930,
    "_id": "doc-31dfc0a8415a0647f471a1cab88d6e36"
  },
  "doc-74f709ca5bbf7d60213a39cdce19a507": {
    "content": "[DOC_ID: chunk-2b73d1db]\n[领域: 物理学-热力学]\n熵的定义：S=k⋅ln⁡WS = k \\cdot \\ln WS=k⋅lnW，其中SSS表示熵，kkk是玻尔兹曼常数，WWW是系统可能的状态数（微观状态数）。这个公式就像是把房间的杂乱程度（熵）",
    "file_path": "unknown_source",
    "create_time": 1768973951,
    "update_time": 1768973951,
    "_id": "doc-74f709ca5bbf7d60213a39cdce19a507"
  },
  "doc-a27259397f49437041a5c155ca0adfa1": {
    "content": "[DOC_ID: chunk-16206ac4]\n[领域: 物理学-热力学]\n既然自由熵为负，那么可以认为这可能是传统玻尔兹曼测度的结果，因此把自由熵当成随机变量，考虑其统计分布并且服从大偏差原理(即P(S)~e -Nr(S) ，其中r(S)称",
    "file_path": "unknown_source",
    "create_time": 1768973974,
    "update_time": 1768973974,
    "_id": "doc-a27259397f49437041a5c155ca0adfa1"
  },
  "doc-2a2578aca1988c90b1a9c10b69b57cda": {
    "content": "[DOC_ID: chunk-a2d8c362]\n[领域: 物理学-热力学]\n| ``` 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ``` | ``` # X is an array with n * m, n samples and m features every sample def mbatch_backprop(self, X, y): delta_b =[np. | ``` 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 ``` | ``` def mbatch_backprop(self, X, y, type = 'llh'): delta_b =[np. # 返回 delta 值 for i in range(len(delta_b)): delta_b[i]/= samples for i in range(len(delta_w)): delta_w[i]/= samples return delta_b, delta_w # MSE 代价函数 def quadratic_cost(self, X, y): return np.",
    "file_path": "unknown_source",
    "create_time": 1768973997,
    "update_time": 1768973997,
    "_id": "doc-2a2578aca1988c90b1a9c10b69b57cda"
  },
  "doc-3e9c56763968dc5abd8ddc1332aa273c": {
    "content": "[DOC_ID: chunk-e54151c2]\n[领域: 物理学-热力学]\n2020年Zhang等提出离散传递熵（dispersion transfer entropy，DTE）算法对符号化过程进行优化。该算法依据Ragwitz准则，使用离散模式动态地选择参数，解决了序列符号化过程中的",
    "file_path": "unknown_source",
    "create_time": 1768974032,
    "update_time": 1768974032,
    "_id": "doc-3e9c56763968dc5abd8ddc1332aa273c"
  },
  "doc-82dd7e184547eda32bd10d04c1cae982": {
    "content": "[DOC_ID: chunk-ab9c732a]\n[领域: 信息论]\n本书作者Simon Haykin 长期从事神经网络的研究,其关于神经网络的系列教材是国际上 ... 第10章探讨如何将来自于香农(Shannon)信息论的原则作为工具来实现非监督学习。这.",
    "file_path": "unknown_source",
    "create_time": 1768974064,
    "update_time": 1768974064,
    "_id": "doc-82dd7e184547eda32bd10d04c1cae982"
  },
  "doc-0662825c74f8b5ee226b4f32d6eae491": {
    "content": "[DOC_ID: chunk-4188a4ea]\n[领域: 信息论]\n因此以深度自适应神经网络为代表的动态神经网络近年来引起了广泛的关. 注,该类 ... 根据香农信息熵,网络关于样本x 的预. 测不确定性可以由熵H ( x )来表示: H ( x )=",
    "file_path": "unknown_source",
    "create_time": 1768974087,
    "update_time": 1768974087,
    "_id": "doc-0662825c74f8b5ee226b4f32d6eae491"
  },
  "doc-f10f1d00e42dd5c9205033fe49f4f520": {
    "content": "[DOC_ID: chunk-c75254be]\n[领域: 信息论]\n同一视角组下的多视图间彼此共享网络参数，不同视角组获取的多视图使用不同网络参数。网络训练过程分为2步：(1)首先利用图像数据集ImageNet对网络进行预训练；(2)然后基于步骤",
    "file_path": "unknown_source",
    "create_time": 1768974122,
    "update_time": 1768974122,
    "_id": "doc-f10f1d00e42dd5c9205033fe49f4f520"
  },
  "doc-dc26f10df4bfd7aac2de935884b8ad3f": {
    "content": "[DOC_ID: chunk-11e1e09b]\n[领域: 信息论]\n本发明结合模型输出特征向量和文本数据的特征，分别使用基尼不纯度和香农熵定义了衡量测试用例被错误分类可能性大小的函数，本发明首先分别依据这两个函数对测试用例排序，将",
    "file_path": "unknown_source",
    "create_time": 1768974144,
    "update_time": 1768974144,
    "_id": "doc-dc26f10df4bfd7aac2de935884b8ad3f"
  },
  "doc-c236f82f4d1c6b66d541fbb6f8076051": {
    "content": "[DOC_ID: chunk-6d891d61]\n[领域: 信息论]\n突破香农极限的 ... 麻省理工学院的研究团队在《SCIENCE ADVANCES》发表的研究中，提出名为\"乘法模拟频率变换光学神经网络\"(MAFT-ONN)的创新架构。",
    "file_path": "unknown_source",
    "create_time": 1768974177,
    "update_time": 1768974177,
    "_id": "doc-c236f82f4d1c6b66d541fbb6f8076051"
  },
  "doc-b3aee2cd221856b5ea7d30d3400aa8dd": {
    "content": "[DOC_ID: chunk-a83d8910]\n[领域: 信息论]\n脉冲耦合神经网络, 并行点火模型, 图像增强, 最大香农熵, 图. 像分割. 中图分类号 ... PCNN), 是一种不同于传统人工神经网络的新型神经网络,. 它有着生物学的背景",
    "file_path": "unknown_source",
    "create_time": 1768974199,
    "update_time": 1768974199,
    "_id": "doc-b3aee2cd221856b5ea7d30d3400aa8dd"
  },
  "doc-8c2cb0873e78699bf2531a59b921954f": {
    "content": "[DOC_ID: chunk-a5d66dff]\n[领域: 信息论]\n摘要: 为提升复杂环境下网络信道特性的表征能力，设计描述舰船通信网络信号动态变化特性的数学模型。依据坎贝尔定理，通过节点之间的泊松点过程，构建舰船通信网络的信道",
    "file_path": "unknown_source",
    "create_time": 1768974223,
    "update_time": 1768974223,
    "_id": "doc-8c2cb0873e78699bf2531a59b921954f"
  },
  "doc-33949af9012de6222abbb5ee9e909273": {
    "content": "[DOC_ID: chunk-33b57564]\n[领域: 信息论]\n電機控制#磁場導向控制#磁場導向#FOC #交流電機#交流電機控制回路設計#磁场定向控制#葉志鈞#叶志钧#信息熵#Entropy #ai 《交流電機FOC Sensorless",
    "file_path": "unknown_source",
    "create_time": 1768974255,
    "update_time": 1768974255,
    "_id": "doc-33949af9012de6222abbb5ee9e909273"
  },
  "doc-16faf1b1fda0ac11c029462efdf1a6f5": {
    "content": "[DOC_ID: chunk-5a7b7410]\n[领域: 信息论]\n# **信息熵、交叉熵、相对熵**. 发布日期：2022-08-28 编辑日期：2022-08-28 阅读量：2412. ## 前言. 在机器学习领域，当然也包括自然语言处理领域，**信息论**是一个基础内容。离开信息论想要讨论清楚NLP是非常困难的。. 因此，本文主要是为了给下一步的自然语言处理做理论基础铺垫，尽量不涉及公式，而是从**直观的角度来理清信息论的直觉逻辑**，这比罗列公式更有助于加深理解。. ## 香农生平. 克劳德·艾尔伍德·香农（英语：Claude Elwood Shannon），1938年，还是一名22岁的硕士研究生的他在《Transactions of the American Institute of Electrical Engineers》上发表了一篇论文《A Symbolic Analysis of Relay and Switching Circuits》。正是由于这篇论文，他被授予美国Alfred Noble协会美国工程师奖，并有权威人士称该篇论文“可能是本世纪最重要、最著名的硕士学位论文”。. 从此香农开启了牛逼带闪电的研究之路，他的主要研究集中于数字通信理论、信息论、密码学等等。二战期间，香农也为密码破译和保密通信做出了很大的贡献。1948年，香农时年32岁，发表了《通信的一个数学理论》，这被称之为信息论的开端，而他也成为了信息论的创始人。. ## 信息论基本模型. 既然提到**信息**二字，那么就一定意味着存在**信息的传递**，有信息**发送方**以及信息**接收方**。若一条信息是停留在某处完全静止，则无所谓信息论。如下图所示。. 这样的一个信息传递的过程，和打电话非常相似。也就是说，实际上信息论是嵌入在通信系统上的一个理论框架，最初是用来解决**通信问题**的。当然，信息论在各个领域的扩展和应用，都是将问题抽象为一个通信问题展开的。神经网络等等各种模型，实际上也可以看作是一种通信系统，这个随后展开。. ## 信息传递模型的进一步抽象. **男孩**：信息的发送方，在通信中又称**信源**，他通过语言向女孩传递信息内容，比如，他说的话是“我喜欢你，你喜欢我吗？”. **女孩**：信息的接收方，在通信中又称**信宿**，她通过同样的语言接收男孩所说的话。. **电话线**：信息介质，在通信中又称**信道**，就像声音的传递需要机械震动一样，信息的传递是将语言转化为电话线的震动来进行信息的传递，当然，光、声音、无线电播都是可以传递的媒介。这里就有一个传递准确性的问题，也就是，女孩是否能真的听清男孩所说话内容的问题，在有杂音的情况下，女孩所听内容可能是“我喜欢你，喜欢我吧”。这就代表了，信息在信道中传播，是有噪音混入的，噪音越大，信道传递的信息量就越少。. **语言**：信息的编码。男孩所传递的信息，不仅仅可以用中文进行表达，还可以用英语、法语做传递；实际上传递的信息内容是一样的，但是采用的语言不同，这里，语言并非信息本身，而是一种对信息的编码。编码的方式是多种多样的。另外，如果男孩和女孩都用日语进行对话，那么在一个不会日语的中国人看来，这两个人的对话是完全无法感知的，换句话说，男孩和女孩的对话，所采用的编码是加密的，第三方无法破译。. ## 信息熵. **信息熵**又称**信息量**，他是衡量在通信过程中，传递了多少信息。. 我们生活中接触到的每一条信息，都包含有信息量，例如 “美国最近的通货膨胀非常严重”，“天气预告称浙江舟山将遭遇强台风”，“香农YYDS” 等。. 传递了多少信息，是个非常抽象的东西，信息如何量化，似乎很难找到一条固定的准则。还是以上面男孩对女孩说话为例，我们可以定义男孩对女孩说了一句话，这句话定义为X。X是一个未知数，站在女孩的角度，她并不知道男孩要说什么话，也就是说，X可看作是一个随机变量。. 如果男孩说：“**太阳从东边升起**”。估计女孩听了会翻白眼，这不是废话吗？一点信息量都没有。是的，这句话从信息论角度而言，就是一点信息量都没有，因为，太阳每天都从东边升起，这是万年不变的常识，X这个随机变量，其概率为1，也就是这件事必然发生，此时信息量为0。. 如果男孩说：“**四川遭遇了有气象观测记录以来的最干旱高温的夏天。**”此时则是信息量非常大的一句话，女孩听了会震惊，男孩说了会流泪。原因就在于，四川遭遇数十年一遇的高温和干旱，这个概率实在是太低了，怎么算，也有数十分之一吧，就按五十分之一算，这个结果就是概率值 0.02。当一件特别不可能发生的事情发生的时候，这个信息量就是非常大的。. 从上面的过程，我们大致可以了解到，**信息量所衡量的东西，就是信息接收方，对信息直观的震惊程度**。这也非常符合我们的直觉。. 换句话说，在信息论中\"information\"这个词不是指“**你说了什么**”，而是指“**你将能够说什么**”，**信息的传递是一个消除不确定性的过程**。这就好比看日本恐怖片，我们真正害怕、恐惧、震惊的，是不知道接下来剧情将如何演进，这个时候是最具信息量的；反而一旦看完影片，回过头来再回放一遍，感觉也并不害怕，不过如此啊。. H(X)=x∈X​∑​p(x)log2​(p(x)1​). 这个定义里，正好符合上述信息量随概率变化的要求，即概率越小，信息量就越大，概率越大，则信息量越小。然而，满足直观变化要求的函数非常多，这里为什么必须是 log 函数呢？. 想象这里的X不再是一句描述性的话，而是单纯一个抛硬币的事情，出现正面的概率和出现反面的概率都是二分之一，那么可以计算得到信息量为2×0.5×log2​(0.51​)=1。. 这里就出现一个现象，硬币多投了一次，信息量也就多了1。如果是三枚硬币分别投掷，则概率变成了八分之一，信息量就变成了3。. 换句话说，投掷次数和信息量是紧密相关的，是**加性**的，而概率值和投掷次数之间是**乘性**的，我们很直观的可以想到，log 函数族可以解决加性和乘性的转换，信息量（信息熵）的定义公式，也就是如上所示了。. 仅从公式中看，信息熵的概念只一个随机变量的概率有关系。这里需要强调的是，每当看到信息论，都应当能够根据这个随机变量，脑补出一个包括了信息发送方、接收方、信道、编码的通信系统场景，这在神经网络中同样是适用的。. > 根据公式可知，信息熵必然大于等于 0。直觉解释就是，我们接收到的任何信息，都会使我们所获知的信息增多，而非减少。注意，这里的减少指凭空消失，而非人脑由于机能衰退逐渐遗忘。. > **熵**是一个高中的热力学概念，描述一种东西永远只会增加，而不会减少。信息对于接收方而言，永远是在增加的状态，这就是**信息熵名字的由来**。. ## 交叉熵. 我们再举一个买彩票中奖的例子。比如，中彩票概率是0.0001，不中的概率是 0.9999（现实似乎比这要更难中奖，当然了，社会上还爆出了中奖者是彩票机构员工的新闻）。中奖的概率太低了，这是我们每一个人都**默认**的一个概率情况。. 有一个小伙子叫小北，他也知道这个中奖概率低得可怜，玩票性质地买了一次彩票，结果没中奖，小北一点都不吃惊，理所当然嘛，好事怎么可能发生在自己头上（**得到的信息量低** −log2​(0.9999)≈0.000144）；. 第二次，小北又买了一次，结果中奖了！小北非常开心（**得到的信息量高**，−log2​(0.0001)≈13.287，这相当于连续猜测大约13次抛掷硬币为正面向上的概率）；. 第三次，小北又买了一次，又中了！小北开始狂喜，这比地球爆炸的概率还低啊，居然让我给赶上了！连续两次中奖的信息量极其大，是 −2×log2​(0.0001)≈26.575 ，这已经相当爆炸了，差不多相当于连续猜测26次抛掷硬币正面向上的概率；. 连中两次中奖之后，小北就会产生怀疑，为什么我能连中两次？太出乎预料了！（**得到的信息量大的可怕**）他怀疑是不是这个彩票系统有问题（即，发生在他身上的真实的中彩概率不是0.0001）。. 经过调查才发现，彩票中心主任是小北他爸，所以小北很容易中奖（他爸给他设定的真实中奖概率是0.3，不中的概率是0.7）；这样连中两次也不奇怪了（−log2​(0.3×0.3)≈3.47，大约就是连续抛硬币三次正面向上的概率）。. 所以，**本以为**中奖概率是0.0001，去进行试验，大呼吃惊，**结果发现**真实的概率是0.3，这个大呼吃惊的吃惊程度就是交叉熵。. 反之，如果小北早就知道了自己的老爸暗中安排了，连中两次似乎也没有多么神奇嘛！（0.3 × 0.3，不吃惊，信息量少）. 交叉熵的直观含义就是：**用自以为的分布去观测一个随机变量，结果发现得到数据多少令自己吃惊，和自以为的分布有差别，此时得到的信息量就是交叉熵**。. 更直白的说，交叉熵本质就是，**信息接收方，接收到的真实分布（后验分布）出乎预料预先猜测的分布（先验分布）的震惊程度**。. 把交叉熵放在通信模型中，它表示，接收方（图中女孩）接收到的信息，相对于它预期的吃惊程度。例如，女孩比较自卑，本以为男孩并不喜欢自己，结果男孩突然打过电话来向自己求爱表白。这时女孩的吃惊程度一定很高。相反，如果女孩是个海王，早就把男孩拿捏死死的，这时候男孩再打过电话来表白，女孩嘴角一撇，老娘只不过是玩玩而已。. 前述“太阳从东边升起”，这是100%的铁律常识，信息量为0。我们知道刘慈欣的流浪地球，地球受到各种影响，导致太阳从西边升起（**当前真实分布**），而此时，生活在地下的人们还不知道呢，以为还从东边升起（**先验的预期分布**），过了一段时间，这个消息被发布，全世界的人都吃了一惊。这个吃惊就是交叉熵。公式表达为. H(X)=x∈X​∑​p(x)log2​(q(x)1​). 其中p概率分布是事件真实发生的概率分布，而q概率分布则是先验的分布，从直观上来讲，公式的直观含义就是，用真实概率分布，与相应的人们心中默认的先验吃惊程度求期望。. > 同理，交叉熵的值一定大于等于 0。即便后验信息完全符合我们的先知先觉，此时的交叉熵值为 0。. > 只要后验信息不符合我们的先知先觉，那么，作为信息接收方，就又获得了新的信息，所获取的信息量永不减少。. ## 相对熵. 相对熵的概念基本上由交叉熵引申而来，本质上**相对熵和交叉熵是一回事**。根据交叉熵的定义，我们知道，交叉熵是一个十分绝对的值，那么相对熵就是一个相对的值。用事件X发生后的后验交叉熵，减去先验默认的信息熵，就是相对熵。相对熵又称KL散度（Kullback-Leibler Divergence）。. KL(p∣∣q)=H(p,q)−H(p). 由此可知，相对熵衡量的是一个相对的吃惊程度，如果先验概率分布（人们心中默认的分布）与真实分布差距过大，则相对熵就变大，反之两者极为相似，则相对熵就很小。. ## 交叉熵与神经网络的关系. 输入 X 是一个 n 维的向量，可以看作是一个随机变量，它可能是一张图片（计算机视觉），也可能是一句话（NLP），也可能是别的，输出 Y 是一个 k 维的向量，它代表了 X 的 k 种可能的输出**类别**结果，即也是一种随机变量。. 其实，可以将神经网络看作一个**通信模型**，输入的信号是 X，输出的信号是 Y，这就可以利用信息论的交叉熵了。. 为了方便理解，我们进一步假设这个分类任务是对一篇文本做分类，类别有政治、经济、娱乐、社会、科技、历史、家庭等7种类型。要分类的文本如下：. 假设我们的输入文章是 x ，那么，真实的标注语料中的 y 有一个概率分布p(x)，而神经网络模型预测出来的Y也有一个概率分布 q(x) 。. y先验​=y真实​=[0,0,0,0,1,0,0]. 其中，第5个元素的1代表科技类别，而其它的0代表所有的其它类别，其含义就是，这篇文章100%属于科技类，0%的概率属于其它类别。假设模型预测的后验分布为. y后验​=y模型预测​=[0.1,0.1,0.04,0.06,0.2,0.4,0.09,0.01]. 在这里，科技类对应的概率是 0.4，也即，模型认为这篇文章有40%的概率属于科技类别，而其它的类别对应了相应不同的概率值，此时，两个概率分布的交叉熵值为. cross\\_entropy=log2​0.41​≈1.32. y后验​=y模型预测​=[0.01,0.01,0.01,0.01,0.01,0.94,0.01,0.01]. cross\\_entropy=log2​0.941​≈0.089. 这样看，**后验分布越接近真实的先验分布，交叉熵值越低；反之越高**。因此，交叉熵实际上就表示了一种震惊程度，本来文本是科技类，模型却分类到了别的类别上去！. ## 总结. ✔️ 交叉熵本质：信息接收方，接收到的真实分布（后验分布）出乎预料预先猜测的分布（先验分布）的震惊程度。. ✔️ 交叉熵就是神经网络中广泛使用的损失函数，用于衡量**真实分布**和**模型预测分布**之间的差异。. https://www.jionlp.com/AI/20220828. 如果您觉得本文还不错，欢迎分享/打赏本文。打赏并非要从中获得收益，而是希望知道 JioNLP 获得了多少读者的真心关注。 当然，如果你无视它，也不会影响你的阅读。再次表示欢迎和感谢！.",
    "file_path": "unknown_source",
    "create_time": 1768974285,
    "update_time": 1768974285,
    "_id": "doc-16faf1b1fda0ac11c029462efdf1a6f5"
  },
  "doc-0da87cc9d247bbbc97d0642c59a0a3ce": {
    "content": "[DOC_ID: chunk-b5d062ff]\n[领域: 信息论]\n给定一串要传输的文本信息，其中字母的出现概率为( )，其最佳编码长度为log_2p(x)，整段文本的平均编码长度为-\\sum_xp(x)\\log_2p(x)，即底为2 的熵. 在对分布( )",
    "file_path": "unknown_source",
    "create_time": 1768974439,
    "update_time": 1768974439,
    "_id": "doc-0da87cc9d247bbbc97d0642c59a0a3ce"
  },
  "doc-c78284e4671aee8befd9c6871d1db529": {
    "content": "[DOC_ID: chunk-be93bba6]\n[领域: 信息论]\n编码定理是信息论的核心理论，1948年由克劳德·香农在《通信的数学理论》中首次提出信道容量概念，证明存在可使错误概率任意小的编码方法。该定理推动汉明码、卷积码等纠错码",
    "file_path": "unknown_source",
    "create_time": 1768974467,
    "update_time": 1768974467,
    "_id": "doc-c78284e4671aee8befd9c6871d1db529"
  },
  "doc-5d34d389fc9a72d98bce995b723199a6": {
    "content": "[DOC_ID: chunk-73ba9ac1]\n[领域: 信息论]\n... 编码定理和信道编码定理。从数学观点看,这些定理是最优编码的存. 在定理。但从工程观点看,这些定理不是结构性的,不能从定理的结果直接得出实现最优编码. 的具体途径。",
    "file_path": "unknown_source",
    "create_time": 1768974495,
    "update_time": 1768974495,
    "_id": "doc-5d34d389fc9a72d98bce995b723199a6"
  },
  "doc-5b929f1d50e05258f54389ac4f58f0ae": {
    "content": "[DOC_ID: chunk-6131370e]\n[领域: 信息论]\n[20] TODERICI G, VINCENT D, JOHNSTON N,et al.Full resolu-tion image compression with recurrent neural net-works[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Successive refinement of images with deep joint source-channel coding[C]//Proceedings of 2019 IEEE 20th International Workshop on Signal Processing Ad-vances in Wireless Communications (SPAWC). Deep joint source-channel coding for wireless image retriev-al[C]//Proceedings of ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Deep learning for joint source-channel coding of text[C]//Proceedings of 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). M to 1 joint source-channel coding of gaussian sources via dichotomy of the input space based on deep learning[C]//Proceedings of 2019 Data Compression Conference (DCC).",
    "file_path": "unknown_source",
    "create_time": 1768974519,
    "update_time": 1768974519,
    "_id": "doc-5b929f1d50e05258f54389ac4f58f0ae"
  },
  "doc-29b81edcfc109c880298f79878843454": {
    "content": "[DOC_ID: chunk-e6c38f5e]\n[领域: 统计力学]\n玻尔兹曼机是一种适用于解决包含了大量“弱”约束的约束满足问题的并行计算结构。这里的并行计算结构就是人工神经网络。“弱”约束是相对“强”约束而言的。“强”约束条件是",
    "file_path": "unknown_source",
    "create_time": 1768974570,
    "update_time": 1768974570,
    "_id": "doc-29b81edcfc109c880298f79878843454"
  },
  "doc-74ac9b8a3a08c569f1c072b77c552920": {
    "content": "[DOC_ID: chunk-e30c4628]\n[领域: 统计力学]\n受限波尔兹曼机（Restricted Boltzmann Machines, RBMs）是无监督学习中的一个关键模型，主要用于特征学习和数据建模。它们是基于能量的随机神经网络，由可见",
    "file_path": "unknown_source",
    "create_time": 1768974595,
    "update_time": 1768974595,
    "_id": "doc-74ac9b8a3a08c569f1c072b77c552920"
  },
  "doc-93a18801c2737ad9536660fe9501c9d8": {
    "content": "[DOC_ID: chunk-e030d53d]\n[领域: 统计力学]\n深度玻尔兹曼机是一种以受限玻尔兹曼机(Restricted Boltzmann Machine，RBM)为基础的深度学习模型，其本质是一种特殊构造的神经网络。深度玻尔兹曼机由多层受限玻尔兹曼机",
    "file_path": "unknown_source",
    "create_time": 1768974619,
    "update_time": 1768974619,
    "_id": "doc-93a18801c2737ad9536660fe9501c9d8"
  },
  "doc-b2aa136f0a91bbc88ed87eed07a66512": {
    "content": "[DOC_ID: chunk-1ab11c63]\n[领域: 统计力学]\n在一般的玻尔兹曼机中隐藏单元之间可以相互连接构成循环神经网络，这使得模型难以进行有效的学习，但若对隐藏单元之间的层内连接进行限制，便可以很好的训练用于处理实际问题，",
    "file_path": "unknown_source",
    "create_time": 1768974638,
    "update_time": 1768974638,
    "_id": "doc-b2aa136f0a91bbc88ed87eed07a66512"
  },
  "doc-673b334bee153545178b73f52b190891": {
    "content": "[DOC_ID: chunk-db26d099]\n[领域: 统计力学]\n历史地位：Hopfield 网络在1980 年代神经网络复兴中起到了重要作用，为后来神经网络的记忆机制和递归结构提供了基础。同时，Hopfield 网络的能量最小化思想影响了后来的玻尔兹",
    "file_path": "unknown_source",
    "create_time": 1768974653,
    "update_time": 1768974653,
    "_id": "doc-673b334bee153545178b73f52b190891"
  },
  "doc-52188c2297ba9ea322f462b96137a3c3": {
    "content": "[DOC_ID: chunk-6952095b]\n[领域: 统计力学]\n# HopField Network and Restricted Boltzmann Machine (RBM). 本文简要介绍HopField网络和受限玻尔兹曼机（Restricted Boltzmann Machine, RBM）的原理。. 设共有N个神经元，$ x\\_i $为第i个神经元的输入，$ w\\_{ij} $为神经元i和j之间的权值（在无自反馈型HopField网络中，$ w\\_{ii} = 0$，即神经元不与自己连接；$ w\\_{ij} = w\\_{ji} $，即权重矩阵对称），$ \\theta\\_i $为第i个神经元的阈值，第i个神经元在时间t的状态为$ y(i, t) $，则有以下递推式：. \\[\\begin{align\\*} u(i, t+1) &= \\sum\\_{j=1}^{N}w\\_{ij}y(j, t) - \\theta\\_i \\\\ y(i, t+1) &= f(u(i, t+1)) \\label{eq1.2} \\end{align\\*}\\]. 为简单起见，我们考虑离散型HopField网络，即$f(x)$为sgn函数。在此，我们引入能量的概念，定义能量的“增量”为：. 此处，y只有1和-1两种取值。当$ y(i, t\\_1) = 1 $时，$ u(i, t\\_1) > 0 $，$ y(i, t\\_1) - y(i, t\\_2) > 0 $，故$ \\Delta E(i, t\\_1, t\\_2) < 0 $。同理，当$ y(i, t\\_1) = -1 $时，$ u(i, t\\_1) < 0 $，$ y(i, t\\_1) - y(i, t\\_2) < 0 $，故$ \\Delta E(i, t\\_1, t\\_2) < 0 $。因此，只要神经元i的状态y发生变化（无论是从1到-1还是从-1到1），能量变化值$ \\Delta E $都会为负，即能量变小。由此我们可以看出，神经网络的变化过程实质上是一个能量不断减小的过程。当HopField网络达到稳定时，能量函数最小。. \\[\\begin{align\\*} E(i, t) &= -\\frac{1}{2}u(i, t)y(i, t) \\nonumber\\\\ &= -(\\sum\\_{j=1}^{N}w\\_{ij}y(j, t) - \\theta\\_i)y(i, t) \\end{align\\*}\\]. \\[\\begin{align\\*} E(t) &= \\sum\\_{i \\in v}E(i, t) + \\sum\\_{i \\in h}E(i, t) \\nonumber\\\\ &= \\sum\\_{i \\in v}-\\frac{1}{2}(\\sum\\_{j=1}^{N}w\\_{ij}y(j, t) - \\theta\\_i)y(i, t) + \\sum\\_{i \\in h}-\\frac{1}{2}(\\sum\\_{j=1}^{N}w\\_{ij}y(j, t) - \\theta\\_i)y(i, t) \\nonumber\\\\ &= \\sum\\_{i \\in v}-\\frac{1}{2}(\\sum\\_{j \\in h}w\\_{ij}y(j, t) - \\theta\\_i)y(i, t) + \\sum\\_{i \\in h}-\\frac{1}{2}(\\sum\\_{j \\in v}w\\_{ij}y(j, t) - \\theta\\_i)y(i, t) \\nonumber\\\\ &= -\\sum\\_{i \\in v}\\sum\\_{j \\in h}w\\_{ij}y(j, t)y(i, t) + \\sum\\_{i=1}^{N}\\theta\\_i y(i, t) \\end{align\\*}\\]. * 限制玻尔兹曼机（Restricted Boltzmann Machine）学习笔记.",
    "file_path": "unknown_source",
    "create_time": 1768974667,
    "update_time": 1768974667,
    "_id": "doc-52188c2297ba9ea322f462b96137a3c3"
  },
  "doc-e1dd53dde2d567ed67b1ff39a2e656e1": {
    "content": "[DOC_ID: chunk-b41307dc]\n[领域: 统计力学]\n# 刘建平Pinard. 在前面我们讲到了深度学习的两类神经网络模型的原理，第一类是前向的神经网络，即DNN和CNN。第二类是有反馈的神经网络，即RNN和LSTM。今天我们就总结下深度学习里的第三类神经网络模型：玻尔兹曼机。主要关注于这类模型中的受限玻尔兹曼机（Restricted Boltzmann Machine，以下简称RBM）， RBM模型及其推广在工业界比如推荐系统中得到了广泛的应用。. 玻尔兹曼机是一大类的神经网络模型，但是在实际应用中使用最多的则是RBM。RBM本身模型很简单，只是一个两层的神经网络，因此严格意义上不能算深度学习的范畴。不过深度玻尔兹曼机（Deep Boltzmann Machine，以下简称DBM）可以看做是RBM的推广。理解了RBM再去研究DBM就不难了，因此本文主要关注于RBM。. 上面一层神经元组成隐藏层(hidden layer), 用$h$向量隐藏层神经元的值。下面一层的神经元组成可见层(visible layer),用$v$向量表示可见层神经元的值。隐藏层和可见层之间是全连接的，这点和DNN类似, 隐藏层神经元之间是独立的，可见层神经元之间也是独立的。连接权重可以用矩阵$W$表示。和DNN的区别是，RBM不区分前向和反向，可见层的状态可以作用于隐藏层，而隐藏层的状态也可以作用于可见层。隐藏层的偏倚系数是向量$b$,而可见层的偏倚系数是向量$a$。. 总结下RBM模型结构的结构：主要是权重矩阵$W$, 偏倚系数向量$a$和$b$，隐藏层神经元状态向量$h$和可见层神经元状态向量$v$。. 对于给定的状态向量$h$和$v$，则RBM当前的能量函数可以表示为：$$E(v,h) = -a^Tv - b^Th - h^TWv $$. 有了能量函数，则我们可以定义RBM的状态为给定$v,h$的概率分布为：$$P(v,h) = \\frac{1}{Z}e^{-E(v,h)}$$. 其中$Z'$为新的归一化系数，表达式为：$$\\frac{1}{Z'} = \\frac{1}{P(v)}\\frac{1}{Z}exp\\{a^Tv\\}$$. 从上面可以看出， RBM里从可见层到隐藏层用的其实就是sigmoid激活函数。同样的方法，我们也可以得到隐藏层到可见层用的也是sigmoid激活函数。即：$$ P(v\\_j =1|h) = sigmoid(a\\_j + W\\_{:,j}^Th)$$. RBM模型的关键就是求出我们模型中的参数$W,a,b$。如果求出呢？对于训练集的m个样本，RBM一般采用对数损失函数，即期望最小化下式：$$L(W,a,b) = -\\sum\\limits\\_{i=1}^{m}ln(P(V^{(i)}))$$. 注意，这里面$V$表示的是某个特定训练样本，而$v$指的是任意一个样本。. 我们以$a\\_i$的梯度计算为例：$$\\begin{align} \\frac{\\partial (-ln(P(V)))}{\\partial a\\_i} & = \\frac{1}{\\partial a\\_i} \\partial{ln(\\sum\\limits\\_{v,h}e^{-E(v,h)})} - \\frac{1}{\\partial a\\_i} \\partial{ln(\\sum\\limits\\_he^{-E(V,h)})} \\\\& = -\\frac{1}{\\sum\\limits\\_{v,h}e^{-E(v,h)}}(\\sum\\limits\\_{v,h}e^{-E(v,h)}\\frac{\\partial E(v,h)}{\\partial a\\_i}) + \\frac{1}{\\sum\\limits\\_{h}e^{-E(V,h)}}(\\sum\\limits\\_{h}e^{-E(V,h)}\\frac{\\partial E(V,h)}{\\partial a\\_i}) \\\\& = \\sum\\limits\\_{h} P(h|V)\\frac{\\partial E(V,h)}{\\partial a\\_i} - \\sum\\limits\\_{v,h}P(h,v)\\frac{\\partial E(v,h)}{\\partial a\\_i} \\\\& = - \\sum\\limits\\_{h} P(h|V)V\\_i + \\sum\\limits\\_{v,h}P(h,v)v\\_i \\\\& = - \\sum\\limits\\_{h} P(h|V)V\\_i + \\sum\\limits\\_{v}P(v)\\sum\\limits\\_{h}P(h|v)v\\_i \\\\& = \\sum\\limits\\_{v}P(v)v\\_i - V\\_i \\end{align} $$. 同样的方法，可以得到$W,b$的梯度。这里就不推导了，直接给出结果：$$ \\frac{\\partial (-ln(P(V)))}{\\partial b\\_i} = \\sum\\limits\\_{v}P(v)P(h\\_i=1|v) - P(h\\_i=1|V) $$$$ \\frac{\\partial (-ln(P(V)))}{\\partial W\\_{ij}} = \\sum\\limits\\_{v}P(v)P(h\\_i=1|v)v\\_j - P(h\\_i=1|V)V\\_j $$. 虽然梯度下降法可以从理论上解决RBM的优化，但是在实际应用中，由于概率分布的计算量大，因为概率分布有$2^{n\\_v+n\\_h}$种情况, 所以往往不直接按上面的梯度公式去求所有样本的梯度和，而是用基于MCMC的方法来模拟计算求解每个样本的梯度损失再求梯度和，常用的方法是基于Gibbs采样的对比散度方法来求解，对于对比散度方法，由于需要MCMC的知识，这里就不展开了。对对比散度方法感兴趣的可以看参考文献中2的《A Practical Guide to Training Restricted Boltzmann Machines》，对于MCMC，后面我专门开篇来讲。. 如果大家对RBM在推荐系统的应用具体内容感兴趣，可以阅读参考文献3中的《Restricted Boltzmann Machines for Collaborative Filtering》. 1） Deep Learning, book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2） A Practical Guide to Training Restricted Boltzmann Machines， by G. 3) Restricted Boltzmann Machines for Collaborative Filtering， by G. posted @ 2017-03-11 09:50 刘建平Pinard 阅读(48890) 评论(44) 收藏) 举报).",
    "file_path": "unknown_source",
    "create_time": 1768974747,
    "update_time": 1768974747,
    "_id": "doc-e1dd53dde2d567ed67b1ff39a2e656e1"
  },
  "doc-9be983c468c91cc26e854198668abadf": {
    "content": "[DOC_ID: chunk-7b6798cb]\n[领域: 统计力学]\n本文提出了一种新的时间 网 络 嵌入方法M2DNE，它结合了 微 观 和 宏 观 动 态 ，以捕获 网 络 结构和时间特性的演变。实际上，时间 网 络 是普遍存在的，它通常是在 微 观 和 宏 观 动 态 方面随时间演化的。 微 观 动力学详细描述了 网 络 结构的形成过程， 宏 观 动力学是指 网 络 规模的演化模式。",
    "file_path": "unknown_source",
    "create_time": 1768974852,
    "update_time": 1768974852,
    "_id": "doc-9be983c468c91cc26e854198668abadf"
  },
  "doc-e0eade45db8d5fa19aa9c81c7da0ecef": {
    "content": "[DOC_ID: chunk-8e298a1c]\n[领域: 统计力学]\n2005年，美国印第安纳大学的奥拉夫·斯庞斯（Olaf Sporns）首先提出了“ 神 经 连接组学”（Connectomics）的概念，一门以研究 神 经 网 络 连接为主的新学科应运而生。 生物体内的 神 经 功能连接图谱是每个生物进行任何行为的 神 经 学基础。",
    "file_path": "unknown_source",
    "create_time": 1768974874,
    "update_time": 1768974874,
    "_id": "doc-e0eade45db8d5fa19aa9c81c7da0ecef"
  },
  "doc-c78a1678dd8dd1d890f89ffee94c2631": {
    "content": "[DOC_ID: chunk-d196641b]\n[领域: 统计力学]\n当 神 经 科学家追踪大脑区域之间的连接时，他们着眼于 神 经 元。 这是大脑细胞被生物电讯号激活时的连接 网 络 。 神 经 元相互连接，通过复杂的 神 经 回路传递电讯号。 人脑内有着860亿个手拉手的 神 经 元。",
    "file_path": "unknown_source",
    "create_time": 1768974899,
    "update_time": 1768974899,
    "_id": "doc-c78a1678dd8dd1d890f89ffee94c2631"
  },
  "doc-3fa882df56ee354fb7b7e6d63cec74d9": {
    "content": "[DOC_ID: chunk-12ec740e]\n[领域: 统计力学]\n像，并发表一系列脑 神 经 连结与脑 网 络 造影技术论文，引领脑 神 经 准确成像，使于临床磁共振仪短时间内取得复杂弥散 神 经 影像与脑 神 经 网 络 。会议主题：树兰俊杰科学Talk第三十五期-脑与 神 经 科学— 宏 观 介 观 微 观 尺度上的连接与涌现. 会议时间：2022/07/28 14:00-17:00.",
    "file_path": "unknown_source",
    "create_time": 1768974933,
    "update_time": 1768974933,
    "_id": "doc-3fa882df56ee354fb7b7e6d63cec74d9"
  },
  "doc-f922579134fc1d01217fc44031832b5f": {
    "content": "[DOC_ID: chunk-9bea0279]\n[领域: 统计力学]\n基于人工 神 经 网 络 的固体推进剂细 观 损伤与 宏 观 刚度映射关系. 张滔韬1, 2, 杨玉新2在单轴拉伸(Uniaxial Tension, UT)、等双轴拉伸(Equibiaxial Tension, ET)、纯剪切(Pure Shear, PS)三种变形条件下的 宏 观 刚度预报能力。",
    "file_path": "unknown_source",
    "create_time": 1768974981,
    "update_time": 1768974981,
    "_id": "doc-f922579134fc1d01217fc44031832b5f"
  },
  "doc-f35d76c7980a2da4b4bac2aba77d73e7": {
    "content": "[DOC_ID: chunk-74e8d199]\n[领域: 统计力学]\n１８０１０００００１１０１３ １３闫雯．ｆｂｄ ３７卷１期 ２０１８年２月 中 国 生 物 医 学 工 程 学 报 ＣｈｉｎｅｓｅＪｏｕｒｎａｌｏｆＢｉｏｍｅｄｉｃａｌＥｎｇｉｎｅｅｒｉｎｇ Ｖｏｌ．３７ Ｎｏ．１ Ｆｅｂｒｕａｒｙ ２０１８ ｄｏｉ：１０􀆰３９６９／ｊ．ｉｓｓｎ．０２５８⁃８０２１􀆰２０１８􀆰０１􀆰００ 收稿日期：２０１７⁃０３⁃１６，录用日期：２０１７⁃１０⁃１３ 基金项目：国家自然科学基金（８１７７１９１０）；深圳科创委技术攻关项（ｓｈｅｎｆａｇａｉ２０１６６２７）；北京市自然科学基金（４１５２０３３）；国家科学技术 重大专项（２０１７ｙｆｃ０１１０９０３）；北京航空航天大学软件开发环境国家重点实验室中央高校基础研究基金（ｓｋｌｓｄｅ⁃２０１７ｚｘ⁃０８）；中国的１１１项目 （ｂ１３００３） ∗通信作者（Ｃｏｒｒｅｓｐｏｎｄｉｎｇａｕｔｈｏｒ），Ｅ⁃ｍａｉｌ：ｌｍｄ＠ｚｊｕ．ｅｄｕ．ｃｎ，ｘｕｙａｎ＠ｂｕａａ．ｅｄｕ．ｃｎ 深度学习在数字病理中的应用 闫 雯 １ 汤 烨 １ 张益肇 ２ 来茂德 ３ 许 燕 ＃１，２ １（北京航空航天大学生物与医学工程系，北京航空航天大学软件开发环境国家重点实验室，深圳北航新兴产业技术研究院， 生物医学工程高精尖创新中心） ２（微软亚洲研究院，北京 １０００８０） ３（浙江大学医学院基础医学院，杭州 ３１００５８） 摘 要：临床上，病理切片是癌症诊断的金标准。病理医生通过对病理切片进行镜检，完成病理诊断和预后评估， 但是这个过程通常费时费力。在病理切片的数字化的背景下，人工智能技术走进病理领域，并推动病理分析逐渐 从定性分析向定量分析转变，这一改变使病理诊断更加准确客观。尤其是以深度学习为代表的人工智能技术在病 理分析中取得令人瞩目的成果，不但使病理诊断更加智能化，而且使诊断结果更加精准和客观。阐述深度学习的 基本概念及其在数字病理切片分析中的应用，简要概述深度学习在细胞和组织的检测和分割、组织层面上癌症的 分类和分级的应用，以及其他一些应用，最后指出目前数字病理切片分析中存在的问题并对未来的发展方向进行 展望。 关键词：数字病理切片；深度学习；定量分析 中图分类号：Ｒ３１８ 文献标志码：Ａ 文章编号：０２５８⁃８０２１（２０１８）０１⁃００００⁃００ ＤｅｅｐＬｅａｒｎｉｎｇｉｎＤｉｇｉｔａｌＰａｔｈｏｌｏｇｙＡｎａｌｙｓｉｓ ＷｅｎＹａｎ １ ＹｅＴａｎｇ １ ＥｒｉｃＩＣｈａｏＣｈａｎｇ ２ ＭａｏｄｅＬａｉ ３ ＹａｎＸｕ ＃１，２ １（ＳｃｈｏｏｌｏｆＢｉｏｌｏｇｉｃａｌＳｃｉｅｎｃｅａｎｄＭｅｄｉｃａｌＥｎｇｉｎｅｅｒｉｎｇ，ＢＵＡＡ，ＳｔａｔｅＫｅｙＬａｂｏｒａｔｏｒｙｏｆＳｏｆｔｗａｒｅＤｅｖｅｌｏｐｍｅｎｔＥｎｖｉｒｏｎｍｅｎｔ ａｎｄＫｅｙＬａｂｏｒａｔｏｒｙｏｆＢｉｏｍｅｃｈａｎｉｃｓａｎｄＭｅｃｈａｎｏｂｉｏｌｏｇｙｏｆＭｉｎｉｓｔｒｙｏｆＥｄｕｃａｔｉｏｎａｎｄＲｅｓｅａｒｃｈ ＩｎｓｔｉｔｕｔｅｏｆＢｅｉｈａｎｇＵｎｉｖｅｒｓｉｔｙｉｎＳｈｅｎｚｈｅｎ，ＢｅｉｊｉｎｇＡｄｖａｎｃｅｄＩｎｎｏｖａｔｉｏｎＣｅｎｔｅｒｆｏｒＢｉｏｍｅｄｉｃａｌＥｎｇｉｎｅｅｒｉｎｇ，Ｂｅｉｊｉｎｇ１００１９１，Ｃｈｉｎａ） ２（ＭｉｃｒｏｓｏｆｔＲｅｓｅａｒｃｈ，Ｂｅｉｊｉｎｇ１０００８０，Ｃｈｉｎａ） ３（ＤｅｐａｒｔｍｅｎｔｏｆＰａｔｈｏｌｏｇｙ，ＳｃｈｏｏｌｏｆＭｅｄｉｃｉｎｅ，ＺｈｅｊｉａｎｇＵｎｉｖｅｒｓｉｔｙ，Ｈａｎｇｚｈｏｕ３１００５８，Ｃｈｉｎａ） Ａｂｓｔｒａｃｔ：Ｐａｔｈｏｌｏｇｙｉｓｒｅｇａｒｄｅｄａｓｔｈｅｇｏｌｄｓｔａｎｄａｒｄｆｏｒｄｉａｇｎｏｓｉｓｏｆｃａｎｃｅｒｉｎｃｌｉｎｉｃａｌ．Ｐａｔｈｏｌｏｇｉｃａｌａｎａｌｙｓｉｓ ａｎｄｐｒｏｇｎｏｓｉｓａｒｅｕｓｕａｌｌｙｐｅｒｆｏｒｍｅｄｂｙｐａｔｈｏｌｏｇｉｓｔｓ，ｈｏｗｅｖｅｒ，ｃｏｕｌｄｂｅｔｉｍｅ⁃ｃｏｎｓｕｍｉｎｇａｎｄｌａｂｏｒ⁃ｉｎｔｅｎｓｉｖｅ．Ａｓ ｔｈｅｄｅｖｅｌｏｐｍｅｎｔｏｆｗｈｏｌｅｓｌｉｄｅｐａｔｈｏｌｏｇｙ，ｉｔｉｓｔｈａｎｋｓｔｏａｒｔｉｆｉｃｉａｌｉｎｔｅｌｌｉｇｅｎｃｅ（ＡＩ）ｔｈａｔｍａｙｐｒｏｍｏｔｅ ｐａｔｈｏｌｏｇｉｃａｌａｎａｌｙｓｉｓｇｒａｄｕａｌｌｙｆｒｏｍｑｕａｌｉｔａｔｉｖｅａｎａｌｙｓｉｓｔｏｑｕａｎｔｉｔａｔｉｖｅａｎａｌｙｓｉｓ． Ｉｎｒｅｃｅｎｔｙｅａｒｓ，ｔｈｅＡＩ ｔｅｃｈｎｏｌｏｇｙ，ｅｓｐｅｃｉａｌｌｙｄｅｅｐｎｅｕｒａｌｎｅｔｗｏｒｋ，ｈａｓｇｒｅａｔｌｙｐｒｏｍｏｔｅｄｔｈｅｐｒｏｇｒｅｓｓｏｆｐａｔｈｏｌｏｇｉｃａｌｄｉａｇｎｏｓｉｓ，ｗｈｉｃｈ ｉｓｍｏｒｅｉｎｔｅｌｌｅｃｔｕａｌｉｚｅｄ，ａｃｃｕｒａｔｅａｎｄｒｅｐｅａｔａｂｌｅ．Ｔｈｉｓｐａｐｅｒｄｅｓｃｒｉｂｅｓｔｈｅｂａｓｉｃｃｏｎｃｅｐｔｏｆｄｅｅｐｌｅａｒｎｉｎｇａｎｄ ｉｔｓａｐｐｌｉｃａｔｉｏｎｉｎｄｉｇｉｔａｌｐａｔｈｏｌｏｇｙａｎａｌｙｓｉｓ．Ｗｅｇａｖｅａｂｒｉｅｆｏｖｅｒｖｉｅｗｏｆｔｈｅａｐｐｌｉｃａｔｉｏｎｏｆｄｅｅｐｌｅａｒｎｉｎｇｉｎｔｈｅ ｄｅｔｅｃｔｉｏｎａｎｄｓｅｇｍｅｎｔａｔｉｏｎｏｆｃｅｌｌａｎｄｔｉｓｓｕｅ，ｃｌａｓｓｉｆｉｃａｔｉｏｎａｎｄｇｒａｄｉｎｇｏｆｃａｎｃｅｒ，ａｎｄｏｔｈｅｒａｐｐｌｉｃａｔｉｏｎｓ． Ｆｉｎａｌｌｙ，ｗｅｒａｉｓｅｄｔｈｅｅｘｉｓｔｉｎｇｐｒｏｂｌｅｍｓａｎｄｔｈｅｐｒｏｓｐｅｃｔｏｆｆｕｔｕｒｅｄｅｖｅｌｏｐｍｅｎｔｉｎｔｈｅａｎａｌｙｓｉｓｏｆｄｉｇｉｔａｌ ｐａｔｈｏｌｏｇｙ． Ｋｅｙｗｏｒｄｓ：ｄｉｇｉｔａｌｐａｔｈｏｌｏｇｙ，ｄｅｅｐｌｅａｒｎｉｎｇ，ｑｕａｎｔｉｔａｔｉｖｅａｎａｌｙｓｉｓ １８０１０００００１１０１３ １３闫雯．ｆｂｄ １期 闫雯，等：深度学习在数字病理中的应用 引言 病理切片作为病理诊断的金标准，在临床和科 研中都有着十分重要的应用。病理医生通过对病 理切片进行镜检，完成病理诊断和预后评估，但是 这个过程通常费时费力。病理切片的数字化被认 为是病理学发展过程中的重要转折点［１］。数字病 理切片的制作首先需要经过组织染色，随后通过显 微相机数字化为数字病理切片。但是，由于最初数 字切片质量不高，其普及程度受到一定限制。１９９９ 年，全片数字化图像（ｗｈｏｌｅｓｌｉｄｅｉｍａｇｅｓ，ＷＳＩ）［１］出 现，使病理切片的保存传输更加方便安全。 随着人工智能技术走进病理分析领域，病理分 析不再局限于传统的定性分析，逐渐向定量分析过 渡［２⁃３］。定性分析则是对切片性质特点的一种概 括，并没有形成量化指标，因此定性分析的结果不 可复现，且受主观因素影响较大。定量分析是指依 据统计数据，建立数学模型，从而计算出与病变相 关的各项指标，如有丝分裂数目、肿瘤的实质与间 质的比例、黏液湖和癌细胞的比例等定量化指标； 并根据定量指标给出病理诊断，其诊断结果更加客 观。因此，病理医生也越来越认识到定量分析的重 要性。 传统机器学习算法和深度学习算法都可用于 定量分析［４］。传统的机器学习算法依赖于人工设 计特征表达，如提取图像的形状、大小和纹理等特 征并做特征挑选，剔除冗余特征得到最优特征集。 但是，这种人工特征的选取依赖于大量专业知识， 且难以涵盖图像的全面特征，导致其使用上的局 限性。 深度学习预先定义了计算规则，通过层级式网 络结构，将数据从输入层传递到输出层，并自动学 习图像特征表达，得到图像的低维特征。相比其他 机器学习算法，深度学习算法对大数据样本的特征 提取能力更强。面对临床上不断积累的ＷＳＩ数据， 深度病理能充分发挥在大数据样本上的优势，推动 病理定量分析的发展，辅助医生完成病理诊断。 因此，基于深度学习的病理切片图像的定量分 析研究，既是数字病理分析的大势所趋，也是学术 界和医学界共同努力的方向。 １ 深度学习的基本概念 深度学习是机器学习中一种基于对数据进行 特征学习的方法。深度学习很宽泛，包含多层隐层 的人工神经网络模型也可以叫做深度学习，其通过 组合低层特征学习到数据的高层特征表达［５］。 在通常情况下，更深的网络深度意味着网络具 有更强的特征提取能力。但是，网络深度的加深往 往会导致模型难以收敛、计算量巨大等问题。为了 克服上述问题，需要一些更强大的技术的辅助，如 卷积、ｐｏｏｌｉｎｇ、ｄｒｏｐｏｕｔ、ＲｅＬＵ函数以及使用ＧＰＵ训 练网络等。 在图像处理与计算机视觉领域，以卷积、ｐｏｏｌｉｎｇ 和ＲｅＬＵ函数为基础构成的卷积网络（ＣｏｎｖＮｅｔ）是 深度学习最常用的网络结构之一，图１为两层 ＣｏｎｖＮｅｔ构成的ＣＮＮ。 图１ 两层卷积神经网络（由卷积层和池化层以及全连接层构成） Ｆｉｇ．１ Ｔｗｏｌａｙｅｒｃｏｎｖｏｌｕｔｉｏｎｎｅｕｒａｌｎｅｔｗｏｒｋ，ｗｈｉｃｈｉｓｃｏｎｓｔｉｔｕｔｅｄｂｙｔｈｅｃｏｎｖｏｌｕｔｉｏｎａｌｌａｙｅｒ，ｔｈｅｐｏｏｌｌａｙｅｒａｎｄ ｔｈｅｆｕｌｌｙｃｏｎｎｅｃｔｅｄｌａｙｅｒ ＣＮＮ将图像直接作为网络的输入，避免了传统 识别算法中复杂的人工特征设计过程。这种网络 结构对平移、比例缩放、倾斜或者其他形式的变形 具有高度不变性，在计算机视觉中有广泛的应用。 １􀆰１ 卷积神经网络 卷积神经网络（ＣＮＮ）引入权值共享和局部视 野域的概念，大大减少了权值的数量。 １）权值共享。每个隐藏神经元具有同样的偏 ７ ９ １８０１０００００１１０１３ １３闫雯．ｆｂｄ 中 国 生 物 医 学 工 程 学 报 ３７卷 置和相同的连接到局部感受野的权重。每个隐藏 神经元可以学习一种特定的特征映射，最后通过多 层堆叠的方式，学习到图片的整体特征表达。 对于ｌ层的第ｊ、ｋ个隐藏神经元，输出为 ａｌｊ＝σ（ｗｌｊｋ·ａｌ－１ｋ＋ｂｌｊ） （１） 式中，σ 是神经元的激活函数，ｂ是偏置的共享值，ｗ 是共享权重的数组，ａ是激活值。 卷积神经网络能够自动学习到合适的权重和 偏置，以至于网络的输出ｙ（ｘ）能够拟合所有的训练 输入ｘ。为了量化输入ｘ与输出ｙ（ｘ）之间的差距， 引入代价函数的概念，有 Ｃ（ｗ，ｂ）＝１ ２ｎ∑ ｘ ‖ｙ（ｘ）－ａ‖２ （２） 式中，ｗ表示网络中所有的权重的矩阵，ｂ是所有的 偏置的矩阵，ｎ是训练输入数据的个数，ａ表示输入 为ｘ是输出的激活值，求和则是在所有的输入数据 ｘ上进行的。 网络训练的过程其实就是找到代价函数最小 的时候对应的权重的值。直接计算代价函数的最 小值得计算量十分巨大，１９８６年，Ｒｕｍｅｌｈａｒｔ提出反 向传播算法［６］，大大减少了最小化代价函数时的计 算量，为加深网络层数提供了可能性。 ２）ＲｅＬＵ函数。该函数是神经网络中最常用的 激活函数之一，通常与卷积层同时出现，其公式为 ｆ（ｘ）＝ｍａｘ（０，ｘ） （３） 式中：当输入ｘ＜０时，输出为０；当输入ｘ＞０时，输 出ｘ。 ＲｅＬＵ函数的优点在于梯度不饱和，在反向传 播过程中，减轻了梯度弥散的问题，神经网络前几 层的参数也可以很快更新；在正向传播过程中，计 算速度更快。 ３）混合层：除了上面提到的卷积层，卷积神经 网络还包含混合层（ｐｏｏｌｉｎｇｌａｙｅｒ）。混合层通常紧 接着卷积层之后使用，其作用是对卷积层的输出信 息进行降采样，减少参数的个数，避免过拟合。详 细地说，混合层将输出的每个特征映射凝缩成一个 新的特征映射，常见的混合层有最大值混合（ｍａｘ⁃ ｐｏｏｌｉｎｇ）和均值混合（ａｖｅｒａｇｅ⁃ｐｏｏｌｉｎｇ）。以ｍａｘ⁃ ｐｏｏｌｉｎｇ层为例，该层提取一定大小窗口内特征图的 最大值作为新的特征值输入到下一层，如图２所示。 １􀆰２ 深度神经网络 ２０１２年，Ｋｒｉｚｈｅｖｓｋｙ提出深度卷积网络 ＡｌｅｘＮｅｔ［７］，在数据集ＩｍａｇｅＮｅｔ［８］上实现了图像分类 任务并将错误率降低到１７􀆰０％。在此基础上， 图２ 最大池化层（窗口大小为２× ２，依次移动窗口 并取该窗口内特征图的最大值，组成下一层的特征 图） Ｆｉｇ．２ Ｇｒａｍｏｆｔｈｅｍａｘ⁃ｐｏｏｌｉｎｇｌａｙｅｒ，ｔｈｅｓｉｚｅｏｆ ｓａｍｐｌｅｗｉｎｄｏｗ ｉｓ２ｘ２． Ｔｈｅ ｗｉｎｄｏｗ ｔａｋｅ ｔｈｅ ｍａｘｉｍｕｍｖａｌｕｅｏｆｆｅａｔｕｒｅｍａｐｗｉｔｈｉｎｔｈｅｗｉｎｄｏｗ， ｔｏｃｏｍｐｏｓｅｔｈｅｆｅａｔｕｒｅｍａｐｏｆｎｅｘｔｌａｙｅｒ Ｚｉｓｓｅｒｍａｎ提出了一种新的深度卷积网络结构—",
    "file_path": "unknown_source",
    "create_time": 1768975021,
    "update_time": 1768975021,
    "_id": "doc-f35d76c7980a2da4b4bac2aba77d73e7"
  },
  "doc-ce6ef636e7f1aba9c1d2e5f6ef438c33": {
    "content": "[DOC_ID: chunk-3095317b]\n[领域: 统计力学]\n图2 水分子及水分子的 微 观 堆积结构. 水的这些特殊的物理化学性质都是源于水的 微 观 结构。 一个自由的水分子是由两个氢(H)和一个氧(O)组成的H2O（如图2上部左），两个氢和氧的化学键夹角大约为105度。",
    "file_path": "unknown_source",
    "create_time": 1768975191,
    "update_time": 1768975191,
    "_id": "doc-ce6ef636e7f1aba9c1d2e5f6ef438c33"
  },
  "doc-7f7cfd0cd4e85028a17edf6e5da1190c": {
    "content": "[DOC_ID: chunk-2017a15e]\n[领域: 统计力学]\n[3] WANG Q X， DING S L， LI Y， et al.The Allen Mouse Brain Common Coordinate Framework: a 3D refer-ence atlas ［J］ .Cell， 2020， 181 （4） ： 936-953.e20. [11] HAN Y Y， KEBSCHULL J M， CAMPBELL R A A， et al.The logic of single-cell projections from visu-al cortex ［J］ .Nature， 2018， 556 （7699） ： 51-56. [3] GAUTHIER B， PRABHU P， KOTEGAR K A， et al.Hippocampal contribution to ordinal psychological time in the human brain ［J］ .Journal of cognitive neuroscience， 2020， 32 （11） ： 1-15. [3] WILLETT F R， AVANSINO D T， HOCHBERG L R， et al.High-performance brain-to-text communication via handwriting ［J］ .Nature， 2021， 593 （7858） ： 249-254.",
    "file_path": "unknown_source",
    "create_time": 1768975215,
    "update_time": 1768975215,
    "_id": "doc-7f7cfd0cd4e85028a17edf6e5da1190c"
  },
  "doc-bc6cd88a44423ac7bc4a08969a8b99f8": {
    "content": "[DOC_ID: chunk-1716cb3a]\n[领域: 统计力学]\n（1）网络的输出层配分函数等于输入数据也即一维伊辛模型有的配分函数，. 并且在训练过程中当时间t 从0 到∞ 变化时始终保持配分函数的值不变。 f(x1w. (t). 1j. +x2w. (t).",
    "file_path": "unknown_source",
    "create_time": 1768975285,
    "update_time": 1768975285,
    "_id": "doc-bc6cd88a44423ac7bc4a08969a8b99f8"
  },
  "doc-7d7618b6fffb013ab6c5a2ca3a3fea50": {
    "content": "[DOC_ID: chunk-c7102035]\n[领域: 统计力学]\n... 网络的配分函数为. 其中. 是L的第i个特征值，配分函数有明确的物理含义，其表示保留在所有原始节点上的信息量。 图1：空手道俱乐部网络的配分函数. 图1",
    "file_path": "unknown_source",
    "create_time": 1768975312,
    "update_time": 1768975312,
    "_id": "doc-7d7618b6fffb013ab6c5a2ca3a3fea50"
  },
  "doc-36562522576eae8192339d548d0a1406": {
    "content": "[DOC_ID: chunk-78091bba]\n[领域: 统计力学]\n... 配分函数的场景。它不能降低计算开销，但是在神经网络的背景下降低batchsize 带来的显存的节省是天量的，而通信和存储往往比计算贵。 发布于2025-06",
    "file_path": "unknown_source",
    "create_time": 1768975339,
    "update_time": 1768975339,
    "_id": "doc-36562522576eae8192339d548d0a1406"
  },
  "doc-08c978d767db01aaeef63f40ea958f51": {
    "content": "[DOC_ID: chunk-53389998]\n[领域: 统计力学]\n基于分数的生成模型是一种生成模型，它通过定义一个分数函数来描述数据的分布。这个分数函数通常被称为\"配分函数\"（partition function）或\"能量函数\"（energy",
    "file_path": "unknown_source",
    "create_time": 1768975361,
    "update_time": 1768975361,
    "_id": "doc-08c978d767db01aaeef63f40ea958f51"
  },
  "doc-5b96b65f16399f35c9b04de8b22d0796": {
    "content": "[DOC_ID: chunk-f326307e]\n[领域: 统计力学]\n文章提出了一种创新的动力配分函数高效计算的方法[图1]。 该方法采用循环神经网络、PixelCNN和Transformer等自回归生成模型，结合强化学习技术，学习并预测",
    "file_path": "unknown_source",
    "create_time": 1768975373,
    "update_time": 1768975373,
    "_id": "doc-5b96b65f16399f35c9b04de8b22d0796"
  },
  "doc-f5c5908d5644515cf8bcda82498e6671": {
    "content": "[DOC_ID: chunk-4ba79a58]\n[领域: 统计力学]\n通过最大似然学习无向模型特别困难的原因在于配分函数依赖于参数。 对数似然相对于参数的梯度具有一项对应于配分函数的梯度： ∇θlogp(x;θ)=∇θlog˜p(x;θ)−∇θlogZ(θ).",
    "file_path": "unknown_source",
    "create_time": 1768975398,
    "update_time": 1768975398,
    "_id": "doc-f5c5908d5644515cf8bcda82498e6671"
  },
  "doc-882f6cc5b251194ec83bb3e0280e7611": {
    "content": "[DOC_ID: chunk-b9060847]\n[领域: 统计力学]\n分母或归一化常数，有时也称为配分函数（其对数称为对数-配分函数）。该名称的起源来自统计物理学中一个模拟粒子群分布的方程。 public NDArray softmax(NDArray X)",
    "file_path": "unknown_source",
    "create_time": 1768975422,
    "update_time": 1768975422,
    "_id": "doc-882f6cc5b251194ec83bb3e0280e7611"
  },
  "doc-f18946bdfdf43151c934dc17cbcb459e": {
    "content": "[DOC_ID: chunk-e91b05d3]\n[领域: 统计力学]\nRev. D， 2019， 100： 054510 [11] Yu J F， Xie Z Y， Meurice Y et al. Rev. B， 2016， 94： 075143； Liu W Y， Dong S， Wang C et al. Rev. B， 2012， 85： 205119 [41] Xie Z Y， Liao H J， Huang R Z et al. Rev. B， 2017， 96： 045128 [42] Liao H J， Xie Z Y， Chen J et al. Rev. B， 2017， 96： 085103 [45] Liu W Y， Dong S J， Han Y J et al. Rev. B， 2017， 95： 195154； Dong S J， Wang C， Han Y J et al. Rev. B， 2018， 97： 035116； Liang X， Liu W Y， Lin P Z et al. Rev. B， 2020， 101： 220409 [80] Liao H J， Liu J G， Wang L et al.",
    "file_path": "unknown_source",
    "create_time": 1768975459,
    "update_time": 1768975459,
    "_id": "doc-f18946bdfdf43151c934dc17cbcb459e"
  },
  "doc-c83ff92607271cc36071ca55ac4a8385": {
    "content": "[DOC_ID: chunk-b281d7dd]\n[领域: 机器学习]\n交叉熵刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。假设概率分布p为期望输出，概率分布q为实际输出，H(p,q)",
    "file_path": "unknown_source",
    "create_time": 1768975517,
    "update_time": 1768975517,
    "_id": "doc-c83ff92607271cc36071ca55ac4a8385"
  },
  "doc-c110f5c53d8d82646eee2498b51ba01a": {
    "content": "[DOC_ID: chunk-18f8d8f0]\n[领域: 机器学习]\n在神经网络训练中，要将输入数据实际的类别概率分布与模型预测的类别概率分布之间的误差（即损失）从输出端向输入端传递，以便来优化模型参数。下面简单介绍根据交叉熵计算得到",
    "file_path": "unknown_source",
    "create_time": 1768975544,
    "update_time": 1768975544,
    "_id": "doc-c110f5c53d8d82646eee2498b51ba01a"
  },
  "doc-04c7bf4ab86cafa8d0bdc6180c0f4db5": {
    "content": "[DOC_ID: chunk-1a3d3093]\n[领域: 机器学习]\n交叉熵损失函数（Cross Entropy Loss）在人工智能领域，尤其是深度学习中，是用于衡量模型预测结果与实际标签之间的差异的重要工具。它源于信息论中的熵和相对",
    "file_path": "unknown_source",
    "create_time": 1768975594,
    "update_time": 1768975594,
    "_id": "doc-04c7bf4ab86cafa8d0bdc6180c0f4db5"
  },
  "doc-2a3c466ac638775ff887f28337c2a65f": {
    "content": "[DOC_ID: chunk-267f1493]\n[领域: 机器学习]\n简单来说，损失函数追踪人工智能 (AI) 模型输出的错误程度。它通过量化给定输入的预测值（即模型输出）与实际值或*标准答案*之间的差异（“损失”）来实现。如果模型的预测值准确，则损失会很小。如果预测值不准确，损失就很大。. ### 专家为您带来最新的 AI 趋势. 获取有关最重要且最有趣的 AI 新闻的精选洞察分析。订阅我们的每周 Think 时事通讯。请参阅 IBM 隐私声明。. ### 均方误差 (MSE). 均方误差损失函数，也称为 L2 损失或二次损失，通常是大多数回归算法的默认函数。顾名思义，MSE 是所有训练样本中预测值和真实值之间的平方差的平均值。计算 n 个数据点的 MSE 的公式写为 1n∑i=1n(yi-yi^)2，其中 y是真实值，ŷ 是预测值。. *平均绝对误差*或*L1 损失*，用于衡量预测值和实际值之间的平均*绝对差*。与 MSE 一样，MAE 始终为正值，不会区分过高或过低的估计值。计算方法是所有误差的绝对值之和除以样本数量：. 由于 MAE 不会对每个损失值求平方，因此，MAE 比 MSE 更能抵御异常值。因此，当数据可能包含一些不应对模型产生过大影响的极值时，MAE 是理想的选择。与 L2 损失相比，L1 损失对较小误差的惩罚力度更大。. Huber 损失，也称为*平滑 L1 损失*，旨在平衡 MAE 和 MSE 两者的优势。它包含一个可调整超参数 *δ*，该超参数可充当过渡点：当损失值小于等于 *δ* 时，Huber 损失是二次的（例如 MSE）；当损失值大于 *δ* 时，Huber 损失为线性（例如 MAE）。. 因此，Huber 损失提供了一个完全可微分的函数，具有 MAE 对异常值的鲁棒性和 MSE 通过梯度下降易于优化的特点。与 MSE 损失相比，在 *δ* 处从二次行为过渡到线性行为也使优化更不易出现梯度消失或爆炸等问题。. 由于需仔细定义 *δ*，从而会增加模型开发的复杂性，于是这些优点会被削弱。当 MSE 和 MAE 均无法产生令人满意的结果时，Huber 损失则最为合适。例如，当模型应对异常值保持稳健性，但它仍会严厉惩罚超出某一特定阈值的极值时。. 分类交叉熵损失 (CCEL) 将相同的原理应用于多类分类。多类分类模型通常会为每个潜在类别输出一个值，代表属于每个相应类别的输入的概率。换句话说，它们以*概率分布*的形式输出预测。. 在使用铰链损失的算法中，每个二进制标签的真实值映射到 {-1, 1} 而非 {0,1}。铰链损失函数 *ℓ* 定义为 **ℓ(𝑦)=max(0,1−𝑡⋅𝑦)，**其中 *t* 是真实标签，*y* 是分类器的输出。这个方程的结果始终为非负数：如果 1−𝑡⋅𝑦 为负数（这只有在 *t* 和 *y* 符号相同时才有可能，因为模型预测了正确的类别），则损失将定义为 0。. * 当模型预测正确且可信时（即当 *y* 是正确的符号且 |*y*| ≥ 1 时），1–*t*⋅𝑦 的值将为负数，因此 *ℓ* = 0。. * 当模型预测正确但不可靠时，即当 *y* 符号正确但 |*y*|< 1 时，*ℓ* 的值将为正值，介于 0 和 1 之间。这会打消人们做出不自信预测的念头。. 想要从 AI 投资中获得更好的回报吗？了解如何通过帮助您最优秀的人才构建和提供创新的新解决方案，在关键领域扩展生成式人工智能来推动变革。. IBM Granite 是我们开放式、性能优异、值得信赖的 AI 模型系列，专门为企业量身定制，并经过优化，可以帮助您扩展 AI 应用程序。深入了解语言、代码、时间序列和护栏选项。. 指南 树立信任，从容自信在 AI 新时代蓬勃发展. 我们对 2,000 家组织进行了调查，旨在了解他们的 AI 计划，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。. 使用面向 AI 构建器的新一代企业级开发平台 IBM watsonx.ai，可以训练、验证、调整和部署生成式 AI、基础模型和机器学习功能。使用一小部分数据，即可在很短的时间内构建 AI 应用程序。. 通过增加 AI 重塑关键工作流程和运营，最大限度提升体验、实时决策和商业价值。. 一站式访问跨越 AI 开发生命周期的功能。利用用户友好型界面、工作流并访问行业标准 API 和 SDK，生成功能强大的 AI 解决方案。.",
    "file_path": "unknown_source",
    "create_time": 1768975622,
    "update_time": 1768975622,
    "_id": "doc-2a3c466ac638775ff887f28337c2a65f"
  },
  "doc-93b5920864ffe7c0f735f87ccf578d34": {
    "content": "[DOC_ID: chunk-7e13310c]\n[领域: 机器学习]\n（1）(4') 信息增益决策树中熵的计算中Ent(D) 什么时候取最大（小）值？值是多少？ （2）(6') 和18年期末类似，根据表格计算信息增益决策树的第一个选择分支。 （3）(6') 简述信息增益",
    "file_path": "unknown_source",
    "create_time": 1768975721,
    "update_time": 1768975721,
    "_id": "doc-93b5920864ffe7c0f735f87ccf578d34"
  },
  "doc-62fad8ab846c9df991d8fe50bdf2edf6": {
    "content": "[DOC_ID: chunk-1d19ba1b]\n[领域: 机器学习]\n2 信息增益. 信息增益是用于度量特征对于决策树的贡献的一个度量标准。信息增益可以通过以下公式计算：. I G ( S , A ) = H ( S ) − ∑ v ∈ A ∣ S",
    "file_path": "unknown_source",
    "create_time": 1768975750,
    "update_time": 1768975750,
    "_id": "doc-62fad8ab846c9df991d8fe50bdf2edf6"
  },
  "doc-a79596200294b5109fdd0cf885110f83": {
    "content": "[DOC_ID: chunk-dbd019af]\n[领域: 机器学习]\n3. **决策树（Decision Tree）**：决策树是一种基于树状结构进行决策的模型，适用于分类和回归任务。手写实现时，会涉及选择最佳分割特征（如信息增益、基尼不",
    "file_path": "unknown_source",
    "create_time": 1768975801,
    "update_time": 1768975801,
    "_id": "doc-a79596200294b5109fdd0cf885110f83"
  },
  "doc-a4d1d3bf1c5551379c10b7d809d654b0": {
    "content": "[DOC_ID: chunk-9740e9e0]\n[领域: 机器学习]\n决策树（Decision Tree）-ID3、C4.5、CART比较. 决策树是一种**基本的分类和回归方法**。决策树呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间和类空间上的条件概率分布。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括三个步骤：**特征选择、决策树的生成和决策树的剪枝**。决策树算法的发展从ID3到C4.5再到CART，下面会分别介绍。. 特征选择在于选取对训练数据具有分类能力的特征。特征选择的基本方法有三种，**（ID3的信息增益、C4.5的信息增益比、CART的基尼系数）**. ## 2.1 信息增益. 特征\\(A\\)对训练数据集\\(D\\)的信息增益\\(g(D,A)\\)，定义为集合\\(D\\)的经验熵\\(H(D)\\)与特征\\(A\\)给定条件下D的经验条件熵\\(H(D|A)\\)之差，即. \\[g(D,A)=H(D)-H(D|A) \\]. ## 2.2 信息增益比. ID3采用信息增益的方式很快就被人发现有问题，在相同条件下，取值比较多的特征比取值少的特征信息增益大，即**信息增益作为标准容易偏向于取值较多的特征**。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。. 所以在C4.5中，引入了信息增益比\\(I\\_R(X,Y)\\)，它是信息增益和特征熵的比值。表达式如下：. \\[I\\_R(D,A)=\\frac{I(A,D)}{H\\_A(D)} \\]. 其中\\(D\\)为样本特征输出的集合，\\(A\\)为样本特征，对于特征熵\\(H\\_A(D)\\), 表达式如下：. \\[H\\_A(D) = -\\sum\\limits\\_{i=1}^{n}\\frac{|D\\_i|}{|D|}log\\_2\\frac{|D\\_i|}{|D|} \\]. 其中\\(n\\)为特征\\(A\\)的类别数，\\(D\\_i\\)为特征\\(A\\)的第\\(i\\)个取值对应的样本个数。\\(D\\)为样本个数。. ## 2.3 基尼系数. 在ID3算法中我们使用了**信息增益**来选择特征，信息增益大的优先选择。在C4.5算法中，采用了**信息增益比**来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算，比较耗时。CART分类树算法使用**基尼系数**来代替信息增益比，**基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的**。. 具体的，在分类问题中，假设有\\(K\\)个类别，第\\(k\\)个类别的概率为\\(p\\_k\\), 则基尼系数的表达式为：. \\[Gini(p) = \\sum\\limits\\_{k=1}^{K}p\\_k(1-p\\_k) = 1- \\sum\\limits\\_{k=1}^{K}p\\_k^2 \\]. ## 3.1 ID3算法. 输入：训练集\\(D\\)，特征集\\(A\\)，阈值\\(\\varepsilon\\). 2. 判断样本是否为同一类输出\\(D\\_i\\)，如果是则返回单节点树\\(T\\)。标记类别为\\(D\\_i\\)。. 3. 判断特征是否为空，如果是则返回单节点树\\(T\\)，标记类别为样本中输出类别\\(D\\)实例数最多的类别。. 4. 计算\\(A\\)中的各个特征（一共n个）对输出\\(D\\)的信息增益，选择信息增益最大的特征\\(A\\_g\\)。. 5. 如果\\(A\\_g\\)的信息增益小于阈值\\(\\varepsilon\\)，则返回单节点树\\(T\\)，标记类别为样本中输出类别\\(D\\)实例数最多的类别。. 6. 否则，按特征\\(A\\_g\\)的不同取值\\(A\\_{gi}\\)将对应的样本输出D分成不同的类别\\(D\\_i\\)。每个类别产生一个子节点。对应特征值为\\(A\\_{gi}\\)。返回增加了节点的数\\(T\\)。. 7. 对于所有的子节点，令\\(D=D\\_i,A=A-{A\\_g}\\)递归调用2-6步，得到子树\\(T\\_i\\)并返回。. ## 3.2 C4.5算法. ## 3.3 CART算法. 输入是训练集\\(D\\)，基尼系数的阈值\\(\\varepsilon\\_1\\)，样本个数阈值\\(\\varepsilon\\_2\\)。. 1. 对于当前节点的数据集为\\(D\\)，如果样本个数小于阈值\\(\\varepsilon\\_2\\)或者没有特征，则返回决策子树，当前节点停止递归。. 2. 计算样本集\\(D\\)的基尼系数，如果基尼系数小于阈值\\(\\varepsilon\\_1\\)，则返回决策树子树，当前节点停止递归。. 4. 在计算出来的各个特征的各个特征值对数据集\\(D\\)的基尼系数中，选择基尼系数最小的特征\\(A\\)和对应的特征值\\(a\\)。根据这个最优特征和最优特征值，把数据集划分成两部分\\(D1\\)和\\(D2\\)，同时建立当前节点的左右节点，做节点的数据集\\(D\\)为\\(D1\\)，右节点的数据集\\(D\\)为\\(D2\\)。. 决策树生成算法递归地产生决策树，直到不能继续下去为止。**这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没那么准确，即出现过拟合现象**。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。. * **预剪枝**：是在构建决策树的过程中，提前终止决策树的生长，从而避免过多的节点产生。预剪枝方法虽然简单但实用性不强，因为很难精确的判断何时终止树的生长。. * **后剪枝**：是在决策树构建完成之后，对那些置信度不达标的节点子树用叶子结点代替，该叶子结点的类标号用该节点子树中频率最高的类标记。后剪枝方法又分为两种:. 2. 使用同一数据集进行决策树生长和剪枝。常见的后剪枝方法有CCP(Cost Complexity Pruning)、REP(Reduced Error Pruning)、PEP(Pessimistic Error Pruning)、MEP(Minimum Error Pruning)。 **C4.5算法采用PEP**(Pessimistic Error Pruning)剪枝法。PEP剪枝法由Quinlan提出，是一种自上而下的剪枝法，根据剪枝前后的错误率来判定是否进行子树的修剪。**CART采用的是CCP**(Cost Complexity Pruning)的剪枝法策略。. ## 4.1 CART的CCP剪枝算法. **总体思路**：由完全树\\(T\\_0\\)开始，剪枝部分结点得到\\(T\\_1\\)，再次剪枝部分结点得到\\(T\\_2\\)...直到剩下树根的树\\(T\\_k\\)；在验证数据集上对这\\(k\\)个树分别评价，选择损失函数最小的树\\(T\\_a\\)。. **变量预定义**：\\(|T\\_{leaf}|\\)表示树\\(T\\)的叶结点个数，\\(t\\)表示树\\(T\\)的叶结点，同时，\\(N\\_t\\)表示该叶结点含有的样本点个数，其中属于\\(k\\)类的样本点有\\(N\\_{tk}\\)个，\\(K\\)表示类别的个数，\\(H\\_t(T)\\)为叶结点\\(t\\)上的经验熵，\\(\\alpha≥0\\)为参数。. \\[N\\_t=\\displaystyle\\sum^{K}\\_{k=1}{N\\_{tk}} \\]. \\[H\\_t(T)=-\\displaystyle \\sum\\_k^K \\frac {N\\_{tk}} {N\\_t} \\log \\frac {N\\_{tk}}{N\\_t} \\]. 经验熵反映了一个叶结点中的分类结果的混乱程度。经验熵越大，说明该叶结点所对应的分类结果越混乱，也就是说分类结果中包含了较多的类别，表明该分支的分类效果较差。. \\[C(T)=\\sum^{|T\\_{leaf}|}\\_{t=1} {N\\_tH\\_t(T)} \\]. 损失函数其实是求叶结点的经验熵期望。用\\(N\\_t\\)给经验熵加权的依据是叶子节点含有的样本个数越多，其分类效果或混乱程度越占主导，相当于求了期望，可以更好的描述分支前后的关系。例如设一个结点\\(r\\)有\\(n\\)个样本，其组成是第\\(i\\)类有\\(n\\_i\\)个样本，在分了几个孩子结点后各个叶结点的成分仍保持原比例，记新的子树为\\(R\\)，可以计算得出评价函数\\(C(r)=C(R)\\)，即在随机分组后不会有任何分类效果的改进。损失函数越小越好。熵的期望和熵一样，越小越好。所以，损失函数越大，说明模型的分类效果越差。. \\[C\\_\\alpha(T)=\\sum^{|T\\_{leaf}|}\\_{t=1} {N\\_tH\\_t(T)+\\alpha|T\\_{leaf}|} \\]. 修正项\\(\\alpha|T\\_{leaf}|\\)是基于复杂度的考虑。如上面提到的情况，\\(r\\)与\\(R\\)其实是一样的，没有进行任何分类的处理，但是我们仍然觉得\\(r\\)更好，原因在于\\(R\\)的复杂度更高。加了此修正项后具有现实的意义：如果\\(\\alpha=0\\)表示未剪枝的完全树损失更小（熵更小的占主导地位），如果\\(\\alpha->\\infty\\)表示剪枝到只剩根结点更好（叶结点个数占主导地位）。修正项\\(\\alpha|T\\_{leaf}|\\)可以避免过拟合。修正项\\(\\alpha|T\\_{leaf}|\\)考虑到了复杂度，\\(\\alpha\\)值设置得好可以避免出现完全树和根结点这类的极端情况，因此可以避免过拟合。. \\[C\\_{\\alpha}(T)=C(T)+\\alpha|T\\_{leaf}| \\]. 假定当前对以\\(t\\)为根的子树\\(T\\_t\\)剪枝，剪枝后只保留\\(t\\)本身而删掉所有的子结点。. 剪枝后的损失函数：\\(C\\_\\alpha(t)=C(t)+\\alpha\\). 剪枝前的损失函数：\\(C\\_\\alpha(T\\_t)=C(T\\_t)+\\alpha|T\\_{leaf}|\\)(\\(C(T\\_t)\\)应该是小于\\(C(t)\\)). 令二者相等，求得：\\(\\alpha=\\frac{C(t)-C(T)}{|T\\_{leaf}|-1}\\)，因为损失相同，那就取复杂度小的，所以就可以剪枝。\\(\\alpha\\)称为结点\\(t\\)的剪枝系数。. 3. 查找最小剪枝系数的结点，剪枝得决策树\\(T\\_k\\)。. 4. 重复以上步骤，直到决策树\\(T\\_k\\)只有一个结点。. 5. 得到决策树序列\\(T\\_0,T\\_1,T\\_2...T\\_k\\)。. 本文从：**特征选择、决策树的生成和决策树的剪枝**三个方面介绍了决策树的原理。并且也介绍了三种决策树的算法ID3、C4.5、CART。下文将比较ID3、C4.5、CART的异同点。. 3. 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。. 4. 有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。. posted @ 2018-10-01 16:52 hyc339408769 阅读(10212) 评论(0) 收藏) 举报).",
    "file_path": "unknown_source",
    "create_time": 1768975822,
    "update_time": 1768975822,
    "_id": "doc-a4d1d3bf1c5551379c10b7d809d654b0"
  },
  "doc-faa47dbbd25c1dde4b3c2e2c2052c496": {
    "content": "[DOC_ID: chunk-c15d98fe]\n[领域: 机器学习]\n## 作者. ## 什么是决策树？. 决策树学习采用分而治之的策略，通过执行贪心搜索来识别决策树中的最佳分割点。然后，以自上而下的递归方式重复此分割过程，直到所有或大多数记录都被归类到特定的类标签下。. 所有数据点是否都被归类为同质集合在很大程度上取决于决策树的复杂性。较小的决策树更容易获得纯净叶节点，即单个类中的数据点。然而，随着决策树规模的增大，维持这种纯度变得越来越困难，通常会出现指定子树内的数据过少的情况。这种情况被称为数据碎片，并且往往会造成过拟合。. 因此，决策树最好采用小树，这与奥卡姆剃刀中的简约原则一致；即，“如无必要，勿增实体”。换句话说，仅在必要时才增加决策树的复杂性，因为最简单的解释往往就是最佳解释。为了降低复杂性并防止过度拟合，决策树通常会采用修剪的方法；这是一个过程，用于去除依据重要性较低的特征分割出的分支。之后，可以通过交叉验证过程来评估模型的拟合度。. ## 决策树类型. 20 世纪 60 年代开发的 Hunt 算法用于在心理学中对人类学习进行建模，该算法成为了许多主流决策树算法的基础，比如以下算法：. **- ID3：**ID3 由 Ross Quinlan 提出，是“迭代二分法 3”的简称。该算法利用熵和信息增益作为评估候选分割的指标。Quinlan 自 1986 年以来对该算法的一些研究可在此处查看。. **- C4.5：**该算法被视为 ID3 的后期迭代版本，也是由 Quinlan 提出的。该算法可以使用信息增益或增益比来评估决策树中的分割点。. **- CART：**术语 CART 是“分类和回归树”的缩写，由 Leo Breiman 提出。此算法通常利用基尼系数来确定要分割的理想属性。基尼系数可衡量随机选择的属性被错误分类的频率。使用基尼系数进行评估时，值越低，表示越理想。. ### 专家为您带来最新的 AI 趋势. 获取有关最重要且最有趣的 AI 新闻的精选洞察分析。订阅我们的每周 Think 时事通讯。请参阅 IBM 隐私声明。. ### 谢谢！您已订阅。. 您的订阅将以英语提供。您会在每份时事通讯中找到一个取消订阅链接。您可以在此处管理您的订阅或取消订阅。有关更多信息，请参阅我们的 IBM 隐私声明。. ## 如何在每个节点上选择最佳属性. * p(c) 表示属于 c 类的数据点占集合 S 中总数据点数量的比例. 熵的值介于 0 和 1 之间。如果数据集 S 中的所有样本都属于一个类，则熵将等于零。如果一半的样本归类为一类，另一半样本归类为另一类，则熵将达到最高值 1。为了选择分割依据的最佳特征并找到最佳决策树，应使用熵量最小的属性。. * *Entropy(S)* 是数据集 S 的熵. Entropy (Tennis) = -(9/14) log2(9/14) – (5/14) log2 (5/14) = 0.94. Gain (Tennis, Humidity) = (0.94)-(7/14)\\*(0.985) – (7/14)\\*(0.592) = 0.151. ### 让 AI 服务于客户服务. 了解生成式 AI 如何提供更加无缝、令人满意的客户体验，并在以下三个关键领域提高组织的工作效率：自助服务、人工客服和联络中心运营。. 电子书 解锁生成式 AI + ML 的强大功能. 报告 下载《2024 年 AI 实际应用》. 我们对全球 2000 家公司的 AI 计划进行了调查。《2024 年 AI 实际应用》是一份数据丰富的报告，全面介绍了 AI 领军者的概况，并提供了专家评论，探讨从他们的早期成就中可以学到什么。. IBM Granite 是我们开放式、性能优异、值得信赖的 AI 模型系列，专门为企业量身定制，并经过优化，可以帮助您扩展 AI 应用程序。深入了解语言、代码、时间序列和护栏选项。. 报告 2024 年 AI 实际应用. 我们对 2,000 家组织进行了调查，旨在了解他们的 AI 计划，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。. 使用面向 AI 构建器的新一代企业级开发平台 IBM watsonx.ai，可以训练、验证、调整和部署生成式 AI、基础模型和机器学习功能。使用一小部分数据，即可在很短的时间内构建 AI 应用程序。. 通过增加 AI 重塑关键工作流程和运营，最大限度提升体验、实时决策和商业价值。. 一站式访问跨越 AI 开发生命周期的功能。利用用户友好型界面、工作流并访问行业标准 API 和 SDK，生成功能强大的 AI 解决方案。.",
    "file_path": "unknown_source",
    "create_time": 1768975947,
    "update_time": 1768975947,
    "_id": "doc-faa47dbbd25c1dde4b3c2e2c2052c496"
  },
  "doc-074ae094f32d31b781b0e74bdeb88703": {
    "content": "[DOC_ID: chunk-d191d067]\n[领域: 机器学习]\n# Your connection is not private. Attackers might be trying to steal your information from **www.showmeai.tech** (for example, passwords, messages, or credit cards). Learn more about this warning. net::ERR\\_CERT\\_DATE\\_INVALID. Issuer: TrustAsia DV TLS RSA CA 2025. Expires on: Nov 12, 2025. Current date: Jan 13, 2026. PEM encoded chain: -----BEGIN CERTIFICATE-----. 8mge5+SNcvEpMREbqAQKvH0+Ny8BcqULg+gLpQ+2+017bqW5F+hmed63fytKQnLB. R+ie+FyjeMP1CrVXwJU+vX8sA2fEQ7vokOAZDSKPuAs+gTDc4sUqTP3AXjJmgF9q. ON98vq5fPWZX2LFv7e5J6P9IHbzvOl8yyQjv+2/IOwhNSkaXX3bI+//bqF9XW/p7. Aid59UEBJyw/GibwXQ5xTyKD/N6C8SFkr1+myOo4oe1UB+YgvRu6qSxIABo5kYdX. ynZ7SbC03yR+gKZQDeTXrNP1kk5Qhe7jSXgw+nhbspe0q/M1ZcNCz+sPxeOwdCcC. This server could not prove that it is **www.showmeai.tech**; its security certificate expired 62 days ago. This may be caused by a misconfiguration or an attacker intercepting your connection. Your computer's clock is currently set to Tuesday, January 13, 2026. If not, you should correct your system's clock and then refresh this page. Proceed to www.showmeai.tech (unsafe).",
    "file_path": "unknown_source",
    "create_time": 1768976068,
    "update_time": 1768976068,
    "_id": "doc-074ae094f32d31b781b0e74bdeb88703"
  },
  "doc-c4744de0d06b9e7e6066f6751368db57": {
    "content": "[DOC_ID: chunk-f65fc231]\n[领域: 机器学习]\n第九讲： Bayes分类、熵、决策树、特征选择 徐玥珠 张锦岳 2020 年6 月24 日 目录 嬰嬮嬱 孂孡孹孥孳分类嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬱 嬰嬮嬱嬮嬱 代价函数 嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬱 嬰嬮嬱嬮嬲 孂孡孹孥孳公式嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬲 嬰嬮嬱嬮嬳 模型嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬲 嬰嬮嬲 熵嬨孅孮孴孲孯孰孹嬩嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬳 嬰嬮嬲嬮嬱 最大熵原则嬨孍孡學孩孭孵孭孅孮孴孲孯孰孹子孲孩孮季孩孰孬孥嬩嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬳 嬰嬮嬲嬮嬲 不确定性 嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬴 嬰嬮嬳 决策树嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬴 嬰嬮嬳嬮嬱 孉孄嬳决策树嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬴 嬰嬮嬳嬮嬲 孃嬴嬮嬵决策树嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬶 嬰嬮嬳嬮嬳 其他决策树嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬸 嬰嬮嬴 特征选择 嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮嬮 嬹 0.1 Bayes分类 0.1.1 代价函数 定义 将样本存分成孎类嬬即Y 嬽{c1, c2, ..., cN}嬬令λi,j是将一个ci的样本点學错分成cj的代价嬮定 义代价R嬨ci|x嬩嬽 N P j=1 P嬨ci|x嬩λi,j嬮进一步地嬬对于一个分类器h 嬺X →Y 嬬定义代价R嬨h嬩嬽EXR嬨h嬨X嬩|X嬩嬮贝 叶斯最优分类器目标即为找h∗嬽argminhR嬨h嬩嬮 例 取λi,j 嬽    嬰, i 嬽j 嬱, i ̸嬽j 嬬则R嬨ci|x嬩嬽P i̸=j P嬨cj|x嬩嬽嬱−P嬨ci|x嬩嬮 则h∗嬽argminh P i 嬱−P嬨ci|x嬩嬽argmax P i P嬨ci|x嬩嬮注意到嬬这与孄孎孎是一致的嬮 嬱 0.1.2 Bayes公式 P嬨x|y嬩嬽P嬨x, y嬩 P嬨y嬩 嬽P嬨y|x嬩 P嬨y嬩 嬨嬱嬩 注意到孂孡孹孥孳公式将子嬨學孼孹嬩和子嬨孹孼學嬩联系了起来嬬这是判别式模型和生成式模型相关联的关 键嬬这块内容会在下一节具体展开嬮 0.1.3 模型 嬱嬮判别式模型 根据子嬨季孼學嬩决定样本点學的归类结果嬬即若學最可能属于ci嬬则将學归入ci类嬮例如孄孎孎最 后经过孳孯学孴孭孡學层之后的输出即为落入每一类的可能性嬬利用可能性的大小决定分类结果嬬是判 别式模型嬮 嬲嬮生成式模型 由孂孡孹孥孳公式嬬P嬨c|x嬩嬽P (x,c) P (x) 嬽P (x|c)P (x) P (x) 嬮其中嬬子嬨學孼季嬩表示在季类中采样嬮而对 于已知的分布存嬬子嬨學嬩与季无关嬮故maxcP嬨c|x嬩问题等价于maxcP嬨x|c嬩P嬨c嬩问题嬬即为生成式模型嬮 例 考虑线性回归嬬样本点为嬨x1, y1嬩, 嬨x2, y2嬩, ..., 嬨xn, yn嬩嬬记作嬨學嬬孹嬩嬮yi 嬽aT xi 嬫ϵi嬬其中孡为参 数嬬则噪音ϵi 嬽y1 −aT xi嬮已知噪音ϵi ∼N嬨嬰, σ2嬩嬬学习参数孡嬮 由噪音的分布嬬我们有P嬨ϵi|嬨xi, yi嬩嬩∼exp嬨−ϵ2 2σ2 嬩嬮对于每个样本点xi嬬似然函数Li 嬽exp嬨−(yi−aT xi)2 2σ2 嬩嬮如 果没有其他的条件嬬我们无法求出P嬨孞 ϵ|嬨x, y嬩嬩嬮于是进一步地嬬假设學独立采样嬬这就是朴素贝叶斯 分类器嬨孎孡孩孶孥孂孡孹孥孳孃孬孡孳孳孩嬌孥孲嬩嬮 P嬨孞 ϵ|嬨x, y嬩嬩嬽 n Y i=1 P嬨ϵi|嬨xi, yi嬩嬩 嬨嬲嬩 取自然对数孬孮方便求最大值嬨孬孮是单调增函数嬩 lnL 嬽ln嬨P嬨孞 ϵ|嬨x, y嬩嬩嬩 嬽ln n Y i=1 exp嬨−嬨yi −aT xi嬩2 嬲σ2 嬩 嬽 n X i=1 −嬨yi −aT xi嬩2 嬲σ2 嬨嬳嬩 从生成式模型的角度嬬当先验分布子嬨孡嬩是均匀分布时 maxaP嬨x|a嬩⇐ ⇒maxaP嬨x|a嬩P嬨a嬩⇐ ⇒maxaL ⇐ ⇒mina n X i=1 嬨yi −aT xi嬩2 嬲 嬨嬴嬩 注意到嬬这与最小二乘是一致的嬬是从生成模型的角度对最小二乘的一种理解嬮 嬲 0.2 熵(Entropy) 0.2.1 最大熵原则(Maximum Entropy Principle) 考虑所有满足期望为嬴的不均匀的骰子{D1, D2, ...}嬮事件域为y 嬽{嬱, 嬲, 嬳, 嬴, 嬵, 嬶}嬮 实验嬺 将{D1, D2, ...}分别扔孎次嬨孎很大嬩嬮假设每一种结果y1, y2, ..., yN等可能出现嬮考虑孎次结果的分 布P 嬽{P1, P2, ..., Pm}嬬其中Pi是数字孩出现的概率嬬这里m 嬽嬶嬮孎中孩出现次数为Ni嬮则有 m X i=1 Ni 嬽N Ni N 嬽Pi m X i=1 Pivi 嬽 m X i=1 Pii 嬽嬖 v 嬽嬴 嬨嬵嬩 排列组合数目为 N!",
    "file_path": "unknown_source",
    "create_time": 1768976101,
    "update_time": 1768976101,
    "_id": "doc-c4744de0d06b9e7e6066f6751368db57"
  },
  "doc-2dd1a13b5c662bd9e747f8aa4b9d3361": {
    "content": "[DOC_ID: chunk-270416cd]\n[领域: 机器学习]\n最大熵模型(Maximum Entropy Model)是一种基于概率的方法，用于构建概率模型。它的核心思想是在给定一组约束条件的情况下，选择熵最大的概率分布作为模型。",
    "file_path": "unknown_source",
    "create_time": 1768976218,
    "update_time": 1768976218,
    "_id": "doc-2dd1a13b5c662bd9e747f8aa4b9d3361"
  },
  "doc-d6085544a08784bc3999c91c72c5594a": {
    "content": "[DOC_ID: chunk-4572c2c7]\n[领域: 机器学习]\n所以一般的分类问题，很少直接用最大熵模型。尤其是深度学习算法流行后，多层神经网络强大的拟合能力和优良的性能，使得我们在通常的分类问题中，几乎不会再使用最大熵模型了。",
    "file_path": "unknown_source",
    "create_time": 1768976233,
    "update_time": 1768976233,
    "_id": "doc-d6085544a08784bc3999c91c72c5594a"
  }
}
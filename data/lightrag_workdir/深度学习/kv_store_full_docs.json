{
  "doc-cb5e07f0bad1ec285e66b4df8be69531": {
    "content": "[DOC_ID: chunk-424249e7]\n[领域: 计算机科学]\n深度学习模型可以识别复杂的图片、文本和声音等数据模式，从而生成准确的见解和预测。神经网络是深度学习的底层技术。它由分层结构中的互连节点或神经元组成。节点在协调的",
    "file_path": "unknown_source",
    "create_time": 1769001774,
    "update_time": 1769001774,
    "_id": "doc-cb5e07f0bad1ec285e66b4df8be69531"
  },
  "doc-7fa0f1d079cce0d3f93ea8a1820caf82": {
    "content": "[DOC_ID: chunk-8f20a124]\n[领域: 计算机科学]\n前两天，我读到 Michael Nielsen 的开源教材《神经网络与深度学习》（Neural Networks and Deep Learning），意外发现里面的解释非常好懂。下面，我就按照这本书，介绍什么是神经网络。. 既然思考的基础是神经元，如果能够\"人造神经元\"（artificial neuron），就能组成人工神经网络，模拟思考。上个世纪六十年代，提出了最早的\"人造神经元\"模型，叫做\"感知器\"（perceptron），直到今天还在用。. 如果三个因素都为1，它们乘以权重的总和就是 8 + 4 + 4 = 16。如果天气和价格因素为1，同伴因素为0，总和就变为 8 + 0 + 4 = 12。. 上面公式中，`x`表示各种外部因素，`w`表示对应的权重。. > * 外部因素 `x1`、`x2`、`x3` 写成矢量 ，简写为 `x`. > * 权重 `w1`、`w2`、`w3` 也写成矢量 `(w1, w2, w3)`，简写为 `w`. > * 定义运算 `w⋅x = ∑ wx`，即 `w` 和 `x` 的点运算，等于因素与权重的乘积之和. > * 定义 `b` 等于负的阈值 `b = -threshold`. 其中，最困难的部分就是确定权重（`w`）和阈值（`b`）。目前为止，这两个值都是主观给出的，但现实中很难估计它们的值，必需有一种方法，可以找出答案。. 这种方法就是试错法。其他参数都不变，`w`（或`b`）的微小变动，记作`Δw`（或`Δb`），然后观察输出有什么变化。不断重复这个过程，直至得到对应最精确输出的那组`w`和`b`，就是我们要的值。这个过程称为模型的训练。. 找到一组已知答案的数据集，用来训练模型，估算`w`和`b`. 一旦新的数据产生，输入模型，就可以得到结果，同时对`w`和`b`进行校正. 这个例子里面，车牌照片就是输入，车牌号码就是输出，照片的清晰度可以设置权重（`w`）。然后，找到一种或多种图像比对算法，作为感知器。算法的得到结果是一个概率，比如75%的概率可以确定是数字`1`。这就需要设置一个阈值（`b`）（比如85%的可信度），低于这个门槛结果就无效。. 上面的模型有一个问题没有解决，按照假设，输出只有两种结果：0和1。但是，模型要求`w`或`b`的微小变化，会引发输出的变化。如果只输出`0`和`1`，未免也太不敏感了，无法保证训练的正确性，因此必须将\"输出\"改造成一个连续性函数。. 首先，将感知器的计算结果`wx + b`记为`z`。. 然后，计算下面的式子，将结果记为`σ(z)`。. > σ(z) = 1 / (1 + e^(-z)). 这是因为如果`z`趋向正无穷`z → +∞`（表示感知器强烈匹配），那么`σ(z) → 1`；如果`z`趋向负无穷`z → -∞`（表示感知器强烈不匹配），那么`σ(z) → 0`。也就是说，只要使用`σ(z)`当作输出结果，那么输出就会变成一个连续性函数。. 即`Δσ`和`Δw`和`Δb`之间是线性关系，变化率是偏导数。这就有利于精确推算出`w`和`b`的值了。. 该课程是纳米学位课程，学员提交的每一行代码都有导师 code review，并且每周可以预约导师一对一辅导，对于提高个人能力极有帮助。. 由于有真人 code review 环节，所以招生人数有限制，本期只有200个名额，目前已经预定了67个。点击这里了解详情，报名从速哦。. * **2019.01.28: Prolog 语言入门教程**. Prolog 是一种与众不同的语言，不用来开发软件，专门解决逻辑问题。比如，\"苏格拉底是人，人都会死，所以苏格拉底会死\"这一类的问题。. > 既然思考的基础是神经元，如果能够\"人造神经元\"（artificial neuron），就能组成人工神经网络，模拟思考。…. 对比王垠的论述(http://www.yinwang.org/blog-cn/2017/04/23/ai)：. 世界上这么多 AI 研究者，有几个真的研究过人脑，解刨过人脑，拿它做过实验，或者读过脑科学的研究成果？最后你发现，几乎没有 AI 研究者真正做过人脑或者认知科学的研究。著名的认知科学家 Douglas Hofstadter 早就在接受采访时指出，这帮所谓“AI 专家”，对人脑和意识（mind）是怎么工作的，其实完全不感兴趣，也从来没有深入研究过，却号称要实现“通用人工智能”（Artificial General Intelligence, AGI），这就是为什么 AI 直到今天都只是一个虚无的梦想。. > 世界上这么多 AI 研究者，有几个真的研究过人脑，解刨过人脑，拿它做过实验，或者读过脑科学的研究成果？最后你发现，几乎没有 AI 研究者真正做过人脑或者认知科学的研究。著名的认知科学家 Douglas Hofstadter 早就在接受采访时指出，这帮所谓“AI 专家”，对人脑和意识（mind）是怎么工作的，其实完全不感兴趣，也从来没有深入研究过，却号称要实现“通用人工智能”（Artificial General Intelligence, AGI），这就是为什么 AI 直到今天都只是一个虚无的梦想。. 一直在奋力追赶，从未遇见，不曾想到会在不同的时间于Neural networks and deep learning相遇。从α教程、ES6、如何变得有思想、黑客于画家、未来世界的幸存者...一路同行。. 重新深刻而清晰的学到 \"权重\" 和 \"阈值\" 的意义, 谢谢大师. > 对比王垠的论述(http://www.yinwang.org/blog-cn/2017/04/23/ai)：. > 世界上这么多 AI 研究者，有几个真的研究过人脑，解刨过人脑，拿它做过实验，或者读过脑科学的研究成果？最后你发现，几乎没有 AI 研究者真正做过人脑或者认知科学的研究。著名的认知科学家 Douglas Hofstadter 早就在接受采访时指出，这帮所谓“AI 专家”，对人脑和意识（mind）是怎么工作的，其实完全不感兴趣，也从来没有深入研究过，却号称要实现“通用人工智能”（Artificial General Intelligence, AGI），这就是为什么 AI 直到今天都只是一个虚无的梦想。.",
    "file_path": "unknown_source",
    "create_time": 1769001796,
    "update_time": 1769001796,
    "_id": "doc-7fa0f1d079cce0d3f93ea8a1820caf82"
  },
  "doc-6993ce492207f4f21e44d2611edc1e49": {
    "content": "[DOC_ID: chunk-65fe4226]\n[领域: 计算机科学]\n跳至主要内容 跳到 Ask Learn 聊天体验. 请升级到 Microsoft Edge 以使用最新的功能、安全更新和技术支持。. 下载 Microsoft Edge 有关 Internet Explorer 和 Microsoft Edge 的详细信息. #### 通过. Facebook x.com 共享 LinkedIn 电子邮件. 你当前正在访问 Microsoft Azure Global Edition 技术文档网站。 如果需要访问由世纪互联运营的 Microsoft Azure 中国技术文档网站，请访问 。. # Azure 机器学习中的深度学习与机器学习. 本文将深度学习与机器学习进行比较，并介绍了它们如何适应更广泛的 AI 类别。 了解可基于 Azure 机器学习构建的深度学习解决方案，如欺诈检测、语音和面部识别、情绪分析及时序预测。. Azure 机器学习中的 Foundry 模型是预先训练的深度学习模型，可针对特定用例进行微调。 有关详细信息，请参阅探索 Microsoft Foundry 模型在 Azure 机器学习中的应用和如何使用由 Azure 机器学习策划的开源基础模型。. ## 深度学习、机器学习和 AI. * 深度学习是机器学习的一个子集，它基于人工神经网络。学习过程之所以具有深度，是因为人工神经网络的结构包含多个输入层、输出层和隐藏层。每一层都包含若干单元，这些单元会将输入数据转换为下一层可用于特定预测任务的信息。 由于此结构，计算机可以通过自己的数据处理来学习。. * **机器学习** 是人工智能的一部分，它使用技术（如深度学习），使计算机能够使用体验来提高其执行任务的能力。 学习过程由以下步骤组成：. * **生成 AI** 是 AI 的子集，它使用技术（如深度学习）来生成新内容。 例如，可以使用生成式 AI 创建图像、文本或音频。 这些模型使用大量预先训练的知识来生成此内容。. ## 深度学习和机器学习的技术. 现在，你已基本了解机器学习与深度学习的区别，接下来让我们比较这两种技术。 在机器学习中，需要通过使用更多信息来告知算法如何进行准确的预测。 （例如，通过执行特征提取。在深度学习中，该算法可以了解如何通过自己的数据处理进行准确的预测，因为它使用人工神经网络结构。. | **硬件依赖项** | 可在低端计算机上工作。 不需要大量的计算能力。 | 依赖于高端计算机。 它本质上会执行大量的矩阵乘法运算。 GPU 可以有效地优化这些运算。 |. | **学习方法** | 将学习过程划分为多个更小的步骤。 然后，将每个步骤的结果合并成一个输出。 | 通过端到端地解决问题来完成学习过程。 |. | **训练时间** | 训练耗时相对较短，范围从几秒到几小时不等。 | 通常训练耗时较长，因为深度学习算法包含多个层级。 |. | **输出** | 输出通常是一个数值，例如评分或分类。 | 输出可以具有多种格式，如文本、分数或声音。 |. ## 什么是迁移学习？. 训练深度学习模型通常需要大量训练数据、高端计算资源（GPU、TPU）和较长的训练时间。 如果没有这些内容，可以使用称为*“转移学习*”的技术来快捷方式训练过程。. 由于神经网络的结构，第一组层通常包含较低级别特征，而最后一组层则包含更贴近所讨论领域的更高级别特征。 通过重新调整最终层的用途，以用于新的领域或问题，可显著减少训练新模型所需的时间、数据和计算资源。 例如，如果你已经有一个识别汽车的模型，则可以通过使用转移学习来重新调整该模型，以识别卡车、摩托车和其他种类的车辆。. 若要了解如何在 Azure 机器学习中使用开源框架为图像分类应用传输学习，请参阅 使用转移学习训练深度学习 PyTorch 模型。. 以下段落将介绍深度学习的一些最常见应用场景。 在 Azure 机器学习中，可以使用从开源框架生成的模型，也可以使用提供的工具生成模型。. 生成对抗网络是为创建真实内容（如映像）而训练的生成模型。 它们由两个名为 *生成器* 和 *判别器* 的网络组成。 这两个网络同时进行训练。 在训练过程中，生成器使用随机噪音来创建新合成数据（与真实数据非常相似）。 鉴别器将生成器的输出作为输入，并使用实际数据来确定生成的内容是真实内容还是合成内容。 每个网络都与另一个网络竞争。 生成器正尝试生成与真实内容无法区分的合成内容，而判别器正在尝试将输入正确分类为真实内容或合成内容。 这一输出随后将被用来更新两个网络的权重，帮助它们更好地实现各自的目标。. 转换器是用于解决包含序列的问题（如文本或时序数据）的模型体系结构。 它们包含编码器层和解码器层#Encoder)。 编码器接受输入，并将其映射到包含上下文等信息的数值表示形式。 解码器使用编码器中的信息生成输出，例如已翻译文本。 使转换器不同于包含编码器和解码器的其他体系结构的是关注子层。 *注意* 是指基于其上下文相对于序列中其他输入的重要性关注输入的特定部分。 例如，当模型汇总新闻文章时，并非所有句子都与描述主理念相关。 通过把重点放在文章的关键词上，总结可以用一句话来完成，即标题。. * Bidirectional Encoder Representations from Transformers (BERT).",
    "file_path": "unknown_source",
    "create_time": 1769001910,
    "update_time": 1769001910,
    "_id": "doc-6993ce492207f4f21e44d2611edc1e49"
  },
  "doc-250e3986b4dd98f48693c0cff1f6d083": {
    "content": "[DOC_ID: chunk-2b6b9c22]\n[领域: 计算机科学]\n虽然人工智能 (AI)、机器学习 (ML)、深度学习和神经网络是相关技术，但这些术语经常被交替使用，这经常导致人们混淆它们之间的区别。这篇博客文章将澄清一些模糊之处。. ## AI 、机器学习、深度学习和神经网络之间有何关联？. 理解 AI、机器学习、深度学习和神经网络的最简单方法是将它们视为从大到小排序的一系列 AI 系统，且系统间为包含关系。. AI 是统领性系统。机器学习是 AI 的一个子集。深度学习是机器学习的一个子领域，而神经网络是深度学习算法的基础。区分是单个神经网络还是深度学习算法的是神经网络的节点层数或深度，深度学习算法必须超过三层。. ## 什么是人工智能 (AI)？. 人工智能 (AI) 是这三个术语中最广泛的术语，用于对模仿人类智能和人类认知功能（如解决问题和学习）的机器进行分类。AI 利用预测和自动化来优化和解决历来由人类完成的复杂任务，例如面部和语音识别、决策和翻译。. ### **AI 类别**. ### **利用 AI 开展业务**. 关键在于从一开始就确定正确的数据集，以确保使用高质量数据，实现最大的竞争优势。您还需要创建一个混合 AI 就绪型架构，以便能够成功地使用位于任何地方的数据，包括大型机、数据中心、私有云和公有云以及边缘。. 机器学习是 AI 的一个子集，可以进行优化。如果设置正确，它可以帮助您做出预测，从而最大限度地减少仅因猜测而产生的错误。例如，Amazon 等公司使用机器学习 ，根据特定客户看过的内容和之前购买过的产品，向其推荐产品。. 虽然被称为深度机器学习的 AI 子集可以在监督学习中充分利用标记数据集为算法提供信息，但它并不一定需要已标记数据集。它可以采集非结构化数据的原始形式（例如文本、图像），并且可以自动确定区分“披萨”、“汉堡”和“墨西哥卷饼”的一组特征。随着我们生成更多的大数据，数据科学家会使用更多的机器学习。要深入了解这些方法之间的区别，请查看《监督学习与无监督学习：有什么区别？》. ### 成为 AI 专家. 训练数据能够教导神经网络，并有助于随时间推移提高其准确性。对学习算法进行微调后，它们就会成为强大的计算机科学和 AI 工具，因为它们能让我们快速对数据进行分类和聚类。利用神经网络，语音和图像识别任务可以在数分钟内完成，而人工识别则需要数小时。Google 的搜索算法是是神经网络的一个著名示例。. ## 管理您的 AI 数据. 虽然 AI 的这些领域可以帮助简化您的业务流程并改善客户体验，但实现 AI 目标可能充满挑战，因为您首先需要确保拥有合适的系统来构建学习算法，从而管理您的数据。数据管理不仅仅是构建企业使用的模型。在开始构建之前，您需要一个存储数据的地方，以及清理数据和控制偏差的机制。. ## IBM、机器学习和 AI. IBM Data and AI Team. 了解如何选择正确的方法来准备数据集和使用 AI 模型，如何使用模型选择框架来平衡性能要求与成本、风险、部署需求和利益相关者要求。. 与 IBM 携手参与网络研讨会，在此期间我们将展示如何通过智能体 AI 计划实现真正的投资回报率，并提供跨行业、用例的示例，甚至还有 IBM 自身的成功案例。. 报告 2024 年 AI 实际应用. 我们对 2,000 家组织进行了调查，旨在了解他们的 AI 计划，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。. 报告 从推行 AI 项目到实现盈利：agentic AI 如何维持财务回报. 了解组织如何从分散的 AI 试点，转向在核心业务中利用 AI 推动转型。. AI 模型 深入了解 IBM Granite. 本课程由 IBM 资深思想领袖带领，旨在帮助企业领导者获得所需的知识，以便划分可以推动增长的 AI 投资的优先级。. 报告 2024 年 AI 实际应用. 我们对 2,000 家组织进行了调查，旨在了解他们的 AI 计划，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。. 激活这五种思维跃迁，以消除不确定性，推动业务重塑，并利用智能体式 AI 促进增长。. 指南 让 AI 充分发挥作用：利用生成式 AI 提高投资回报率. 想要从 AI 投资中获得更好的回报吗？了解如何通过帮助您最优秀的人才构建和提供创新的新解决方案，在关键领域扩展生成式人工智能来推动变革。. 电子书 解锁生成式 AI + ML 的强大功能. 指南 树立信任，从容自信在 AI 新时代蓬勃发展. 使用面向 AI 构建器的新一代企业级开发平台 IBM watsonx.ai，可以训练、验证、调整和部署生成式 AI、基础模型和机器学习功能。使用一小部分数据，即可在很短的时间内构建 AI 应用程序。. 借助 IBM 业界领先的 AI 专业知识和解决方案组合，让 AI 在您的业务中发挥作用。. IBM Consulting AI 服务有助于重塑企业利用 AI 实现转型的方式。.",
    "file_path": "unknown_source",
    "create_time": 1769001988,
    "update_time": 1769001988,
    "_id": "doc-250e3986b4dd98f48693c0cff1f6d083"
  },
  "doc-cb25bd59eb20459d90d689a472568aea": {
    "content": "[DOC_ID: chunk-ab0b2d35]\n[领域: 计算机科学]\n如需详细了解人工智能和机器学习如何为您的业务提供帮助，请在此处详细了解 Google Cloud 的 AI 和机器学习产品及解决方案。. 新客户最高可获享 $300 赠金，用于试用 Vertex AI 和其他 Google Cloud 产品。. | | 不固定。简单的 AI 可在基础硬件上运行。 | 通常可在标准 CPU 上运行，但复杂模型更依赖计算能力。 | 由于需要进行大规模并行计算，通常依赖高性能计算资源，特别是 GPU 或 TPU，以实现高效训练。 |. 获享 $300 赠金以及 20 多种提供“始终免费”用量的产品，开始在 Google Cloud 上构建项目。. * 适用于 Google Cloud 的 Gemini. * 将 Oracle 工作负载迁移到 Google Cloud. * SQL Server on Google Cloud. 用于在 Google Cloud 上运行 SQL Server 虚拟机的各种方案。. * Red Hat on Google Cloud. Google 和 Red Hat 为传统的本地应用和自定义应用提供了一个企业级平台。. 适用于 MySQL、PostgreSQL 和 SQL Server 的关系型数据库服务。. 以全代管式方式持续交付到 GKE 和 Cloud Run。. 以全代管式方式持续交付到 GKE 和 Cloud Run。. AI 赋能的助理，可在 Google Cloud 和 IDE 中使用。. * Google Distributed Cloud Air-gapped. 通过 Google Cloud 上的关联 Fitbit 数据全方位了解患者。. Google Cloud 上的 API 部署和开发管理服务。. 用于管理 Google Cloud 资源的 Kubernetes 插件。. 将您的 VMware 工作负载迁移到 Google Cloud 并继续以原生方式运行。. 管理 Google Cloud 审核日志、平台日志和应用日志。. * Google Cloud 的随用随付价格方案会根据预付费资源的每月用量和折扣费率自动为您节省费用。请立即联系我们，获取报价。. * 适用于 Google Cloud 的 Gemini. * 将 Oracle 工作负载迁移到 Google Cloud. * SQL Server on Google Cloud. * Red Hat on Google Cloud. * Google Distributed Cloud Air-gapped.",
    "file_path": "unknown_source",
    "create_time": 1769002067,
    "update_time": 1769002067,
    "_id": "doc-cb25bd59eb20459d90d689a472568aea"
  },
  "doc-9fcf6610f361d14578f6d58fe5b5ea26": {
    "content": "[DOC_ID: chunk-0b442e5f]\n[领域: 计算机科学]\n反向传播利用动态规划的思想来高效地计算梯度。动态规划是一种通过将问题分解为子问题并存储其结果以避免重复计算的技术。反向传播正是通过从输出层向输入层逐层计算误差，",
    "file_path": "unknown_source",
    "create_time": 1769002155,
    "update_time": 1769002155,
    "_id": "doc-9fcf6610f361d14578f6d58fe5b5ea26"
  },
  "doc-09961ed14d51e2b7bb5974e42ff8dedd": {
    "content": "[DOC_ID: chunk-29e5074e]\n[领域: 计算机科学]\n其中\\(\\boldsymbol{W}^{(1)} \\in \\mathbb{R}^{h \\times d}\\)是隐藏层的权重参数。把中间变量\\(\\boldsymbol{z} \\in \\mathbb{R}^h\\)输入按元素运算的激活函数\\(\\phi\\)后，将得到向量长度为\\(h\\)的隐藏层变量. 隐藏层变量\\(\\boldsymbol{h}\\)也是一个中间变量。假设输出层参数只有权重\\(\\boldsymbol{W}^{(2)} \\in \\mathbb{R}^{q \\times h}\\)，可以得到向量长度为\\(q\\)的输出层变量. \\[s = \\frac{\\lambda}{2} \\left(\\|\\boldsymbol{W}^{(1)}\\|\\_F^2 + \\|\\boldsymbol{W}^{(2)}\\|\\_F^2\\right),\\]. 回顾一下本节中样例模型，它的参数是\\(\\boldsymbol{W}^{(1)}\\)和\\(\\boldsymbol{W}^{(2)}\\)，因此反向传播的目标是计算\\(\\partial J/\\partial \\boldsymbol{W}^{(1)}\\)和\\(\\partial J/\\partial \\boldsymbol{W}^{(2)}\\)。我们将应用链式法则依次计算各中间变量和参数的梯度，其计算次序与前向传播中相应中间变量的计算次序恰恰相反。首先，分别计算目标函数\\(J=L+s\\)有关损失项\\(L\\)和正则项\\(s\\)的梯度. 其次，依据链式法则计算目标函数有关输出层变量的梯度\\(\\partial J/\\partial \\boldsymbol{o} \\in \\mathbb{R}^q\\)：. \\[\\frac{\\partial J}{\\partial \\boldsymbol{o}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\boldsymbol{o}}\\right) = \\frac{\\partial L}{\\partial \\boldsymbol{o}}.\\]. 现在，我们可以计算最靠近输出层的模型参数的梯度\\(\\partial J/\\partial \\boldsymbol{W}^{(2)} \\in \\mathbb{R}^{q \\times h}\\)。依据链式法则，得到. \\[\\frac{\\partial J}{\\partial \\boldsymbol{W}^{(2)}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{o}}, \\frac{\\partial \\boldsymbol{o}}{\\partial \\boldsymbol{W}^{(2)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\boldsymbol{W}^{(2)}}\\right) = \\frac{\\partial J}{\\partial \\boldsymbol{o}} \\boldsymbol{h}^\\top + \\lambda \\boldsymbol{W}^{(2)}.\\]. 沿着输出层向隐藏层继续反向传播，隐藏层变量的梯度\\(\\partial J/\\partial \\boldsymbol{h} \\in \\mathbb{R}^h\\)可以这样计算：. \\[\\frac{\\partial J}{\\partial \\boldsymbol{h}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{o}}, \\frac{\\partial \\boldsymbol{o}}{\\partial \\boldsymbol{h}}\\right) = {\\boldsymbol{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\boldsymbol{o}}.\\]. 由于激活函数\\(\\phi\\)是按元素运算的，中间变量\\(\\boldsymbol{z}\\)的梯度\\(\\partial J/\\partial \\boldsymbol{z} \\in \\mathbb{R}^h\\)的计算需要使用按元素乘法符\\(\\odot\\)：. \\[\\frac{\\partial J}{\\partial \\boldsymbol{z}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{h}}, \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}}\\right) = \\frac{\\partial J}{\\partial \\boldsymbol{h}} \\odot \\phi'\\left(\\boldsymbol{z}\\right).\\]. 最终，我们可以得到最靠近输入层的模型参数的梯度\\(\\partial J/\\partial \\boldsymbol{W}^{(1)} \\in \\mathbb{R}^{h \\times d}\\)。依据链式法则，得到. \\[\\frac{\\partial J}{\\partial \\boldsymbol{W}^{(1)}} = \\text{prod}\\left(\\frac{\\partial J}{\\partial \\boldsymbol{z}}, \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}^{(1)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\boldsymbol{W}^{(1)}}\\right) = \\frac{\\partial J}{\\partial \\boldsymbol{z}} \\boldsymbol{x}^\\top + \\lambda \\boldsymbol{W}^{(1)}.\\]. 另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。举例来说，参数梯度\\(\\partial J/\\partial \\boldsymbol{W}^{(2)} = (\\partial J / \\partial \\boldsymbol{o}) \\boldsymbol{h}^\\top + \\lambda \\boldsymbol{W}^{(2)}\\)的计算需要依赖隐藏层变量的当前值\\(\\boldsymbol{h}\\)。这个当前值是通过从输入层到输出层的正向传播计算并存储得到的。.",
    "file_path": "unknown_source",
    "create_time": 1769002176,
    "update_time": 1769002176,
    "_id": "doc-09961ed14d51e2b7bb5974e42ff8dedd"
  },
  "doc-e3f063f3b413fbd20b9634ee1bc86c59": {
    "content": "[DOC_ID: chunk-38f599cc]\n[领域: 计算机科学]\n# 3-3 反向传播的深入理解. 反向传播（Back Propagation）是深度学习中的核心算法，几乎是所有神经网络训练学习的基础。对它的认知十分重要，但初学者往往会陷入两个极端：要么只浅显的明白是误差的传递，但对于为什么倒推及怎么计算不熟悉；要么陷入数学公式和细节，只会一个个例子却看不清全貌。能否既深刻理解，又好记不忘呢？. “哪怕是小小的蝴蝶扇动一下翅膀，也能造成千里之外的飓风”，这就是混沌理论中著名的蝴蝶效应（Butterfly Effect）。它描述了在一个动态系统中，初始条件的微小变化，却能带动整个系统长期且巨大的链式反应。影片讲述的是具有能穿越过去超能力的主人公埃文生命过程中的蝴蝶效应。每次回到过去，一个小小的改变却像蝴蝶震动了翅膀，带来故事情节的急速闪变。前一个悲剧消失，却引发另外的悲剧、更大的变化，验证了蝴蝶之羽翼的轻轻摆动足以引发人生的海啸。奇幻的时空交错，思维的不断重组。在感慨人生的同时，你有没有好奇，主人公埃文为什么要一次一次的穿越，而每次穿越后结果又各不尽相同呢？. 无论神经网络，还是混沌现象，都是典型的非线性动力学系统（Nonlinear Dynamical System）。简单的说就是跟求导相关的系统，通常用微分方程或差分方程才能描述其动力学特性。根据国际著名非线性动力学家、美国科学院院士Strogatz教授的分类，用动力学的角度看世界的话，如下图所示：. 其中横轴是系统变量维度，纵轴是非线性程度。我们可以看到混沌理论（Chaos）在中间，是三维非线性；著名的三体问题（3-body problem）也是这个维度的；而神经网络则是更高维的非线性动力学系统。. 这类系统的复杂性来源于非线性造成的不稳定性和对初始条件的极度敏感。通常一个系统都会受到各种小的扰动，非线性则会放大这些扰动。以混沌为例，系统一方面对于初始条件依赖，另一方面又在有限范围内运动，这使得那些初始状态和速度充分接近的轨道会以指数速度分离开来。由于轨道自己不能相交 ，所以它们只能在有限的空间内缠绕往复而形成非常复杂的形状，这就是所谓的“混沌”。电影中的每次穿越其实都是一条新的人生轨道。. 对神经网络而言，正常情况下，它的信息流都是前向传播的。如下图所示：一个由两个隐藏层六个神经元组成的网路，\\(f\\)表示神经元的激活函数。输入数据经过加权线性组合，激活函数实现非线性变换，在不同层级间反复迭代，得到最后的输出。就如同人生，一路向前不可逆转。网络中的每条链路组合都形成了一条人生轨道。. 电影脑洞大开，想象了一种奇幻的场景：男主埃文拥有超能力，当面对理想与现实的巨大落差时，可以穿越回过去重写命运。深度学习中，反向传播算法的思想可谓与电影构思异曲同工。通常神经网络的训练往往以随机参数开始，初始输出结果一般都不尽人意。因此，学习的目的就是赋予它穿越的力量，使其能够不断更新自我，最终实现完美的效果。. 电影中，20岁的埃文每次穿越都选择最惨的关系入手。先是因为女友凯丽的自杀，第二次是抑郁的好友兰尼，最后是为了患癌症的妈妈。穿越时间也分为13岁和7岁两个层级。通过分解问题找到的事发原因，所有的故事情节都会随之发生新的变化。这个过程是不是像极了有两个隐藏层、多个神经元的网络？不同的人物对应不同的神经元，层级对应时间。. 在这种类比下，误差损失函数描述的就是对生活的不满意程度。比如，通常我们可以用理想值和实际输入的均方差来表示，其中f是激活函数，z是最终结果的标准值，N是训练样本个数。. \\[E =\\frac{1}{2N} \\Sigma\\_x\\|f\\_w(x)-z\\|^2\\]. \\[\\mathbf{min}\\_w \\frac{1}{2N}\\Sigma\\_x\\|f\\_w(x)-z\\|^2\\]. 在深度学习中，这可以用梯度下降法（Gradient Descent）来实现。具体说，就是根据梯度逐步更新权重参数：. \\[w^+=w-\\eta\\cdot\\frac{\\partial E}{\\partial w}\\]. 其中\\(\\eta\\)是学习率，一般选择0-1之间的小数。求导\\(\\partial E/\\partial w\\)则体现了网路的非线性动力学特性，类比电影就是每次穿越都要先找最惨。换句话说，在损失函数确定的情况下，反向传播最难的就是求导了。. 正如可怜之人必有可恨之处，找最惨要像剥洋葱一样一层层的进行。对损失函数这样的复合函数，不同层间连续穿越称为求导的链式法则。具体来说，可以分解为两步实现：. 首先，按网络关系把误差乘以每条链上的权重。下图以输出神经元\\(f\\_6\\)为例，进行误差分解。. 如果同一个神经元有多条链的话，就汇聚求和。例如下图所示神经元\\(f\\_1\\)，. 其次，获得每个神经元的误差后，再根据各自的梯度公式，逐个更新链接的权重。每个神经元的梯度可以根据复合函数的链式求导法则获得：. \\[\\begin{split}\\begin{equation} \\begin{aligned} \\frac{\\partial E}{\\partial w\\_i}&= \\frac{\\partial (\\Sigma\\_x\\|f(x)-z\\|^2)}{2N \\cdot \\partial w\\_i}\\\\ &=(f\\_i(x)-z)\\frac{\\partial f\\_i(x)}{\\partial w\\_i}\\\\ &=\\delta\\_i \\cdot\\frac{\\partial f\\_i(e)}{\\partial e}\\cdot \\frac{\\partial e}{\\partial w\\_i}\\\\ &=\\delta\\_i \\cdot\\frac{\\partial f\\_i(e)}{\\partial e}\\cdot x\\_i \\end{aligned} \\label{f2} \\end{equation}\\end{split}\\]. 其中\\(x\\_i\\)表示第i号神经元的输入，\\(e\\_i=w\\_ix\\_i+b\\_i\\)是该神经元的输入线性组合，\\(f\\_i\\)是它对应的激活函数。让我们以其中的\\(f\\_4\\)神经元为例，看看它是如何调整输入权重的：. 如果喂给神经网络一个训练样本，就更新一次权重，这被称为随机梯度下降法。如果把所有的样本都喂给神经网络并计算误差后，再统一调整权重，这叫批量梯度下降法。当然，还有一小批一小批喂的方式，每次调整一下权重，叫做小批量梯度下降法。. 总得来说，神经网络的反向传播，就如同电影中人生不断地穿越重写。通过成千上万个训练样本一遍遍的自我救赎，最终实现对完美结果的执着追求。. 科学和艺术，看似不想干的两个领域，往往却有着深层次的逻辑联系。这也是为什么博士学位往往被称为Doctor of Philosophy的原因了。想要学好机器学习，在看似枯燥的理论和算法背后，多思考其中的道理。毕竟人工智能，研究的还是人生的智能。通过恰当的类比，更容易深刻理解算法设计的初衷。. NONLINEAR DYNAMICS AND CHAOS with Applications to Physics, Biology, Chemistry, and Engineering (second edition), Boca Raton, FL: CRC Press, 2018. [2] 盛昭瀚，马军海，“非线性动力系统分析引论”， 科学出版社， 2001.",
    "file_path": "unknown_source",
    "create_time": 1769002258,
    "update_time": 1769002258,
    "_id": "doc-e3f063f3b413fbd20b9634ee1bc86c59"
  },
  "doc-7844fb38c827827af06d614fe4afb4df": {
    "content": "[DOC_ID: chunk-f89c5726]\n[领域: 计算机科学]\n如上图所示，反向传播的计算顺序是，将信号 E 乘以节点的局部导数 ∂ y ∂ x ，然后将结果传递给下一个节点。这里所说的局部导数是指正向传播中 y = f ( x ) 的导数，也就是 y",
    "file_path": "unknown_source",
    "create_time": 1769002338,
    "update_time": 1769002338,
    "_id": "doc-7844fb38c827827af06d614fe4afb4df"
  },
  "doc-10c76eb7b6589378a533aee8633d6ad4": {
    "content": "[DOC_ID: chunk-652cb8bb]\n[领域: 计算机科学]\n反向传播是一种机器学习技术，对优化神经网络至关重要。它有助于使用梯度下降算法更新网络权重，这就是深度学习模型推动现代人工智能 (AI) “学习”的方式。. 这三个相互交织的流程（其一为用于跟踪不同输入中模型误差的损失函数，其二为反向传播该误差以了解网络的不同部分是如何导致该误差的，其三则是可相应调整模型权重的梯度下降算法）便是深度学习模型的“学习”方式。因此，从最基本的多层感知器到用于生成式 AI 的复杂深度神经网络架构，反向传播都是训练神经网络模型的基础所在。. ### 专家为您带来最新的 AI 趋势. 获取有关最重要且最有趣的 AI 新闻的精选洞察分析。订阅我们的每周 Think 时事通讯。请参阅 IBM 隐私声明。. * **双曲正切**（或 **tanh**）函数，它可将输入映射到介于 -1 和 1 之间的值。. * **softmax** 函数，它可将输入向量转换为元素范围介于 0 和 1 之间且总和为 1 的向量。. 假设一个神经网络的第二层有 3 个输入节点（*a*、*b* 和 *c*），输入层的隐藏单元 *z* 具有 *tanh* 激活函数和偏置项 *t*。输入节点和节点 *z* 之间的每个连接都具有唯一的权重 *w。*我们可以使用简化的方程 *z* = *tanh* ( *wa z \\*a + w bz* \\*b + *w cz \\* c* *+ t* ) *来描述节点 z* *将传递给下一层神经元的输出值。*. 为了说明反向传播的效率，Michael Nielsen 在其在线教科书《神经⽹络与深度学习》中，将此算法与计算神经网络损失函数梯度的一种简单直观的替代方法进行了比较*。*. 抽象地说，反向传播的目的是训练神经网络通过监督学习做出更优质的预测。而更根本的是，反向传播的目标是确定如何调整模型权重和偏差，从而最小化由“损失函数”衡量的误差*。*. * 数据会流经*隐藏层*，而每个层均会逐步提取关键特征，直到到达*输出层*。. **“损失函数”、“成本函数”还是“误差函数”？** 须注意的是，在某些情况下，*成本函数*或*误差函数*一词会用于代替*损失函数*，即用“成本”或“误差”来代替“损失*”。*. 回到前面的分类器模型示例，我们将从最后一层的 5 个神经元开始，我们将其称为 *L* 层。每个输出神经元的 softmax 值表示输入属于其类别的可能性（满分为 1）。在完美训练的模型中，代表正确分类的神经元的输出值接近 1，其他神经元的输出值接近 0。. 现在，我们会重点介绍表示正确预测的输出单元，而我们会将其称为 *Lc。L*c 的激活函数是一个复合函数，其中包含整个神经网络从输入层到输出层的众多嵌套激活函数。最小化损失函数需在整个网络中进行调整，以使 *L*c的激活函数的输出更接近于 1。. *L*c 的激活函数的输出取决于它从倒数第二层（我们称之为 *L-1* 层）的神经元接收到的贡献。改变 *L* c 输出的方法之一是改变 *L-1*和*L* c 中神经元之间的权重。通过计算每个 *L-1* 权重相对于其他权重的偏导数，我们可以看到增加或减少其中任何一个权重将如何使 *L*c的输出更接近（或更远离）1。. IBM Granite 是我们开放式、性能优异、值得信赖的 AI 模型系列，专门为企业量身定制，并经过优化，可以帮助您扩展 AI 应用程序。深入了解语言、代码、时间序列和护栏选项。. 报告 2024 年 AI 实际应用. 我们对 2,000 家组织进行了调查，旨在了解他们的 AI 计划，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。. 指南 面向 CEO 的生成式 AI 指南. 了解 CEOs 如何在生成式 AI 所能创造的价值与其所需的投资和带来的风险之间取得平衡。. 指南 让 AI 充分发挥作用：利用生成式 AI 提高投资回报率. 想要从 AI 投资中获得更好的回报吗？了解如何通过帮助您最优秀的人才构建和提供创新的新解决方案，在关键领域扩展生成式人工智能来推动变革。. 使用面向 AI 构建器的新一代企业级开发平台 IBM watsonx.ai，可以训练、验证、调整和部署生成式 AI、基础模型和机器学习功能。使用一小部分数据，即可在很短的时间内构建 AI 应用程序。. 通过增加 AI 重塑关键工作流程和运营，最大限度提升体验、实时决策和商业价值。. 一站式访问跨越 AI 开发生命周期的功能。利用用户友好型界面、工作流并访问行业标准 API 和 SDK，生成功能强大的 AI 解决方案。.",
    "file_path": "unknown_source",
    "create_time": 1769002360,
    "update_time": 1769002360,
    "_id": "doc-10c76eb7b6589378a533aee8633d6ad4"
  },
  "doc-a729e7ced2028bbfeb4754b6a72084b3": {
    "content": "[DOC_ID: chunk-96381a94]\n[领域: 计算机科学]\n前向传播、反向传播和计算图¶ Open the notebook in Colab Open the notebook in Colab Open the notebook in Colab Open the notebook in Colab Open the notebook in SageMaker Studio Lab. 我们已经学习了如何用小批量随机梯度下降训练模型。 然而当实现该算法时，我们只考虑了通过*前向传播*（forward propagation）所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。. 梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。 本节将通过一些基本的数学和计算图， 深入探讨*反向传播*的细节。 首先，我们将重点放在带权重衰减（\\(L\\_2\\)正则化）的单隐藏层多层感知机上。. *前向传播*（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。. 我们将一步步研究单隐藏层神经网络的机制， 为了简单起见，我们假设输入样本是 \\(\\mathbf{x}\\in \\mathbb{R}^d\\)， 并且我们的隐藏层不包括偏置项。 这里的中间变量是：. 隐藏变量\\(\\mathbf{h}\\)也是一个中间变量。 假设输出层的参数只有权重\\(\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}\\)， 我们可以得到输出层变量，它是一个长度为\\(q\\)的向量：. (4.7.3)¶\\[\\mathbf{o}= \\mathbf{W}^{(2)} \\mathbf{h}.\\]. 假设损失函数为\\(l\\)，样本标签为\\(y\\)，我们可以计算单个数据样本的损失项，. (4.7.4)¶\\[L = l(\\mathbf{o}, y).\\]. 根据\\(L\\_2\\)正则化的定义，给定超参数\\(\\lambda\\)，正则化项为. (4.7.5)¶\\[s = \\frac{\\lambda}{2} \\left(\\|\\mathbf{W}^{(1)}\\|\\_F^2 + \\|\\mathbf{W}^{(2)}\\|\\_F^2\\right),\\]. 其中矩阵的Frobenius范数是将矩阵展平为向量后应用的\\(L\\_2\\)范数。 最后，模型在给定数据样本上的正则化损失为：. (4.7.6)¶\\[J = L + s.\\]. 在下面的讨论中，我们将\\(J\\)称为*目标函数*（objective function）。. 绘制*计算图*有助于我们可视化计算中操作符和变量的依赖关系。 图4.7.1 是与上述简单网络相对应的计算图， 其中正方形表示变量，圆圈表示操作符。 左下角表示输入，右上角表示输出。 注意显示数据流的箭头方向主要是向右和向上的。. *反向传播*（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的*链式规则*，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 假设我们有函数\\(\\mathsf{Y}=f(\\mathsf{X})\\)和\\(\\mathsf{Z}=g(\\mathsf{Y})\\)， 其中输入和输出\\(\\mathsf{X}, \\mathsf{Y}, \\mathsf{Z}\\)是任意形状的张量。 利用链式法则，我们可以计算\\(\\mathsf{Z}\\)关于\\(\\mathsf{X}\\)的导数. 在这里，我们使用\\(\\text{prod}\\)运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。 对于向量，这很简单，它只是矩阵-矩阵乘法。 对于高维张量，我们使用适当的对应项。 运算符\\(\\text{prod}\\)指代了所有的这些符号。. 回想一下，在计算图 图4.7.1中的单隐藏层简单网络的参数是 \\(\\mathbf{W}^{(1)}\\)和\\(\\mathbf{W}^{(2)}\\)。 反向传播的目的是计算梯度\\(\\partial J/\\partial \\mathbf{W}^{(1)}\\)和 \\(\\partial J/\\partial \\mathbf{W}^{(2)}\\)。 为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。 计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数\\(J=L+s\\)相对于损失项\\(L\\)和正则项\\(s\\)的梯度。. (4.7.8)¶\\[\\frac{\\partial J}{\\partial L} = 1 \\; \\text{and} \\; \\frac{\\partial J}{\\partial s} = 1.\\]. (4.7.10)¶\\[\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)} \\; \\text{and} \\; \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}.\\]. 现在我们可以计算最接近输出层的模型参数的梯度 \\(\\partial J/\\partial \\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}\\)。 使用链式法则得出：. 以上述简单网络为例：一方面，在前向传播期间计算正则项 (4.7.5)取决于模型参数\\(\\mathbf{W}^{(1)}\\)和 \\(\\mathbf{W}^{(2)}\\)的当前值。 它们是由优化算法根据最近迭代的反向传播给出的。 另一方面，反向传播期间参数 (4.7.11)的梯度计算， 取决于由前向传播给出的隐藏变量\\(\\mathbf{h}\\)的当前值。. 1. 假设一些标量函数\\(\\mathbf{X}\\)的输入\\(\\mathbf{X}\\)是\\(n \\times m\\)矩阵。\\(f\\)相对于\\(\\mathbf{X}\\)的梯度维数是多少？.",
    "file_path": "unknown_source",
    "create_time": 1769002470,
    "update_time": 1769002470,
    "_id": "doc-a729e7ced2028bbfeb4754b6a72084b3"
  },
  "doc-92d9fc2fcb5c3187adc0d3da64c15ae8": {
    "content": "[DOC_ID: chunk-75be8312]\n[领域: 计算机科学]\n最新推荐文章于 2025-12-17 10:46:47 发布. 于 2019-05-15 11:37:33 发布. CC 4.0 BY-SA版权. 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。. + - CASE 1（图示讲解，看不太懂没关系，看第二组图）. - CASE 2（具体计算举例，嫌麻烦的可直接看这个，强烈推荐！！！！！）. 如果去问一下了解BP算法的人“**BP算法怎推导？**”，大概率得到的回答是“不就是**链式求导法则**嘛”，我觉得这种答案对于提问题的人来说没有任何帮助。BP的推导需要**链式求导**不错，但提问者往往想得到的是直观的回答，毕竟理解才是王道。直观的答案，非**图解**莫属了。. 注：下图的确是反向传播算法，但不是深度学习中的backprop，不过backward的大体思想是一样的，毕竟**误差**没法从前往后计算啊。（在深度学习中操作的是**计算图**—Computational graph），如果暂时不理解上面那句话，你可以当我没说过，不要紧~（手动?）. 下面通过**两组图**来进行神经网络**前向传播**和**反向传播**算法的讲解，**第一组图**来自国外某网站，配图生动形象。如果对你来说，单纯的讲解理解起来比较费劲，那么可以参考**第二组图**——一个具体的前向传播和反向传播算法的例子。通过本篇博客，相信就算是刚刚入门的小白（只要有一点点高等数学基础知识），也**一定可以理解反向传播算法！**. #### CASE 1（图示讲解，看不太懂没关系，看第二组图）. 每个神经元由两部分组成，第一部分（e）是**输入值**和**权重系数**乘积的**和**，第二部分（f(e)）是一个**激活函数**（非线性函数）的输出， y=f(e)即为某个神经元的输出，如下：. 下面开始利用反向传播的误差，计算各个神经元（权重）的导数，开始反向传播修改权重（When the error signal for each neuron is computed, the weights coefficients of each neuron input node may be modified. \\dfrac {df\\left( e\\right) }{de}. dedf(e)​ represents derivative of neuron activation function (which weights are modified). #### CASE 2（具体计算举例，嫌麻烦的可直接看这个，强烈推荐！！！！！）. 首先明确，**“正向传播”求损失，“反向传播”回传误差**。同时，神经网络每层的每个神经元都可以**根据误差信号修正每层的权重**，只要能明确上面两点，那么下面的例子，只要会一点**链式求导**规则，就一定能看懂！. 我们先来求最简单的，求误差E对w5的导数。首先明确这是一个“**链式求导**”过程，要求误差E对w5的导数，需要先求误差E对out o1的导数，再求out o1对net o1的导数，最后再求net o1对w5的导数，经过这个**链式法则**，我们就可以求出误差E对w5的导数（偏导），如下图所示：. http://galaxy.agh.edu.pl/~vlsi/AI/backp\\_t\\_en/backprop.html. https://www.cnblogs.com/charlotte77/p/5629865.html. 需要注意的是，*反向传播*并不是一个单一的*算法*，而是一种通用的思想，可以适用于各种神经网络结构和优化*算法*。*反向传播*（*Backpropagation*）是一种用于训练神经网络的*算法*，主要用于计算网络中每个参数对于损失函数的梯度，从而通过梯度下降法来更新网络参数，使得神经网络能够逐步适应训练数据。*反向传播*的关键是计算每个参数的梯度，这些梯度告诉我们如何调整参数，以使网络的输出更接近真实标签。使用计算得到的梯度，按照梯度下降法或其变种更新神经网络的参数，使损失函数逐步减小。. *反向传播**算法**(**过程*及*公式**推导**)*\\_*反向传播**算法**(**过程*及*公式**推导**)*. *公式*1*(*计算最后一层神经网络产生的错误*)*: 其中, 表示Hadamard乘积,用于矩阵或向量之间点对点的乘法运算。 *公式*1的*推导**过程*如下: *公式*2*(*由后往前,计算每一层神经网络产生的错误*)*: *推导**过程*: *公式*3*(*计算权重的梯度*)*: *推导**过程*: *公式*4*(*计算偏置的梯度*)*: *推导**过程*: 4. Proof of Back Propagation Algorithm.pdf. 前向传播*算法**(*Forward propagation*)*与*反向传播**算法**(*Back propagation*)*以... *反向传播*的工作*过程*以及*公式**推导*\\_陈唯源. 前向传播 对于节点 来说, 的净输入 如下: 接着对 做一个sigmoid函数得到节点 的输出: 类似的,我们能得到节点 、 、 的输出 、 、 。 误差 得到结果后,整个神经网络的输出误差可以表示为: 其中 就是刚刚通过前向传播算出来的 、 ; 是节点 、. *反向传播**算法*（*Backpropagation*）是目前用来训练人工神经网络（Artificial Neural Network，ANN）的最常用且最有效的*算法*。. W*(*l*)*ij：表示第l−1层的第j个特征到第l层第i个神经元的权值. RNN *反向传播**公式**推导**(*非矩阵式*)*\\_rnn*反向传播*详细*推导*. 深度学习：完全理解*反向传播**算法*（一）. 《深度学习系列》*反向传播**算法*的*公式**推导*\\_深度学习 反向算子 推算-CSDN... *反向传播**算法*及其梯度扩散 前言 最近开始认真学习了下*反向传播**算法*和梯度传递的问题,其本质是导数的链式法则的应用,由此可以分析出为什么sigmoid激活函数不适合用于做多层网络的激活函数,可以考虑联系我的另一篇关于激活函数的文章。如有谬误,请联系指正。转载请注明出处。. *反向传播**算法*的*直观*理解. 点击上方，选择星标或置顶，每天给你送干货！阅读大概需要5分钟跟随小博主，每天进步一丢丢作者：陈楠来源：知乎整理：*机器学习**算法*与自然语言处理公众号链接：https://zhuanlan.z... 【AI深究】CNN中的*反向传播*中的卷积梯度*推导*（*Backpropagation* in Convolutional Layer）——全网最详细全流程详解与案例（附详尽Python代码演示）|大量数学*公式*. 本篇延续AI深究专栏风格，系统梳理卷积神经网络（CNN）中卷积层*反向传播*（*Backpropagation* in Convolutional Layer）的数学*推导*、核心原理、*公式*、实际案例和可视化代码演示，帮助你彻底理解CNN训练的“灵魂机制”。. 首先已知，这个是我们定义的，不用*推导*，但是为什么要这样定义呢？. [[深度学习] *反向传播*的四个基本*公式*证明及*算法*流程](https://devpress.csdn.net/v1/article/detail/75095504). 最后我们得到*反向传播**算法*的*算法*流程：. 一次性搞*懂**反向传播*（*Backpropagation*）原理. \\delta$是上游传来的梯度$\\partial L/\\partial z$，$x$是该层的输入向量。偏置项$b$的梯度直接等于$\\delta$。*反向传播*是训练神经网络的核心*算法*，通过计算损失函数对网络参数的梯度，利用梯度下降优化网络权重。关键在于高效计算梯度，避免重复计算。$\\Delta Z$是上游梯度矩阵。其中$L$是损失函数，$z$是当前层的输出。$\\eta$是学习率。更复杂的优化器（如Adam）会引入动量、自适应学习率等机制。这些梯度会与上游梯度相乘后继续*反向传播*。. 一文彻底搞*懂*深度学习 - *反向传播*（Back Propagation）. AI大模型作为人工智能领域的重要技术突破，正成为推动各行各业创新和转型的关键力量。抓住AI大模型的风口，掌握AI大模型的知识和技能将变得越来越重要。学习AI大模型是一个系统的*过程*，需要从基础开始，逐步深入到更高级的技术。这里给大家精心整理了一份全面的AI大模型学习资源，包括：AI大模型全套学习路线图（从入门到实战）、精品AI大模型学习书籍手册、视频教程、实战学习、面试题等，资料免费分享！.",
    "file_path": "unknown_source",
    "create_time": 1769002554,
    "update_time": 1769002554,
    "_id": "doc-92d9fc2fcb5c3187adc0d3da64c15ae8"
  },
  "doc-b0c9962abc15c97c8cc9a40c8f13e070": {
    "content": "[DOC_ID: chunk-615fc181]\n[领域: 计算机科学]\n更新隐含层的权重系数 92 for in range(len(self.hidden_layer.neurons)): 93 for in range(len(self.hidden_layer.neurons[h].weights)): 94 95 # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ 96 self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih) 97 98 # Δw = α * ∂Eⱼ/∂wᵢ 99 pd_error_wrt_weight 100 101 def calculate_total_error(self, training_sets): 102 0 103 for in range(len(training_sets)): 104 training_sets[t] 105 self.feed_forward(training_inputs) 106 for in range(len(training_outputs)): 107 self.output_layer.neurons[o].calculate_error(training_outputs[o]) 108 return total_error 109 110 class NeuronLayer: 111 def __init__(self, num_neurons, bias): 112 113 # 同一层的神经元共享一个截距项b 114 if else random.random() 115 116 [] 117 for in range(num_neurons): 118 self.neurons.append(Neuron(self.bias)) 119 120 def inspect(self): 121 print 'Neurons: ', len(self.neurons)) 122 for in range(len(self.neurons)): 123 print ' Neuron ', n) 124 for in range(len(self.neurons[n].weights)): 125 print ' Weight: ', self.neurons[n].weights[w]) 126 print ' Bias: ', self.bias) 127 128 def feed_forward(self, inputs): 129 [] 130 for in self.neurons: 131 outputs.append(neuron.calculate_output(inputs)) 132 return outputs 133 134 def get_outputs(self): 135 [] 136 for in self.neurons: 137 outputs.append(neuron.output) 138 return outputs 139 140 class Neuron: 141 def __init__(self, bias): 142 bias 143 [] 144 145 def calculate_output(self, inputs): 146 inputs 147 self.squash(self.calculate_total_net_input()) 148 return self.output 149 150 def calculate_total_net_input(self): 151 0 152 for in range(len(self.inputs)): 153 self.weights[i] 154 return self.bias 155 156 # 激活函数sigmoid 157 def squash(self, total_net_input): 158 returntotal_net_input)) 159 160 161 def calculate_pd_error_wrt_total_net_input(self, target_output): 162 return self.calculate_pd_total_net_input_wrt_input(); 163 164 # 每一个神经元的误差是由平方差公式计算的 165 def calculate_error(self, target_output): 166 return 167 168 169 def calculate_pd_error_wrt_output(self, target_output): 170 return self.output) 171 172 173 def calculate_pd_total_net_input_wrt_input(self): 174 return self.output) 175 176 177 def calculate_pd_total_net_input_wrt_weight(self, index): 178 return self.inputs[index] 179 180 181 # 文中的例子: 182 183) 184 for in): 185]) 186 print)) 187 188 189 #另外一个例子，可以把上面的例子注释掉再运行一下: 190 191 # training_sets = [192 # [[0, 0], [0]], 193 # [[0, 1], [1]], 194 # [[1, 0], [1]], 195 # [[1, 1], [0]] 196 # ] 197 198 # nn = NeuralNetwork(len(training_sets[0][0]), 5, len(training_sets[0][1])) 199 # for i in range(10000): 200 # training_inputs, training_outputs = random.choice(training_sets) 201 # nn.train(training_inputs, training_outputs) 202 # print(i, nn.calculate_total_error(training_sets)).",
    "file_path": "unknown_source",
    "create_time": 1769002668,
    "update_time": 1769002668,
    "_id": "doc-b0c9962abc15c97c8cc9a40c8f13e070"
  },
  "doc-d22adf07bd762e4e32d9bceea2fa6218": {
    "content": "[DOC_ID: chunk-d4a5d2de]\n[领域: 计算机科学]\n... 深度学习，发现哪种方法适合您的具体应用，并探索您可能会遇到的一些挑战。 在本视频中探索卷积神经网络（也称为CNN或ConvNets）的基础。 您将学习3个",
    "file_path": "unknown_source",
    "create_time": 1769002720,
    "update_time": 1769002720,
    "_id": "doc-d22adf07bd762e4e32d9bceea2fa6218"
  },
  "doc-37f0a7ef7d63be3d8878c173058865c3": {
    "content": "[DOC_ID: chunk-637f05cd]\n[领域: 计算机科学]\n在前面的章节中，我们遇到过图像数据。 这种数据的每个样本都由一个二维像素网格组成， 每个像素可能是一个或者多个数值，取决于是黑白还是彩色图像。 到目前为止，我们处理这类结构丰富的数据的方式还不够有效。 我们仅仅通过将图像数据展平成一维向量而忽略了每个图像的空间结构信息，再将数据送入一个全连接的多层感知机中。 因为这些网络特征元素的顺序是不变的，因此最优的结果是利用先验知识，即利用相近像素之间的相互关联性，从图像数据中学习得到有效的模型。. 本章介绍的*卷积神经网络*（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。 基于卷积神经网络架构的模型在计算机视觉领域中已经占主导地位，当今几乎所有的图像识别、目标检测或语义分割相关的学术竞赛和商业应用都以这种方法为基础。. 现代卷积神经网络的设计得益于生物学、群论和一系列的补充实验。 卷积神经网络需要的参数少于全连接架构的网络，而且卷积也很容易用GPU并行计算。 因此卷积神经网络除了能够高效地采样从而获得精确的模型，还能够高效地计算。 久而久之，从业人员越来越多地使用卷积神经网络。即使在通常使用循环神经网络的一维序列结构任务上（例如音频、文本和时间序列分析），卷积神经网络也越来越受欢迎。 通过对卷积神经网络一些巧妙的调整，也使它们在图结构数据和推荐系统中发挥作用。. 在本章的开始，我们将介绍构成所有卷积网络主干的基本元素。 这包括卷积层本身、填充（padding）和步幅（stride）的基本细节、用于在相邻区域汇聚信息的汇聚层（pooling）、在每一层中多通道（channel）的使用，以及有关现代卷积网络架构的仔细讨论。 在本章的最后，我们将介绍一个完整的、可运行的LeNet模型：这是第一个成功应用的卷积神经网络，比现代深度学习兴起时间还要早。 在下一章中，我们将深入研究一些流行的、相对较新的卷积神经网络架构的完整实现，这些网络架构涵盖了现代从业者通常使用的大多数经典技术。.",
    "file_path": "unknown_source",
    "create_time": 1769002735,
    "update_time": 1769002735,
    "_id": "doc-37f0a7ef7d63be3d8878c173058865c3"
  },
  "doc-4748f90dd65ccac3d7f0e4da7f935778": {
    "content": "[DOC_ID: chunk-4896491a]\n[领域: 计算机科学]\n已于 2023-09-25 19:15:35 修改. CC 4.0 BY-SA版权. 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。. 于 2023-09-24 21:27:05 首次发布. ## 引言：. 卷积神经网络（Convolutional Neural Network，CNN）是一种在计算机视觉领域取得了巨大成功的深度学习模型。它们的设计灵感来自于生物学中的视觉系统，旨在模拟人类视觉处理的方式。在过去的几年中，CNN已经在图像识别、目标检测、图像生成和许多其他领域取得了显著的进展，成为了计算机视觉和深度学习研究的重要组成部分。在了解卷积神经网络前，还不知道什么是神经网络的同学可以看我的这篇博客：深入了解神经网络：构建人工智能的基石-CSDN博客. ## 一、图像原理. 图像在计算机中是一堆按顺序排列的数字，数值为0到255。0表示最暗，255表示最亮。 如下图：. 上图是只有黑白颜色的灰度图，而更普遍的图片表达方式是RGB颜色模型，即红、绿、蓝三原色的色光以不同的比例相加，以产生多种多样的色光。RGB颜色模型中，单个矩阵就扩展成了有序排列的三个矩阵，也可以用三维张量去理解。. 其中的每一个矩阵又叫这个图片的一个channel（通道），宽, 高, 深来描述。. ## 二、为什么要学习卷积神经网络？. 在传统神经网络中，我们要识别下图红色框中的图像时，我们很可能识别不出来，因为这六张图的位置都不通，计算机无法分辨出他们其实是一种形状或物体。. 我们希望一个物体不管在画面左侧还是右侧，都会被识别为同一物体，这一特点就是不变性。为了实现平移不变性，卷积神经网络（CNN）等深度学习模型在卷积层中使用了卷积操作，这个操作可以捕捉到图像中的局部特征而不受其位置的影响。. ## 三、什么是卷积？. 在卷积神经网络中，卷积操作是指将一个可移动的小窗口（称为数据窗口，如下图绿色矩形）与图像进行逐元素相乘然后相加的操作。这个小窗口其实是一组固定的权重，它可以被看作是一个特定的滤波器（filter）或**卷积核**。这个操作的名称“卷积”，源自于这种元素级相乘和求和的过程。这一操作是卷积神经网络名字的来源。. 上图这个绿色小窗就是数据窗口。简而言之，卷积操作就是用一个可移动的小窗口来提取图像中的特征，这个小窗口包含了一组特定的权重，通过与图像的不同位置进行卷积操作，网络能够学习并捕捉到不同特征的信息。文字解释可能太难懂，下面直接上动图：. 这张图中蓝色的框就是指一个数据窗口，红色框为卷积核（滤波器），最后得到的绿色方形就是卷积的结果（数据窗口中的数据与卷积核逐个元素相乘再求和）. ### 一张图带你了解卷积计算过程：. ### 卷积需要注意哪些问题？. c. 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。. #### 为什么要进行数据填充：. 现在，我们要应用一个 3x3 的卷积核进行卷积操作，步幅（stride）为 1，且要使用填充（padding）为 1。如果不使用填充，卷积核的中心将无法对齐到输入图像的边缘，导致输出特征图尺寸变小。假设我们使用步幅（stride）为 1 进行卷积，那么在不使用填充的情况下，输出特征图的尺寸将是 2x2。. 所以我们要在它的周围填充一圈0，填充为 1 意味着在输入图像的周围添加一圈零值。添加填充后的图像：. 现在，我们将 3x3 的卷积核应用于这个填充后的输入图像，计算卷积结果，得到大小不变的特征图。. ### 卷积神经网络的模型是什么样的？. ## 四、卷积神经网络的构造. #### 1 输入层. 输入层接收原始图像数据。图像通常由三个颜色通道（红、绿、蓝）组成，形成一个二维矩阵，表示像素的强度值。. #### 2 卷积和激活. 卷积层将输入图像与卷积核进行卷积操作。然后，通过应用激活函数（如ReLU）来引入非线性。这一步使网络能够学习复杂的特征。. #### 3 池化层. #### 4 多层堆叠. #### 5 全连接和输出. ## 五、图片经过卷积后的样子. 这里的*神经网络*，也指人工*神经网络*（Artificial Neural Networks，简称ANNs），是一种模仿生物*神经网络*行为特征的算法数学模型，由神经元、节点与节点之间的连接（突触）所构成，如下图：. 每个*神经网络*单元抽象出来的数学模型如下，也叫感知器，它接收多个输入（x1，x2，x3…），产生一个输出，这就好比是神经末梢感受各种外部环境的变化（外部刺激），然后产生电信号，以便于转导到神经细胞（又叫神经元）。. 自动驾驶、智能医疗保健和自助零售这些领域直到最近还被认为是不可能实现的，而计算机视觉已经帮助我们达到了这些事情。如今，拥有自动驾驶汽车或自动杂货店的梦想听起来不再那么遥不可及了。事实上，我们每天都在使用计算机视觉——当我们用面部解锁手机或在社交媒体上发照片前使用自动修图。*卷积神经网络*可能是这一巨大成功背后最关键的构建模块。这一次，我们将加深理解*神经网络*如何工作于*CNN*s。出于建议，这篇文章将包括相当复杂的数学方程，如果你不习惯线性代数和微分，请不要气馁。. *CNN*笔记:通俗理解*卷积神经网络*\\_*cnn**卷积神经网络*. *CNN*笔记:通俗理解*卷积神经网络* 本文通过生动的例子和直观的比喻,详细解析了*卷积神经网络*的基本*原理*,包括神经元、多层*神经网络*、卷积操作的过程以及ReLU激活函数和池化层的作用。 该文章已生成可运行项目,预览并下载项目源码 前言 2012年我在北京组织过8期machine learning读书会,那时“机器学习”非常火,很多人都对其抱有... [[*深度学习*概念]·*CNN**卷积神经网络**原理*分析\\_*卷积神经网络**原理*解读 csdn...](https://blog.csdn.net/xiaosongshine/article/details/86560279). 2.1 二维卷积层 *卷积神经网络**(*convolutional neural network*)*是含有卷积层*(*convolutional layer*)*的*神经网络*。本章中介绍的*卷积神经网络*均使用最常见的二维卷积层。它有高和宽两个空间维度,常用来处理图像数据。本节中,我们将介绍简单形式的二维卷积层的工作*原理*。. *卷积神经网络*（*CNN*）的*原理*与应用. *卷积神经网络*是计算机视觉和图像处理领域的关键技术之一，已经在许多实际问题中取得了巨大成功。通过本讲义，你将深入了解*CNN*的工作*原理*、架构设计和应用方法，并能够使用*深度学习*框架构建自己的*CNN*模型。希望这个讲义能够为你提供坚实的*CNN*知识基础。. 【*深度学习*】*一文**搞懂**卷积神经网络*（*CNN*）的*原理*（*超详细*）\\_*卷积神经网络**原理*-CSDN博客.pdf. 【*深度学习*】*一文**搞懂**卷积神经网络*（*CNN*）的*原理*（*超详细*）\\_*卷积神经网络**原理*-CSDN博客.pdf. *深度学习*--*卷积神经网络*工作*原理*\\_*卷积神经网络**原理*详解. 从知乎上看一篇文章,问题是*卷积神经网络*工作*原理*直观的解释,知乎大神用各种动图对*卷积神经网络*的训练做了介绍。YJango的回答最为精彩。奉上链接,希望你也能从这个回答中对*卷积神经网络*有更为确切的认识。https://www.zhihu.com/question/39022858/answer/194996805? utm\\_medium=social&utm\\_source=wechat\\_session&from=... *卷积神经网络**(**CNN**)**原理*,详解*卷积神经网络*是如何卷的? *卷积神经网络**(*ConvolutionalNeural Network,简称*CNN**)*,是*深度学习**神经网络*经典形式之一,由于其计算过程中包含卷积运算,因此得名。*卷积神经网络**(**CNN**)*通过使用卷积层来提取图像数据的局部特征,再通过池化层*(*PoolingLayer*)*来降低特征的空间维度,最后通过全连接层*(*Fully Connected Layer*)*进行分类或回归任务。*CNN*已经在图像识别、目... 4大*CNN*模型详解+实战代码（从基础*CNN*到Mask R-*CNN*），小白必看！. 基础*CNN*：分类入门，结构简单，适合纯分类任务。SSD：单阶段检测，速度快，适合实时场景（如视频监控）。Faster R-*CNN*：双阶段检测，精度高，适合精准检测（如医疗影像）。Mask R-*CNN*：全能选手，检测+分割，适合复杂场景（如工业质检、自动驾驶）。实战项目无需手动处理数据和训练，小白可直接跑通，生成的可视化图表和数据能帮助快速理解模型差异。后续可根据实际需求选择合适的模型，或进行进一步优化！原文 资料 这里！. *(*完整版*)**一文*读懂*卷积神经网络**CNN*.pdf. *(*完整版*)**一文*读懂*卷积神经网络**CNN*.pdf*(*完整版*)**一文*读懂*卷积神经网络**CNN*.pdf. *深度学习*之*卷积神经网络**原理**(**cnn**)*\\_卷积平摊. 对图像*(*不同的窗口数据*)*和卷积核*(*一组固定的权重:因为每个神经元的多个权重固定,所以又可以看做一个恒定的滤波器filter*)*做内积*(*逐个元素相乘再求和*)*的操作就是所谓的『卷积』操作,也是*卷积神经网络*的名字来源。 卷积层 卷积操作存在的问题? *卷积神经网络*（*CNN*）的*原理*（*超详细*）. 本文系统阐述了*卷积神经网络**(**CNN**)*的核心*原理*及其在计算机视觉中的应用。文章首先解析了图像的数字化表示方法，指出*CNN*通过卷积操作克服了传统*神经网络*在图像处理中的局限性，实现了平移不变性。重点介绍了卷积操作机制，包括滤波器滑动、乘积累加等关键过程，以及滤波器尺寸、步长、填充等参数设置。详细剖析了*CNN*的层级架构，从输入层、卷积层、池化层到全连接层，分析了各层功能及其协同工作机制。最后阐述了*CNN*通过层级化特征提取实现从基础视觉元素到高级语义理解的特征抽象过程，揭示了其在计算机视觉领域取得成功的核心机制。. 【*深度学习*】5：*CNN**卷积神经网络**原理*. *卷积神经网络*由一个或多个卷积层、池化层以及全连接层等组成。与其他*深度学习*结构相比，*卷积神经网络*在图像等方面能够给出更好的结果。这一模型也可以使用反向传播算法进行训练。相比较其他浅层或深度*神经网络*，*卷积神经网络*需要考量的参数更少，使之成为一种颇具吸引力的*深度学习*结构。. *卷积神经网络**(**CNN**)*--*原理*详细解读（通俗易懂）. 在*人工智能*的浪潮中，*卷积神经网络*（*CNN*）以其卓越的性能在图像识别、视频处理等领域大放异彩。它通过模拟人脑视觉系统，利用卷积层、池化层等结构，从数据中自动提取并学习特征，实现高效且准确的分类与识别。本文旨在以简洁明了的方式，揭开*CNN*的神秘面纱，解析其工作*原理*，让小伙伴们快速了解这一前沿技术如何推动图像处理的进步，并激发对*深度学习*领域的兴趣与探索。定义。. 【*深度学习*系列】*卷积神经网络**CNN**原理*详解*(*一*)*——基本*原理*. 感谢关注天善智能，走好数据之路↑↑↑欢迎关注天善智能，我们是专注于商业智能BI，*人工智能*AI，大数据分析与挖掘领域的垂直社区，学习，问答、求职一站式搞定！对商业智能BI、大数据分析挖掘、机器学习，python，R等数据领域感兴趣的同学加微信：tstoutiao，邀请你进入数据爱好者交流群，数据爱好者们都在这儿。上篇文章我们给出了用paddlepaddle来做手写数字识别的... 本文来自于腾讯云，全文阐述了*卷积神经网络*的基本结构和*原理*，希望对您的学习有帮助。先明确一点就是，DeepLearning是全部*深度学习*算法的总称，*CNN*是*深度学习*算法在图像处理领域的一个应用。第一点，在学习Deeplearning和*CNN*之前，总以为它们是很了不得的知识，总以为它们能解决很多问题，学习了之后，才知道它们不过与其他机器学习算法如svm等相似，仍然可以把它当做一个分类器，仍然可以像使用一个黑盒子那样使用它。第二点，DeepLearning强大的地方就是可以利用网络中间某一层的输出当做是数据的另一种表达，从而可以将其认为是经过网络学习到的特征。基于该特征，可以进行进一步的相似度比较等. 本ppt详细介绍了*卷积神经网络*的起源背景、算法*原理*、算法的执行过程、以及*CNN*的应用场景. 有关*卷积神经网络*，在训练方面的论文。都是我自己在知网上下载的，都是一些硕士论文，对于想要了解*卷积神经网络*是如何训练与识别方面的理论知识比较有用，希望能帮到初学*卷积神经网络*的人。这些都是CAJ文档哦，打开要下载CAJ工具。. 详细解读了*卷积神经网络*是如何工作的，从*CNN*卷积层、激活层、池化层到全链接层，及多层*CNN*作用进行了通熟易懂的讲解. *一文*读懂*卷积神经网络**CNN*.docx. *卷积神经网络*（*CNN*，Convolutional Neural Networks）是*深度学习*领域中的一种核心模型，尤其在图像识别、计算机视觉和自然语言处理等任务上表现出色。*CNN*的设计灵感来源于生物视觉系统，尤其是Hubel和Wiesel两位科学... *一文**搞懂**卷积神经网络*（*CNN*）的*原理*（*超详细*）. 它们的设计灵感来自于生物学中的视觉系统，旨在模拟人类视觉处理的方式。简而言之，卷积操作就是用一个可移动的小窗口来提取图像中的特征，这个小窗口包含了一组特定的权重，通过与图像的不同位置进行卷积操作，网络能够学习并捕捉到不同特征的信息。上图是只有黑白颜色的灰度图，而更普遍的图片表达方式是RGB颜色模型，即红、绿、蓝三原色的色光以不同的比例相加，以产生多种多样的色光。这张图中蓝色的框就是指一个数据窗口，红色框为卷积核（滤波器），最后得到的绿色方形就是卷积的结果（数据窗口中的数据与卷积核逐个元素相乘再求和）. * 工作时间 8:30-22:00.",
    "file_path": "unknown_source",
    "create_time": 1769002789,
    "update_time": 1769002789,
    "_id": "doc-4748f90dd65ccac3d7f0e4da7f935778"
  },
  "doc-c79065b95e8586e85c1d4a6dfcdb5d37": {
    "content": "[DOC_ID: chunk-5198d991]\n[领域: 计算机科学]\n有两种具备不同架构的主要深度学习系统类型：卷积神经网络（CNN）和循环神经网络（RNN）。 CNN 架构. CNN 有三个层组：. 卷积层使用预配置的筛选条件从输入的数据中提取信息。",
    "file_path": "unknown_source",
    "create_time": 1769002960,
    "update_time": 1769002960,
    "_id": "doc-c79065b95e8586e85c1d4a6dfcdb5d37"
  },
  "doc-c79b805e7da972f5a80b0f9eb1144d75": {
    "content": "[DOC_ID: chunk-3772c747]\n[领域: 计算机科学]\n卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表",
    "file_path": "unknown_source",
    "create_time": 1769002981,
    "update_time": 1769002981,
    "_id": "doc-c79b805e7da972f5a80b0f9eb1144d75"
  },
  "doc-225c135f952fba89e32fc67dcf80d0f5": {
    "content": "[DOC_ID: chunk-7913493a]\n[领域: 计算机科学]\n卷积神经网络（CNN 或ConvNet）是一种直接从数据中学习的深度学习网络架构。 CNN 特别适合在图像中寻找模式以识别对象、类和类别。它们也能很好地对音频、时间序列和信号",
    "file_path": "unknown_source",
    "create_time": 1769003015,
    "update_time": 1769003015,
    "_id": "doc-225c135f952fba89e32fc67dcf80d0f5"
  },
  "doc-2a3ed384e5cd9787590d994e1b861e3a": {
    "content": "[DOC_ID: chunk-75298eae]\n[领域: 计算机科学]\n# 什么是卷积神经网络 (CNN)？. 卷积神经网络 (CNN) 是一类专为图像识别与目标检测设计的深度学习模型。通过卷积层、池化层与全连接层组合，CNN 能自动提取图像特征并完成分类任务。. IBM 在 Watson Studio 和 watsonx.ai 平台中提供卷积神经网络 (CNN) 架构支持，助力医疗影像诊断、自动驾驶感知和智能安防等应用实现高性能部署。. ## 卷积神经网络 (CNN) 如何工作？. 卷积层是卷积网络的第一层。虽然卷积层可以后跟另外的卷积层或池化层，但全连接层肯定是最后一层。随着层级的递进，卷积神经网络 (CNN) 的复杂性也逐步增加，能够识别图像的更多部分。靠前的层关注于简单的特征，比如颜色和边缘。随着图像数据沿着卷积神经网络 (CNN) 的层级逐渐推进，它开始识别对象中更大的元素或形状，直到最终识别出预期的对象。. 卷积层是 CNN 的核心构建块，负责执行大部分计算。它需要几个组件，包括输入数据、过滤器和特征图。假设输入是彩色图像，由三维的像素矩阵组成。这意味着，输入具有三个维度：高度、宽度和深度，对应于图像中的 RGB。我们还有一个特征检测器，也称为内核或过滤器，它在图像的各个感受野中移动，检查是否存在特征。这个过程称为卷积。. 在每次卷积运算之后，卷积神经网络 (CNN) 对特征图应用修正线性单元 (ReLU) 转换，为模型引入非线性特性。. 如前所述，初始卷积层可以后跟另一个卷积层。如果是这种情况，CNN 的结构就变成一个分层结构，因为后面层可以看到前面层的感受野中的像素。例如，假设我们尝试确定图像中是否包含自行车。可将自行车视为各种零件的总和，它由车架、车把、车轮、踏板等组成。自行车的每个零件构成神经网络中一个较低层次的模式，而零件的组合则表示一个较高层次的模式，从而在卷积神经网络 (CNN) 中形成特征层次结构。最终，卷积层将图像转换为数值，允许神经网络解释和提取相关模式。. 虽然池化层中会丢失大量信息，但它还是给 CNN 带来的许多好处。该层有助于降低复杂性、提高效率，并限制过度拟合的风险。. ## 卷积神经网络 (CNN) 的类型. 然而，LeNet-5 被公认为经典的卷积神经网络 (CNN) 架构。. Mixture of Experts | 12 月 12 日，第 85 集. 观看 Mixture of Experts 所有剧集. ## 卷积神经网络 (CNN) 与计算机视觉. 卷积神经网络有力地推动了影像识别和计算机视觉任务的执行。计算机视觉是人工智能 (AI) 的一个领域，让计算机和系统能够从数字图像、视频和其他视觉输入中获取有意义的信息，并根据这些输入采取行动。这种提供建议的能力让它有别于图像识别任务。目前可以看到的计算机视觉的一些常见应用领域包括：. ## 卷积神经网络 (CNN) 热门问答精选 (FAQ). CNN 与传统前馈神经网络相比，具备参数共享和局部感知能力，在处理图像和音频等结构化数据时更高效，能显著减少模型参数数量并提升训练速度。. CNN 广泛应用于医疗影像分析、自动驾驶感知、安防监控、人脸识别、工业缺陷检测和零售视觉搜索等领域，是当前主流的计算机视觉模型之一。. 常见 CNN 架构有 LeNet-5、AlexNet、VGGNet、GoogLeNet、ResNet 等，适用于不同复杂度与性能需求的图像任务。. IBM 在 Watson Studio 和 watsonx.ai 等平台中提供 CNN 模型构建、训练、调优与部署的全流程工具，帮助企业高效落地图像识别与智能视觉解决方案。. ## 卷积神经网络 (CNN) 热门问答精选 (FAQ). CNN 与传统前馈神经网络相比，具备参数共享和局部感知能力，在处理图像和音频等结构化数据时更高效，能显著减少模型参数数量并提升训练速度。. CNN 广泛应用于医疗影像分析、自动驾驶感知、安防监控、人脸识别、工业缺陷检测和零售视觉搜索等领域，是当前主流的计算机视觉模型之一。. 常见 CNN 架构有 LeNet-5、AlexNet、VGGNet、GoogLeNet、ResNet 等，适用于不同复杂度与性能需求的图像任务。. IBM 在 Watson Studio 和 watsonx.ai 等平台中提供 CNN 模型构建、训练、调优与部署的全流程工具，帮助企业高效落地图像识别与智能视觉解决方案。. 如何准备数据集并选择 AI 模型，平衡性能、成本、风险、部署需求和利益相关者要求，确定最佳模型。. IBM Granite 是我们开放式、性能优异、值得信赖的 AI 模型系列，专门为企业量身定制，并经过优化，可以帮助您扩展 AI 应用程序。深入了解语言、代码、时间序列和护栏选项。. 报告 2024 年 AI 实际应用. 我们对 2,000 家组织进行了调查，旨在了解他们的 AI 计划，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。. 指南 面向 CEO 的生成式 AI 指南. 了解 CEOs 如何在生成式 AI 所能创造的价值与其所需的投资和带来的风险之间取得平衡。. 想要从 AI 投资中获得更好的回报吗？了解如何通过帮助您最优秀的人才构建和提供创新的新解决方案，在关键领域扩展生成式人工智能来推动变革。. 使用面向 AI 构建器的新一代企业级开发平台 IBM watsonx.ai，可以训练、验证、调整和部署生成式 AI、基础模型和机器学习功能。使用一小部分数据，即可在很短的时间内构建 AI 应用程序。. 通过增加 AI 重塑关键工作流程和运营，最大限度提升体验、实时决策和商业价值。.",
    "file_path": "unknown_source",
    "create_time": 1769003046,
    "update_time": 1769003046,
    "_id": "doc-2a3ed384e5cd9787590d994e1b861e3a"
  },
  "doc-df57bc33c49fe522c66616c1a4be5336": {
    "content": "[DOC_ID: chunk-ab08cdfb]\n[领域: 计算机科学]\n卷积神经网络（英语：convolutional neural network，缩写：CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。",
    "file_path": "unknown_source",
    "create_time": 1769003200,
    "update_time": 1769003200,
    "_id": "doc-df57bc33c49fe522c66616c1a4be5336"
  },
  "doc-8e1305a63a0b2c4b823467f52bfa4ab8": {
    "content": "[DOC_ID: chunk-325cdbee]\n[领域: 计算机科学]\n# 3-4 卷积神经网络. 如果说神经网络是个黑盒子，吃进去数据，吐出来结果，那么不同神经网络的最大区别就是肚子里的网络结构。卷积神经网络有什么过人之处呢？我们首先来宏观的看一下它的网络结构。. 卷积神经网络英文为Convolutional Neural Network，简称CNN，模型结构如下图所示。. 左侧是输入层，可以是文字、语音、图像、视频等各种数字化信号。最右边是输出层，一般是分类的类别及其概率。比如我们要识别手写体数字，那输入就是手写体图片，输出就是0-9这10个数字；要识别车型，那输入就是车子的图片，输出就是奔驰、宝马、奥迪等品牌的概率。. 它的就蕴含着三种不同的网络层：卷积层、池化层和全连接层。多个“卷积+池化”结构层层连接，就是卷积神经网络最重要的特点。. ### 1.1为什么用卷积层呢？. 坦率地讲，它其实和神经网络一样，是计算机学者受生理学家和数学家的启发而创造出来的。开始算力不行，数据也少，只能靠想象。后来硬件升级，数据也多了，通过实验发现效果很好，从而一炮走红。. 节点表示神经元，连线表示计算关系，每条线都有个权重。所谓的学习或者训练就是：喂数据，调整权重。如果用审美的眼光来看，他就像个体态臃肿的大胖子。为什么这么说呢？因为每层节点都和上一层的所有节点相连，构成所谓的全连接网络，看似密密麻麻，其实冗余度大，无效的肥肉多，一点也不结实性感。. 也就是所谓的光吃饭不干活，空有一身赘肉，力气却不大，显得特别的笨。从模型角度来说，就是参数众多、难以训练。. 举个例子，哪怕输入是个100x100 的小图像，全连接层的每个神经元就要有10000个参数，如果要保证输出维度仍然是100x100，全连接层就要有10000个神经元，总计需要1亿个参数。而且层数一多，就会容易产生过拟合，就好像吃饭挑食，没见过的不吃。. 卷积运算恰恰相反，以二维图像为例，小小的3x3卷积核却能起到特征提取的大作用。通过二维离散卷积操作，把全连接变成了一个个小的局部连接，实现了高效的图像特征提取。. 无论图像大小如何，使用卷积操作，每个滑动区域都具有相同的共享权重，如果使用3x3大小的卷积核，就只需要学习9个参数。从1亿个参数变成9个，卷积核帮助神经网络在能够保证性能的条件下实现了瘦身。. 这些小小的卷积核还具备着传说中的空间平移不变的特性，简单理解，就是对于一张照片，不管你的脸在哪个位置，它都能找出来。. 卷积运算得到的结果，常常被称作：特征图feature map，特征图上的每个点，都是由上一层的若干个点共同决定的，比如3x3卷积核时，每个点是由上一层的9个点决定，我们就把这个9叫做这一点的感受野。可以想象一下，网络层数越多，每一点的感受野就越大。而感受野越广阔，就越能够捕捉更大尺寸的特征。. 如果一层卷积不够强，那就层层堆叠起来，这就是所谓的深度卷积网络。卷积层数越多，对复杂特征的表达能力越强，其实这也很好理解：. 以图像分类为例，在多层卷积网络中，第一层可以表示在特定的位置和角度是否出现边缘；而第二层则可能能够将这些边缘组合出有趣的模式，如花纹；在第三层中，也许上一层的花纹能够进一步汇合成物体特定部位的模式。这样逐级表示下去，就能学到图像的各种特征了。. 我们前面讲过神经网络之所以能解决非线性问题，本质上就是加入了激活函数这一非线性因素，否则就和线性回归区别不大。卷积神经网络也不例外，科学家们在卷积过后，也加入了激活函数。. ### 1.2 池化层的作用. 池化层英文是Pooling，翻译的听上去有点晕，本意其实是淤积或者汇聚更贴切些。直白点儿说，就是抓主要矛盾，忽略次要因素。池化层把局部神经元的输出，组合成下层单个神经元来减少数据维度。用数学语言描述，就是在一个小矩阵中找最大值或者平均值，这就是所谓的最大池化或者平均池化运算。局部池化一般选择2x2的矩阵就够了。. 卷积层屁股后边跟一个池化层，这个操作的目的是进一步放大主要特征，忽略掉几个像素的偏差。其意义不光能够降低数据维度，减少训练参数，同时还能避免所谓的过拟合，也就是别画蛇添足，学过了头。. ### 1.3 全连接层. 卷积神经网络的最后往往都保留一到两个全连接层，其中最后一层全连接层也就是输出层。其作用好比从全局出发，做最终结论和前面的卷积层搭班子，形成了先局部，再整体的学习结构。输出层会给数据进行降维，将数据维度降低到和结果的类别数相等。“卷积+池化”多层级联，对输入数据进行多尺度特征提取和深度学习，这就是卷积神经网络的特点。. 卷积和神经网络的最初相遇发生在计算机视觉领域，第一个发现者是日本学者福岛邦彦 他受前人关于生物视觉系统研究的启发，提出了层级化的人工神经网络，即“神经认知模型”，处理手写字符识别问题，这被认为是卷积神经网络的前身。. 不过，卷积和神经网络真正意义上的合体被认为是法国学者,Yann LeCuu等人在1998年提出 基于梯度学习的卷积神经网络算法LeNet,它被广泛应用于美国邮政系统的手写数字和字符识别。. 卷积神经网络的第一次大放异彩，要回溯到2012年，在有计算机视觉界“世界杯”之称的ImageNet竞赛上，Hinton等人凭借AlexNet一举夺魁，这个网络的结构和LeNet非常类似，但是更深更大。并且使用了层叠的卷积层来获取特征，超高的识别率让学术界意识到了卷积对神经网络改造的巨大潜力。. 之后，Google, facebook, 微软，国内的BAT等公司纷纷投入巨资研究，各种变体的卷积神经网络层出不穷。例如ZFNet通过可视化展示了卷积神经网络各层的功能和作用；牛津大学提出的VGGNet模型采用堆积的小卷积核替代采用大的卷积核；不仅能够增加决策函数的判别性还能减少参数量；GoogleNet增加了卷积神经网络的宽度，使用1x1卷积降维减少参数量，在多个不同尺寸的卷积核上进行卷积后再聚合；ResNet解决了网络模型的退化问题，允许神经网络更深。. 接下来我们以MNIST模型为例，来实现一个卷积神经网络。模型框架依然选择PyTorch，前面讲过MINST作为深度学习的Helloworld,其数据集已经集成在各大主流的框架里面了，可以随用随取。. Conv2d(1, 16, kernel_size = 3), nn. Conv2d(16, 32, kernel_size = 3), nn. MaxPool2d(kernel_size = 2, stride = 2)). Linear(128* 4* 4, 1024), nn. def forward(self, x): x = self. size(0), - 1) x = self. 因为MNIST是一个图像分类任务，因此我们采用交叉熵作为损失函数，优化器则选取一个SGD，学习率设为0.1。.",
    "file_path": "unknown_source",
    "create_time": 1769003219,
    "update_time": 1769003219,
    "_id": "doc-8e1305a63a0b2c4b823467f52bfa4ab8"
  },
  "doc-e101b215c048456f8049a66e9ebd2c54": {
    "content": "[DOC_ID: chunk-5f7ffd16]\n[领域: 认知神经科学]\n自编码器（auto-encoder）是一类执行无监督学习任务的神经网络结构，它的目的是学习一组数据的重新表达，也就是编码。 在结构上，自编码器是包含若干隐藏层的深度前馈神经",
    "file_path": "unknown_source",
    "create_time": 1769003362,
    "update_time": 1769003362,
    "_id": "doc-e101b215c048456f8049a66e9ebd2c54"
  },
  "doc-cc43c5c5bb53711c8b6a6ae6cdc8e9c8": {
    "content": "[DOC_ID: chunk-b539fb0c]\n[领域: 认知神经科学]\n在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。 想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中， 以便同一组",
    "file_path": "unknown_source",
    "create_time": 1769003382,
    "update_time": 1769003382,
    "_id": "doc-cc43c5c5bb53711c8b6a6ae6cdc8e9c8"
  },
  "doc-0a82bdcf222bdb547252e178dfe4d622": {
    "content": "[DOC_ID: chunk-61bda500]\n[领域: 认知神经科学]\n自动编码器(AutoEncoder)是神经网络的一种,一般来讲自动编码器包括两部分:编码器和解码器,编码器和解码器相互串联合作,实现数据的降维或特征学习,现在也广泛用于生成模型",
    "file_path": "unknown_source",
    "create_time": 1769003415,
    "update_time": 1769003415,
    "_id": "doc-0a82bdcf222bdb547252e178dfe4d622"
  },
  "doc-0c8c61555719c99f2248173e3fd5ae4f": {
    "content": "[DOC_ID: chunk-e56f03da]\n[领域: 认知神经科学]\n（1）自编码器是前馈神经网络的一种，最开始主要用于数据的降维以及特征的抽取，随着技术的不断发展，现在也被用于生成模型中，可用来生成图片等。 （2）前馈神经网络是有监督学习，",
    "file_path": "unknown_source",
    "create_time": 1769003441,
    "update_time": 1769003441,
    "_id": "doc-0c8c61555719c99f2248173e3fd5ae4f"
  },
  "doc-261214465e9e66011cd2c39022b3dc6b": {
    "content": "[DOC_ID: chunk-5de23bbf]\n[领域: 认知神经科学]\n### 专家为您带来最新的 AI 趋势. 获取有关最重要且最有趣的 AI 新闻的精选洞察分析。订阅我们的每周 Think 时事通讯。请参阅 IBM 隐私声明。. ## 编码器-解码器架构. ## 编码器-解码器模型工作原理. ### 编码器. 想要从 AI 投资中获得更好的回报吗？了解如何通过帮助您最优秀的人才构建和提供创新的新解决方案，在关键领域扩展生成式人工智能来推动变革。. IBM Granite 是我们开放式、性能优异、值得信赖的 AI 模型系列，专门为企业量身定制，并经过优化，可以帮助您扩展 AI 应用程序。深入了解语言、代码、时间序列和护栏选项。. 指南 树立信任，从容自信在 AI 新时代蓬勃发展. 使用面向 AI 构建器的新一代企业级开发平台 IBM watsonx.ai，可以训练、验证、调整和部署生成式 AI、基础模型和机器学习功能。使用一小部分数据，即可在很短的时间内构建 AI 应用程序。. 通过增加 AI 重塑关键工作流程和运营，最大限度提升体验、实时决策和商业价值。. and Martin, J., “Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition”, Third edition, 2023. “Natural Language Processing with Transformers”, Revised Edition, O’Reilly, 2022. “Speech and Language Processing”, Third Edition, 2023. “Natural Language Processing with Transformers”, Revised Edition, O’Reilly, 2022. “Natural Language Processing with Transformers”, Revised Edition, O’Reilly, 2022. “Neural network methods for Natural Language Processing”, Springer, 2022. “Hands-on Large Language Models”, O’Reilly, 2024. “Speech and Language Processing”, Third Edition, 2023. “Transformers for Natural Language Processing”, Second Edition, 2022. “Speech and Language Processing”, Third Edition, 2023. “Natural Language Processing with Transformers”, Revised Edition, O’Reilly, 2022. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”, 2019. “IBM Granite Large Language Models Whitepaper” 2024.",
    "file_path": "unknown_source",
    "create_time": 1769003450,
    "update_time": 1769003450,
    "_id": "doc-261214465e9e66011cd2c39022b3dc6b"
  },
  "doc-93860b80c6ea559ea9b38a8562c8df8c": {
    "content": "[DOC_ID: chunk-3cac9f99]\n[领域: 认知神经科学]\n多层感知机（MLP） 是一种前馈神经网络，最早由Frank Rosenblatt 在1950 年代提出，并在1980 年代通过反向传播算法取得重大进展。MLP 由输入层、隐藏层和输出",
    "file_path": "unknown_source",
    "create_time": 1769003549,
    "update_time": 1769003549,
    "_id": "doc-93860b80c6ea559ea9b38a8562c8df8c"
  },
  "doc-2bd963b40a794cdd2a7befd072b521f8": {
    "content": "[DOC_ID: chunk-9cdb6498]\n[领域: 认知神经科学]\n在本章中，我们将介绍你的第一个真正的深度网络。最简单的深度网络称为多层感知机， 它们由多层神经元组成，每一层都与下面一层（从中接收输入）和上面一层（反过来影响当前层的",
    "file_path": "unknown_source",
    "create_time": 1769003565,
    "update_time": 1769003565,
    "_id": "doc-2bd963b40a794cdd2a7befd072b521f8"
  },
  "doc-8bed972d3227719ebad6c051fae785c9": {
    "content": "[DOC_ID: chunk-2b8bd67d]\n[领域: 认知神经科学]\nplot(x, y, 'x','relu(x)', figsize =(5,2.5)). numpy(), 'x','relu(x)', figsize =(5,2.5)). numpy(), 'x','relu(x)', figsize =(5,2.5)). grad, 'x', 'grad of relu', figsize =(5,2.5)). grad, 'x', 'grad of relu', figsize =(5,2.5)). numpy(), 'x', 'grad of relu', figsize =(5,2.5)). numpy(), 'x', 'grad of relu', figsize =(5,2.5)). plot(x, y, 'x','sigmoid(x)', figsize =(5,2.5)). detach(), 'x','sigmoid(x)', figsize =(5,2.5)). numpy(), 'x','sigmoid(x)', figsize =(5,2.5)). numpy(), 'x','sigmoid(x)', figsize =(5,2.5)). grad, 'x', 'grad of sigmoid', figsize =(5,2.5)). grad, 'x', 'grad of sigmoid', figsize =(5,2.5)). numpy(), 'x', 'grad of sigmoid', figsize =(5,2.5)). numpy(), 'x', 'grad of sigmoid', figsize =(5,2.5)). plot(x, y, 'x','tanh(x)', figsize =(5,2.5)). detach(), 'x','tanh(x)', figsize =(5,2.5)). numpy(), 'x','tanh(x)', figsize =(5,2.5)). numpy(), 'x','tanh(x)', figsize =(5,2.5)). grad, 'x', 'grad of tanh', figsize =(5,2.5)). grad, 'x', 'grad of tanh', figsize =(5,2.5)). numpy(), 'x', 'grad of tanh', figsize =(5,2.5)). numpy(), 'x', 'grad of tanh', figsize =(5,2.5)). 3. 证明\\(\\operatorname{tanh}(x) + 1 = 2 \\operatorname{sigmoid}(2x)\\)。.",
    "file_path": "unknown_source",
    "create_time": 1769003576,
    "update_time": 1769003576,
    "_id": "doc-8bed972d3227719ebad6c051fae785c9"
  },
  "doc-80c468f42f64d269b4538a150a4f94cd": {
    "content": "[DOC_ID: chunk-58e1affb]\n[领域: 认知神经科学]\n| 范式 * 监督学习 * 无监督学习 * 线上机器学习 * 元学习&action=edit&redlink=1 \"元学习 (计算机科学)（页面不存在）\")（英语：Meta-learning (computer science) \"en:Meta-learning (computer science)\")） * 半监督学习 * 自监督学习 * 强化学习 * 基于规则的机器学习（英语：Rule-based machine learning） * 量子机器学习 |. | 强化学习 * Q学习 * SARSA * 时序差分（TD） * 多智能体（英语：Multi-agent reinforcement learning） + Self-play&action=edit&redlink=1 \"Self-play (强化学习技术)（页面不存在）\")（英语：Self-play (reinforcement learning technique) \"en:Self-play (reinforcement learning technique)\")） * RLHF |. | 与人类学习 * 主动学习&action=edit&redlink=1 \"主动学习 (机器学习)（页面不存在）\")（英语：Active learning (machine learning) \"en:Active learning (machine learning)\")） * 众包 * Human-in-the-loop（英语：Human-in-the-loop） |. | 模型诊断 * 学习曲线&action=edit&redlink=1 \"学习曲线 (机器学习)（页面不存在）\")（英语：Learning curve (machine learning) \"en:Learning curve (machine learning)\")） |. | 大会与出版物 * NeurIPS * ICML（英语：International Conference on Machine Learning） * ICLR * AAAI * IJCAI * CVPR * ML&action=edit&redlink=1 \"机器学习 (期刊)（页面不存在）\")（英语：Machine Learning (journal) \"en:Machine Learning (journal)\")） * JMLR（英语：Journal of Machine Learning Research） |. 其中 yi 是前一个神经元的输出，η是学习率。η需要精心挑选，保证权重可以快速收敛而不发生震荡。. | 软件库 | * Theano * TensorFlow + Keras * PyTorch + Caffe * JAX * MindSpore（英语：MindSpore） * Flux.jl&action=edit&redlink=1 \"Flux (机器学习框架)（页面不存在）\")（英语：Flux (machine-learning framework) \"en:Flux (machine-learning framework)\")） |.",
    "file_path": "unknown_source",
    "create_time": 1769003610,
    "update_time": 1769003610,
    "_id": "doc-80c468f42f64d269b4538a150a4f94cd"
  },
  "doc-6d725020748a8a4cbf40cdbf97928919": {
    "content": "[DOC_ID: chunk-94824c1b]\n[领域: 认知神经科学]\n8.3 隐含层与多层感知机 · 区间内的问题可以选用 ·。MLP 相比于单层感知机的表达能力提升，关键就在于非线性激活函数的复合。理论上可以证明，任意一个 · 上的连续函数，都可以",
    "file_path": "unknown_source",
    "create_time": 1769003677,
    "update_time": 1769003677,
    "_id": "doc-6d725020748a8a4cbf40cdbf97928919"
  },
  "doc-88c27f8b7b31c4aea51cfc3d8f79a013": {
    "content": "[DOC_ID: chunk-b8a60b69]\n[领域: 认知神经科学]\n这项研究深入探讨了在深度学习中通过随机梯度下降（SGD）与损失函数地形（landscape） 相互作用而实现有效学习的机制。我们发现，在SGD导航损失函数地形时，它",
    "file_path": "unknown_source",
    "create_time": 1769003697,
    "update_time": 1769003697,
    "_id": "doc-88c27f8b7b31c4aea51cfc3d8f79a013"
  },
  "doc-92f537cdcce6583c2c63c400d634feaf": {
    "content": "[DOC_ID: chunk-22ad52c2]\n[领域: 认知神经科学]\n可以说，没有世界一流的计算神经科学，就不可能有世界一流的类脑智能技术创新。反过来，机器学习（包括深度学习）能引进新的方法来处理脑科学数据，新的思路来模拟脑功能。",
    "file_path": "unknown_source",
    "create_time": 1769003717,
    "update_time": 1769003717,
    "_id": "doc-92f537cdcce6583c2c63c400d634feaf"
  },
  "doc-9ed0526c7eff8b1a424224e53050b149": {
    "content": "[DOC_ID: chunk-5aefad9f]\n[领域: 认知神经科学]\n第二步，请的深度学习的基础，我觉得掌握深度学习的基本知识很有必要，我推荐动手学 ... 如果要继续加深对计算认知和计算神经科学的理解：. 推荐Coursera上面的",
    "file_path": "unknown_source",
    "create_time": 1769003737,
    "update_time": 1769003737,
    "_id": "doc-9ed0526c7eff8b1a424224e53050b149"
  },
  "doc-ceba1324d8578bf32520312053ae6580": {
    "content": "[DOC_ID: chunk-5fd284c9]\n[领域: 认知神经科学]\n#### 李澄宇 机器学习与神经科学. 十二岁的流浪汉Walter Pitts1,2,3 被一群黑帮匪徒追打，躲进了一个图书馆。打手们骂骂咧咧的找了一圈以后走了。Walter却没离开，他的注意力完全被一套三卷本的书吸引了：Alfred North Whitehead 与 Bertrand Russell 合著的 Principia Mathematica 。这是作者为纯数学提供逻辑基础的雄心勃勃的大部头著作，努力构建严谨的逻辑构架，把数学命题在自己的构架里一步步推导出来。独立学习了三年之后，15岁的Walter Pitts写信给Russell，向Russel指出在第一卷里的几个错误。Russel马上回信，邀请Pitts去英国学习。Walter没有去湿冷的英格兰，而去了芝加哥大学，没有固定生活来源的他流浪在芝加哥大学的讲堂和校园里（很多时间睡在露天的躺椅上）。在进芝加哥大学第一年的时候，芝大的数学教授Carnap出了一本关于逻辑学的新书，那是1938年。Carnap不是泛泛之辈，他关于逻辑悖论的论述是三十年代时最完备的。一天，15岁的Walter Pitts走进Carnap的办公室，手里拿着写着自己注释的Carnap的新书，向Carnap对书中的几个地方提出尖锐的评论。在开始的震惊之后，Carnap开始反驳，两人交流了一个小时左右。Walter把书留在Carnap那里，离开了办公室。由于Pitts没有介绍自己，于是后来的两个月里Carnap在芝大里满世界找“懂逻辑的报童”。. Walter Pitts的故事和其他天才的故事一样，让人难以想象，就像莫扎特，王维，冯纽曼，这些年纪轻轻就在自己的领域独步天下，又似乎全不费力、信手拈来。那么这些天才是否可以人造呢？这个问题在以前也许是很幼稚的，不过现在出现了AlphaGo这类人工智能算法，在棋盘上打败了几乎所有的顶级围棋职业高手，包括柯杰这样的天才，也许这个问题是可以被容许讨论的了。. 1942年的时候18岁的Walter认识了University of Illinois 教授Warren McCulloch。Warren热情好客，当时Walter Pitts无家可归，于是Warren邀请他住在自己家里。在Walter住进Warren家的当夜，他们就开始合作神经系统如何进行逻辑计算的问题。大概300年前，莱不尼兹证明，任何问题，只要可以用有限数目的词、完整而没有歧义的表达出来，就可以用‘逻辑机’(logical machine)计算出来。六年之前， Alan Turning 发表了著名的关于universal computing engine的文章。Walter Pitts和Warren的问题是，如何把神经系统看成这样一个逻辑机器呢？考虑到当时对脑神经的粗浅而不确定的认识，Warren和Pitts对神经系统的简化是划时代的。他们把神经元的状态简化为0或1 的二进制表示，用一个连接强度的矩阵来标准神经元之间的相互影响关系，于是在给予一定输入的情况下，McCulloch-Pitts网络将根据这个连接矩阵的状态以及输入，而决定输出是什么。他们在1943年出版的\"A Logical Calculus of the Ideas Immanent in Nervous Activity.\"是McCulloch 和Pitts最著名的工作。从控制论角度来讲， McCulloch-Pitts网络本身就是最早的有限自动机（finite automata）之一。这个工作还提供了神经生理问题的逻辑基础，可以说是最早的计算神经科学成果之一。而这篇文章对于人工智能领域来讲，则是开创性的，神经系统的复杂性被高度抽象，并用数学家和工程师容易理解的语言来描述。从此，人工智能开始以独立的领域而开始其蓬勃发展之势。. 要描绘神经科学与人工智能领域相互借鉴、相互促进的完整画面，本文的篇幅是不够的。但是两个领域都意识到“学习”对于生物和电脑行为的重要性，神经科学发现神经元之间的联结“突触”，可以被神经元的活动所改变，这一点很可能是学习的物质基础；在人工智能领域，人工神经元之间的联结强度的改变，也是人工神经网络可以学习的基础。这一特性，被目前的“深度学习”推到了极致，可以说引起了一个深度学习革命，在网络搜索、网站过滤、广告推送、图像识别、语言翻译等大量应用都可以找到深度学习的实际应用。其中Geoffrey Hinton做出了突出的贡献。. Geoffrey Hinton4在英国出生，接受的是基督教学校的教育，虽然早早的就认为基督教神学是完全的废话5。1982年左右Hinton和Terry Sejnowski一起找到了漂亮的训练人工神经网络的方法，从而发明了玻尔兹曼机（Boltzmann machine），在人工智能和计算神经科学领域都有很重要的应用。他还和同行一起推动了BP算法，即反向传播算法（Back-propagation）的广泛应用；之后他一直致力于优化人工神经网络的学习。但是在九十年代到2006之间，人工神经网络慢慢的淡出了人工智能领域的前沿，也许是由于在实际应用中，人工神经网络计算结果容易进入局部极小值（Local minimum），找不到全局最小值（Global minimum），从而无法实现对实际问题的求解。但是有时候坚守会得到回报的，Hinton和Yann LeCun，另一位深度学习的奠基人，就是这句话的最好诠释。他们两个人都从神经系统的原理得到灵感：LeCun把脑的视觉系统的组织结构扩展到人工神经网络中，发明了卷积神经网络，实现了高效的手写字的识别；而Hinton则把脑皮层中分层的组织原则移植到人工神经网络中，在2006年发明了深度学习的训练方法。恰逢计算机能力在2006前后获得了急速提升，网络的大量普及使用提供了大规模的数据，使得深度学习得以发挥其多层网络的优势，在大数据的训练和冲击下，哪些局部极小值消失了，取而代之的是全局最小值。目前，深度学习在很多特定任务，例如人脸识别，图像中的物体识别，语音识别等应用中的正确率已经超过了人类。Hinton兼职进入Google，而谷歌也收购了“深脑”（DeepMind），这一由系统神经科学家创造的公司，结合深度学习与神经科学中的强化学习（Reinforcement learning）概念，领导的团队创造了AlphaGo，并在围棋上战胜李世石和其他顶级职业高手，也许这是深度学习革命的最好注解6。. 开篇的天才故事让我们讲到结束吧。Pitts虽然做出了巨大的贡献，但他极度排斥社会的正式承认。在芝加哥大学旁听、而不注册学生、只是个开始。也许是在Norbert的安排下，MIT要给他一个学位，所有Pitts需要做的事就是签署一个文件、翻译一篇德文文章（他懂德文，所以那是小菜一碟）；而他断然拒绝。后来MIT想要给他一个正式的教职，这回只要Pitts签署一个文件，同样被Pitts回绝。Pitts刻意销毁自己工作的记录，逃避任何拍照的可能，没有日记以及信件流传。总之，对于不亲身识得他的人，Pitts是躲在雾里的一个谜，无法看清。. 但是有趣的事，与Pitts认识的人都会觉着Pitts乐于交流，极好相处，谈吐不凡，知识渊博。往往你问他一个科学或者艺术的问题，他可以把这个问题的来龙去脉、前因后果、娓娓道来，讲上两三个小时，闻者不倦。Pitts让人联想起电影《海上钢琴师》中的音乐家，天才飘洒大海，却又仅局限于一叶扁舟。Pitts人如美玉，却选择只是泽被身边有限的好友。. Pitts在1969年逝世于MIT的寓所里。死的时候身旁没有一个人。如今，神经元的数学模型被称为McCulloch–Pitts 神经元。. Pitts的天才和遭遇也许正是目前人工智能还无法企及的能力。深度学习需要“监督学习”，也就是需要提前知道最终答案，从而训练人工神经网络。Pitts的创造性思维，在1942年的时候并没有最终答案，人工智能是否在未来可以创造出这一能力？Pitts的社会能力远远不如大多数社会人，那么人工智能在社会行为这一人类特别出色的能力上，能给我们什么样的惊喜？也许，还是要从神经科学中找到一些关键性的原理性灵感，从而拓展人工智能的能力，也让我们不断思考人类与计算机在智能上的边界。. 1. Pitts留世的资料极少，请参见Jim Anderson与Lettvin 的访谈录： Talking nets， James Anderson and Edward Rosenfeld, 1998, MIT Press. 4. Hinton的访谈录：Talking nets， James Anderson and Edward Rosenfeld, 1998, MIT Press. 5. “I was convinced throughout my childhood that the whole Christian ideology at school was just complete rubbish. I’m still convinced of that.” Hinton的访谈录：Talking nets， James Anderson and Edward Rosenfeld, 1998, MIT Press. 6. 深度学习的介绍有很多，可以从Yann LeCun, Yoshua Bengio 和Geoffrey Hinton2015年在Nature上的“Deep learning”这一综述开始。. 电话：86-21-54921723 传真：86-21-54921735 邮件：query@ion.ac.cn.",
    "file_path": "unknown_source",
    "create_time": 1769003755,
    "update_time": 1769003755,
    "_id": "doc-ceba1324d8578bf32520312053ae6580"
  },
  "doc-03a5749c6bb89c89b202fa903dbb847f": {
    "content": "[DOC_ID: chunk-b321b1e4]\n[领域: 认知神经科学]\n9分钟带你了解神经网络&深度学习(Excel还原神经网络计算). 607 views ... 科學實證的「人生外掛」總整理，即學即用! / 科學驗證：運氣是想出來的",
    "file_path": "unknown_source",
    "create_time": 1769003896,
    "update_time": 1769003896,
    "_id": "doc-03a5749c6bb89c89b202fa903dbb847f"
  },
  "doc-3954d1f048ce5cd91b5693da96becc81": {
    "content": "[DOC_ID: chunk-3ad2432e]\n[领域: 认知神经科学]\nby 焦李成 · 2019 · Cited by 3 — 类脑智能旨在模拟人脑神经元的运行机制、感知模式与认知机理,借助机器强大的信息. 整合、搜索、计算等能力,以软硬件联合的智能新形态构造接近人类水平的智能机器,是未来",
    "file_path": "unknown_source",
    "create_time": 1769003931,
    "update_time": 1769003931,
    "_id": "doc-3954d1f048ce5cd91b5693da96becc81"
  },
  "doc-a0a9ca6f9242eabc77977a8fe45c5022": {
    "content": "[DOC_ID: chunk-a3329ce0]\n[领域: 认知神经科学]\n#### 固定科研人员(按首字母拼音顺序）. #### 兼职科研人员. #### 行政管理. #### 博士后. #### （专任）工程师/助理研究员. #### 来访学者. #### 教育部重点实验室. #### 认知神经科学中心. #### 生物医学影像中心（张江国际脑影像中心）. #### 生物医学大数据中心(张江国际脑库)\"). #### 神经与智能工程中心. #### 全脑计算平台. #### 群体神经科学中心. #### 老年脑健康智能科学中心. #### 计算系统生物学中心. #### 人工智能算法中心. #### 仪器设备简介. #### 科研进展. #### 科研项目. #### 上海市市级科技重大专项. #### 科研成果. #### 学术报告. #### 伦理申请. #### 通知公告. #### 招生. #### 培养. #### 学位. #### 奖助体系. #### 服务指南. #### 下载专区. #### 国际合作. #### 产学研合作. #### 联合研究中心. #### 固定科研团队. #### 博士后. #### 行政管理. #### 专任研究/工程师系列. #### 科研助理. #### 联系地址. ### 综合新闻.",
    "file_path": "unknown_source",
    "create_time": 1769003943,
    "update_time": 1769003943,
    "_id": "doc-a0a9ca6f9242eabc77977a8fe45c5022"
  },
  "doc-99be6d80a8d1a4129da56c2930cc8b6d": {
    "content": "[DOC_ID: chunk-9f04a41b]\n[领域: 生物信息学]\nby 王天尧 · 2024 · Cited by 1 — 蛋白质结构预测通常指借助计算机计算模拟方法从氨基酸序列推断其三维空间结构. 而空间结构决定其生理功能，故结构预测问题尤为重要. 基于单纯物理学的预测仅能应对较短",
    "file_path": "unknown_source",
    "create_time": 1769004006,
    "update_time": 1769004006,
    "_id": "doc-99be6d80a8d1a4129da56c2930cc8b6d"
  },
  "doc-c5eafae240ac5d07dadf7c4eb744e8eb": {
    "content": "[DOC_ID: chunk-bcc5aa82]\n[领域: 生物信息学]\n本文首先介绍了蛋. 白质结构预测的背景和重要性，然后详细阐述了机器学习和深度学习在蛋白质结构预测中的应用，包括常用的. 算法、模型架构以及优化策略。最后，本文展望了",
    "file_path": "unknown_source",
    "create_time": 1769004027,
    "update_time": 1769004027,
    "_id": "doc-c5eafae240ac5d07dadf7c4eb744e8eb"
  },
  "doc-62c73611ffdf6388dc832abdf23ad216": {
    "content": "[DOC_ID: chunk-0a5a78a5]\n[领域: 生物信息学]\n... 深度学习的算法，因其预测准确而闻名于世来自FAIR 的ESMFold 是另一种基于深度学习的高精度方法，用于根据氨基酸序列预测蛋白质结构。ESMFold 以大型",
    "file_path": "unknown_source",
    "create_time": 1769004051,
    "update_time": 1769004051,
    "_id": "doc-62c73611ffdf6388dc832abdf23ad216"
  },
  "doc-94655c02fc2c25c9a0fd0b9bf8d8dc6f": {
    "content": "[DOC_ID: chunk-30a2aa14]\n[领域: 生物信息学]\nDeepMind 公司决定改进 AlphaFold 系统，但经过 6 个月尝试后却远远未达预期，不得不开始重新调整思路，首先进行人事变动。. 江珀带领年轻团队对 AlphaFold 最初版本进行了重新梳理，在此基础上展开全面调整和改进，对每个细节给予挖掘以期达到尽善尽美。比如，他们引入空间立体结构和进化理念、整合已有蛋白质结构的详细信息如原子半径和键角等、完善机器有效学习策略以利于从有限数据中提取最大信息，特别是抛弃传统算法的束缚，更强调空间靠近而非线性相邻等。没有任何一种改进对最终结果有决定性影响，但正是这些奇妙新想法的完美结合，才最终实现真正意义的突破。. 2020 年，AlphaFold2 在第 14 届 CASP 竞赛上进一步大杀四方，在给定预测蛋白质中 GDT 平均得分 92.4，远超所有竞争对手；对高难度蛋白预测 GDT 平均得分 87，比第二名高出 25 分。2021 年 7 月 15 日，详细描述 AlphaFold2 内容的论文在《自然》周刊在线发表，至今引用近万次。Deepmind 不久还公布算法供全球研究人员免费使用。**这项成就被《科学》周刊评选为 2021 年度十大科学突破之首，蛋白结构预测也成为《自然-方法学》2021 年度方法。**. **AlphaFold2 解决了蛋白质结构预测问题，目前虽无法做到尽善尽美，但大多数情况下对非结构解析专业的普通研究者而言已经足够。**尤为重要的是，当研究人员获得感兴趣蛋白质序列时能够在几天甚至几小时内获得完美结构信息，而不再需要数月甚至数年时间和数百万美元的花费，对生命科学研究而言，就是难以置信的突破。. AlphaFold 解决了困扰生命科学多年的一个重大难题，并有望为其他生物学问题解决提供借鉴。大数据已成为当前科学发展的重要特征，如基因组测序结果和海量论文等，借助 AI 工具解决生命科学问题已成为一个重要方向。AlphaFold 成功的另一层意义在于激励年轻人要敢于挑战自我。. Starting at Go: Protein structure prediction succumbs to machine learning. Advances in protein structure prediction and design. The Protein Structure Prediction Revolution and Its Implications for Medicine: 2023 Albert Lasker Basic Medical Research Award. [6] Senior AW, Evans R, Jumper J, et al. Improved protein structure prediction using potentials from deep learning.Nature. [7] Jumper J, Evans R, Pritzel A, et al. Highly accurate protein structure prediction with AlphaFold. Method of the Year: protein structure prediction. AlphaFold heralds a data-driven revolution in biology and medicine. AlphaFold2 protein structure prediction: Implications for drug discovery. Accurate structure prediction of biomolecular interactions with AlphaFold 3. AlphaFold developers Demis Hassabis and John Jumper share the 2023 Albert Lasker Basic Medical Research Award.",
    "file_path": "unknown_source",
    "create_time": 1769004071,
    "update_time": 1769004071,
    "_id": "doc-94655c02fc2c25c9a0fd0b9bf8d8dc6f"
  },
  "doc-02f3c5dec4cb0c3b0004aa9d7c2bbffb": {
    "content": "[DOC_ID: chunk-ef27d008]\n[领域: 生物信息学]\n| 首页 | 南开要闻 | 媒体南开 | 南开校史 | 光影南开 | 南开故事 | 南开大学报 | 视频 | 广播 | |. | | | | --- | | 您当前的位置 ： 南开大学 >> 南开要闻 | | | | --- | | 南开团队开发高精度蛋白质结构预测算法 | | 来源： 南开大学新闻网发稿时间：2025-06-06 17:43 | | **南开新闻网讯**（通讯员 倪文韬 王萁淇）5月23日，南开大学统计与数据科学学院郑伟教授作为第一作者，在*Nature Biotechnology*发表了题为“Deep-learning-based single-domain and multidomain protein structure prediction with D-I-TASSER”的重要研究成果。 该研究开发了一种融合深度学习空间约束与统计能量函数的蛋白质结构预测算法——D-I-TASSER，实现了超越AlphaFold3算法的高精度蛋白质结构预测。该论文通讯作者为新加坡国立大学张阳教授、密歇根大学Lydia Freddolino副教授。 *Nature Biotechnology*是国际生物技术领域公认的顶尖期刊，为*Nature*系列期刊中计算结构生物学领域的顶级期刊之一，近五年平均影响因子高达57.0。该刊每年发表约百余篇高质量论文，纯计算方法相关的文章数量仅占十余篇左右。 近年来，以AlphaFold2和AlphaFold3为代表的深度学习方法在蛋白质结构预测领域取得了突破性进展。因此，2024年的诺贝尔化学奖授予了AlphaFold的开发团队。但AlphaFold仍然存在两个“短板”：一是对孤儿蛋白（同源序列较少的蛋白质）的预测效果仍不理想；二是难以处理由多个结构域组成的复杂蛋白质。D-I-TASSER正是为应对这两个难题而设计的，且在实际测试中表现出了更强的预测能力。 D-I-TASSER算法流程 D-I-TASSER算法在结构预测精度上显著优于AlphaFold系列算法。在困难单结构域蛋白预测问题中，D-I-TASSER在84%的案例中生成了比AlphaFold2质量更高的结构模型。在多结构域蛋白结构预测问题中， D-I-TASSER的全链预测精度较AlphaFold2提升了近12.9%。 D-I-TASSER参加了第15届CASP (Critical Assessment of protein Structure Prediction) 比赛，在单结构域蛋白和多结构域蛋白两个单项比赛中均排名世界第一，领先包括AlphaFold2、AlphaFold3在内的全球80余个学术界及工业界的课题组，展现出其在复杂蛋白结构预测中的世界领先优势。CASP是国际公认的世界级蛋白质结构预测权威竞赛，被业界誉为“蛋白质结构预测的奥林匹克竞赛”。 D-I-TASSER还对人类基因组的蛋白质进行了结构和功能预测（GO标签、酶分类和小分子结合位点），其成功预测了80.5%的单结构域和72.8%的全链蛋白质结构，并准确预测了其中3020个AlphaFold2难以预测的蛋白质，彰显其在弥补结构预测盲区的价值。 D-I-TASSER在基准测试和CASP15中表现优异，并成功应用于人类全蛋白组的结构与功能预测 D-I-TASSER在大规模蛋白质结构预测中，尤其在处理结构复杂的困难蛋白质与多结构域蛋白质预测方面表现出色，展现了融合深度学习空间约束与统计能量函数的新范式。该方法不仅助力理解蛋白质折叠与功能，也在抗体筛选与优化、罕见病致病基因识别、病毒感染性预测、辅助冷冻电镜结构解析等任务中取得了初步进展。未来有望与相关研究机构和生物医药企业合作，推动技术的转化与落地。 本研究得到了南开大学前沿交叉学科研究院、传染病溯源预警与智能决策全国重点实验室、自然科学基金委等多家单位的支持。 论文第一作者郑伟，现任南开大学统计与数据科学学院教授，长期从事于基于深度学习及统计能量函数的生物分子及其互作的结构预测研究，开发了D-I-TASSER、DMFold等一系列结构预测算法，累计获得CASP国际大赛十项冠军，并多次受邀在国际会议作特邀报告。截至目前，郑伟教授累计在*Nature Biotechnology、Nature Methods、Nature Communications、PNAS*等高水平SCI期刊发表文章50余篇，累计引用3500余次，其开发的结构预测算法已服务100多个国家的近10万名用户。 此外，南开统计在生物医学交叉领域持续发力，研究方向涵盖生物统计、生物信息学、数学流行病学、计算机视觉和自然语言处理在生物医学中的应用。近年来在*Nature Biotechnology*、*Nature Cancer*、*Circulation*、*Gut*、*Nature Communication*s等生物医学权威杂志发表数十篇论文，多篇疫情研判报告获得国家领导人批示，在国际蛋白质结构和功能预测等竞赛中多年多次获得冠军。 论文链接： | | 编辑：丛敏 | | | | | | | | | | | | | | | | | --- | | 微信往期推送 | | 更多... | | | | | --- | --- | | 新闻热线：022-23508464 022-85358737投稿信箱：nknews@nankai.edu.cn 本网站由南开大学新闻中心设计维护 Copyright@2014 津ICP备12003308号-1 | 南开大学 校史网 版权声明：本网站由南开大学版权所有，如转载本网站内容，请注明出处。 | |.",
    "file_path": "unknown_source",
    "create_time": 1769004153,
    "update_time": 1769004153,
    "_id": "doc-02f3c5dec4cb0c3b0004aa9d7c2bbffb"
  },
  "doc-d23247c8edaf8929e7855d9186007c46": {
    "content": "[DOC_ID: chunk-64584f52]\n[领域: 生物信息学]\nPropelled by press releases from CASP and DeepMind,AlphaFold 2's success received wide media attention.As well as news pieces in the specialist science press, such as *Nature \"Nature (journal)\")*,*Science \"Science (journal)\")*,*MIT Technology Review*,and *New Scientist*,the story was widely covered by major national newspapers,as well as general news-services and weekly publications, such as *Fortune \"Fortune (magazine)\")*,*The Economist*,Bloomberg,*Der Spiegel*,and *The Spectator*.In London *The Times* made the story its front-page photo lead, with two further pages of inside coverage and an editorial.A frequent theme was that ability to predict protein structures accurately based on the constituent amino acid sequence is expected to have a wide variety of benefits in the life sciences space including accelerating advanced drug discovery and enabling better understanding of diseases.Writing about the event, the *MIT Technology Review* noted that the AI had \"solved a fifty-year old grand challenge of biology.\"The same article went on to note that the AI algorithm could \"predict the shape of proteins to within the width of an atom.\".",
    "file_path": "unknown_source",
    "create_time": 1769004251,
    "update_time": 1769004251,
    "_id": "doc-d23247c8edaf8929e7855d9186007c46"
  },
  "doc-8ac6b800c024bfc59d45d5769abdcb86": {
    "content": "[DOC_ID: chunk-62248d47]\n[领域: 生物信息学]\n通过深入分析，本文旨在阐述深度学习技术在增强基因组数据分析的准确性和处理能力方面的作用，并构建一个概念性框架，以指导畜禽基因组学研究策略的发展及其在具体场景下的",
    "file_path": "unknown_source",
    "create_time": 1769004303,
    "update_time": 1769004303,
    "_id": "doc-8ac6b800c024bfc59d45d5769abdcb86"
  },
  "doc-dcad4a5abb7347f0e6403bab97eb576b": {
    "content": "[DOC_ID: chunk-9ea06c7f]\n[领域: 生物信息学]\nMIT 6.874 是全球顶校麻省理工MIT 开设的生命科学与计算机科学的交叉专业课程。课程广泛地介绍了基因组学、生命科学领域中，机器学习/深度学习的基础知识和前沿挑战；并使用",
    "file_path": "unknown_source",
    "create_time": 1769004317,
    "update_time": 1769004317,
    "_id": "doc-dcad4a5abb7347f0e6403bab97eb576b"
  },
  "doc-41671c7e5bea6a9ae05e6b008b3a38f2": {
    "content": "[DOC_ID: chunk-18295298]\n[领域: 生物信息学]\nDeep Learning in Life Sciences - Lecture 07 - Regulatory Genomics (Spring 2021) 6.874/6.802/20.390/20.490/HST.506 Spring 2021 Prof.",
    "file_path": "unknown_source",
    "create_time": 1769004343,
    "update_time": 1769004343,
    "_id": "doc-41671c7e5bea6a9ae05e6b008b3a38f2"
  },
  "doc-7d901977dd7e6d929821ca079c401c7e": {
    "content": "[DOC_ID: chunk-f2d08722]\n[领域: 生物信息学]\nAlphaFold 构建的模型都依赖深度神经网络，这些经过训练的神经网络可以从基因序列中预测蛋白质的属性。DeepMind 的研究人员表示，神经网络预测的蛋白质属性主要有：（a）",
    "file_path": "unknown_source",
    "create_time": 1769004377,
    "update_time": 1769004377,
    "_id": "doc-7d901977dd7e6d929821ca079c401c7e"
  },
  "doc-13f6a612ad21f33701d7916a1d76aa3e": {
    "content": "[DOC_ID: chunk-b92f6a4b]\n[领域: 生物信息学]\nAlphaFold使用人工神经网络直接学习序列和结构之间的复杂关系，而传统的工具多依赖于物理模型或简单的机器学习。（二）多序列比对（MSA）。有些传统工具虽然也",
    "file_path": "unknown_source",
    "create_time": 1769004398,
    "update_time": 1769004398,
    "_id": "doc-13f6a612ad21f33701d7916a1d76aa3e"
  },
  "doc-5899806c994fc2f391826abc901a8669": {
    "content": "[DOC_ID: chunk-b76f90a7]\n[领域: 生物信息学]\n02 技术原理：AlphaFold的运作方式. AlphaFold的技术核心是将深度学习与生物信息学结合，通过分析大量蛋白质序列和结构数据，学习序列与结构之间的关系。",
    "file_path": "unknown_source",
    "create_time": 1769004425,
    "update_time": 1769004425,
    "_id": "doc-5899806c994fc2f391826abc901a8669"
  },
  "doc-3692370e0fbe26ddf6ddebef5657ecc2": {
    "content": "[DOC_ID: chunk-d10a669b]\n[领域: 生物信息学]\nDeepMind 公司决定改进 AlphaFold 系统，但经过 6 个月尝试后却远远未达预期，不得不开始重新调整思路，首先进行人事变动。. 江珀带领年轻团队对 AlphaFold 最初版本进行了重新梳理，在此基础上展开全面调整和改进，对每个细节给予挖掘以期达到尽善尽美。比如，他们引入空间立体结构和进化理念、整合已有蛋白质结构的详细信息如原子半径和键角等、完善机器有效学习策略以利于从有限数据中提取最大信息，特别是抛弃传统算法的束缚，更强调空间靠近而非线性相邻等。没有任何一种改进对最终结果有决定性影响，但正是这些奇妙新想法的完美结合，才最终实现真正意义的突破。. 2020 年，AlphaFold2 在第 14 届 CASP 竞赛上进一步大杀四方，在给定预测蛋白质中 GDT 平均得分 92.4，远超所有竞争对手；对高难度蛋白预测 GDT 平均得分 87，比第二名高出 25 分。2021 年 7 月 15 日，详细描述 AlphaFold2 内容的论文在《自然》周刊在线发表，至今引用近万次。Deepmind 不久还公布算法供全球研究人员免费使用。**这项成就被《科学》周刊评选为 2021 年度十大科学突破之首，蛋白结构预测也成为《自然-方法学》2021 年度方法。**. **AlphaFold2 解决了蛋白质结构预测问题，目前虽无法做到尽善尽美，但大多数情况下对非结构解析专业的普通研究者而言已经足够。**尤为重要的是，当研究人员获得感兴趣蛋白质序列时能够在几天甚至几小时内获得完美结构信息，而不再需要数月甚至数年时间和数百万美元的花费，对生命科学研究而言，就是难以置信的突破。. AlphaFold 解决了困扰生命科学多年的一个重大难题，并有望为其他生物学问题解决提供借鉴。大数据已成为当前科学发展的重要特征，如基因组测序结果和海量论文等，借助 AI 工具解决生命科学问题已成为一个重要方向。AlphaFold 成功的另一层意义在于激励年轻人要敢于挑战自我。. Starting at Go: Protein structure prediction succumbs to machine learning. Advances in protein structure prediction and design. The Protein Structure Prediction Revolution and Its Implications for Medicine: 2023 Albert Lasker Basic Medical Research Award. [6] Senior AW, Evans R, Jumper J, et al. Improved protein structure prediction using potentials from deep learning.Nature. [7] Jumper J, Evans R, Pritzel A, et al. Highly accurate protein structure prediction with AlphaFold. Method of the Year: protein structure prediction. AlphaFold heralds a data-driven revolution in biology and medicine. AlphaFold2 protein structure prediction: Implications for drug discovery. Accurate structure prediction of biomolecular interactions with AlphaFold 3. AlphaFold developers Demis Hassabis and John Jumper share the 2023 Albert Lasker Basic Medical Research Award.",
    "file_path": "unknown_source",
    "create_time": 1769004443,
    "update_time": 1769004443,
    "_id": "doc-3692370e0fbe26ddf6ddebef5657ecc2"
  },
  "doc-82b403277dd4d1f793acce70ac7e2f82": {
    "content": "[DOC_ID: chunk-8e84417d]\n[领域: 生物信息学]\n谷歌DeepMind 训练了一个叫做AlphaDev 的强化学习智能体，用来寻找更好的排序程序。它从头开始发现了小型排序算法，这些算法的性能优于之前已知的人类基准，",
    "file_path": "unknown_source",
    "create_time": 1769004507,
    "update_time": 1769004507,
    "_id": "doc-82b403277dd4d1f793acce70ac7e2f82"
  },
  "doc-b1d2ad62c7c26e8ff618cb6a7b89c7df": {
    "content": "[DOC_ID: chunk-eacde1ca]\n[领域: 生物信息学]\n要说谁是引领蛋白质设计的世界级大师，美国华盛顿大学的David Baker 教授可谓是当之无愧，作为该领域的顶级专家，Baker 在蛋白质方向发表研究论文700 余篇，引用量累计",
    "file_path": "unknown_source",
    "create_time": 1769004526,
    "update_time": 1769004526,
    "_id": "doc-b1d2ad62c7c26e8ff618cb6a7b89c7df"
  },
  "doc-224a7273b9442fd233a7ec61f799fa44": {
    "content": "[DOC_ID: chunk-b1e21024]\n[领域: 生物信息学]\nNature, 2020, 577(7792): 706-710 多序列比对 预测距离分布和 二面角分布 结构优化 Chapter1 蛋白质结构预测 Alphafold2 的整体架构：在2020公布的信息 Picture taken from DeepMind slides in CASP14 8 主要特点： • End-to-end架构 • 1D与2D信息之间使用 了Attention • 3D Equivariant（等变） Structure Module Chapter2 Alphafold模型架构 Alphafold2 的整体架构：2021年发表的Nature论文 Jumper, J, et al. Nature, 2021 Chapter2 Alphafold模型架构 整体架构的精彩之四： Structure Module的关键——Equivariant 15 Protein backbone = gas of 3-D rigid bodies 重要架构：IPA (Invariant Point Attention) 和Residue Gas IPA Residue Gas • IPA用于实现3D Equivariant（平移旋转等变性） • Residue Gas用于表示蛋白质结构 • 输入： • 序列信息（目标蛋白） • Distance Map信息 • 蛋白质骨架初始Residue Gas • 输出： • 全原子的位置坐标 • lDDT-Cα（评估建模精度） Jumper, J, et al. Nature, 2021 Chapter2 Alphafold模型架构 再回顾Alphafold2的整体架构 18 更强大的MSA & Templates Evoformer 用于结构预测的 Attention架构 3D Equivariant Structure Module Recycling多轮迭代 多输出 Jumper, J, et al.",
    "file_path": "unknown_source",
    "create_time": 1769004546,
    "update_time": 1769004546,
    "_id": "doc-224a7273b9442fd233a7ec61f799fa44"
  },
  "doc-cfa0a8834ed9c35960b3f0264cbc2d1d": {
    "content": "[DOC_ID: chunk-720cb100]\n[领域: 生物信息学]\nIn order to better understand the current research status of protein structure prediction globally and scientifically promote the high-quality development of the protein structure prediction industry, this paper, based on patent data, analyzes the application trends, regional distribution, major applicants, technological evolution, and opportunities and challenges in the field of protein structure prediction worldwide.",
    "file_path": "unknown_source",
    "create_time": 1769004685,
    "update_time": 1769004685,
    "_id": "doc-cfa0a8834ed9c35960b3f0264cbc2d1d"
  },
  "doc-201fcd200de194e722ae83a479174b8b": {
    "content": "[DOC_ID: chunk-36a1c9f8]\n[领域: 金融学]\ndeep-algotrading 是一个面向学习者的资源库，它涵盖了从简单的回归分析到复杂的时间序列预测技术，如LSTM（长短时记忆网络）以及利用金融数据的强化学习方法",
    "file_path": "unknown_source",
    "create_time": 1769004716,
    "update_time": 1769004716,
    "_id": "doc-201fcd200de194e722ae83a479174b8b"
  },
  "doc-001d35968d5312a9b345486c646a5cf9": {
    "content": "[DOC_ID: chunk-8061036e]\n[领域: 金融学]\n市场参与者对算法交易有着普遍需求。本文从深度强化学习（Deep Reinforcement Learning, DRL）视角分析了算法交易的实现，包括策略输入输出、激励函数与",
    "file_path": "unknown_source",
    "create_time": 1769004752,
    "update_time": 1769004752,
    "_id": "doc-001d35968d5312a9b345486c646a5cf9"
  },
  "doc-a21e1bb9888d36e0f2daf5ed5c43c4f3": {
    "content": "[DOC_ID: chunk-f71ed7c0]\n[领域: 金融学]\n3 B a c k -t e s t i n g r e s u l t s o f h i g h -f r e q u e n c y t r a d i n g s t r a t e g y b a s e d o nA N N p 交易次数 胜率 平均盈利 平均亏损 盈亏比 期望收益 0 . 4 B a c k -t e s t i n g r e s u l t s o f h i g h -f r e q u e n c y t r a d i n g s t r a t e g y b a s e d o nC N N p 交易次数 胜率 平均盈利 平均亏损 盈亏比 期望收益 0 .",
    "file_path": "unknown_source",
    "create_time": 1769004772,
    "update_time": 1769004772,
    "_id": "doc-a21e1bb9888d36e0f2daf5ed5c43c4f3"
  },
  "doc-4c95d5a15ed3f57bef9a360c159df34a": {
    "content": "[DOC_ID: chunk-0c337178]\n[领域: 金融学]\n首頁 » 深度學習（Deep Learning）在量化交易中的實踐. 深度學習作為人工智慧領域的顛覆性技術，正以前所未有的速度改變著各行各業，金融領域的量化交易也不例外。傳統的量化交易策略，主要依賴於技術指標、基本面分析或統計模型，這些方法在處理複雜、非線性的市場數據時往往力有未逮。然而，具備強大學習能力和模式識別特性的深度神經網路，為量化交易帶來了全新的視角與工具 。本文將深入探討深度學習在量化交易中的應用場景、模型選擇、所帶來的顯著優勢，以及目前所面臨的挑戰與限制，旨在為讀者提供一個全面而深入的理解，而非側重於具體的程式碼實現。. ### 深度學習在量化交易中的核心優勢. 1. **擅長非線性預測與複雜訊號捕捉**：金融市場數據的本質是高度非線性且複雜多變的。傳統的線性模型難以捕捉價格變動背後隱藏的微妙關聯。深度學習模型，特別是循環神經網路（RNN）及其變體長短期記憶網路（LSTM），能夠有效地捕捉時間序列數據中的複雜非線性關係和長期依賴性，從而辨識出傳統模型難以察覺的複雜訊號，提升預測的精準度 。. 2. **自動特徵提取能力**：在傳統的量化交易策略開發中，投資者或分析師需要花費大量時間和精力進行「特徵工程」，即人工選擇和設計有助於預測的指標（如移動平均線、RSI等）。而深度學習模型，尤其是卷積神經網路（CNN），具備強大的自動特徵提取能力。它們能夠直接從原始的歷史價格數據、自訂指標，甚至K線圖的視覺模式中，自動學習並提取出高階、抽象且對預測有用的特徵，極大地簡化了模型開發流程 。. 3. **處理高維數據的能力**：量化交易涉及的數據維度通常非常高，包含了來自不同資產類別、不同時間尺度的海量資訊。深度學習模型能夠有效處理這些高維數據，並在學習過程中自動篩選出最重要的信息，避免了傳統統計方法在高維空間中常遇到的「維度災難」問題。. 4. **與其他模型的高度整合性**：深度學習模型並非孤立的工具，它們可以與已有的經典因子模型（如價值因子、動量因子）或統計套利策略無縫結合，作為複合式交易策略的一個關鍵組成部分。例如，深度學習模型可以負責生成更精確的交易訊號，而傳統的資金分配模型則負責最佳化投資組合，這種混合式方法往往能達到更好的整體效果 。. 5. **適應不斷變化的市場環境**：金融市場的規律並非一成不變，模型需要具備一定的適應性。通過持續的數據訓練和模型更新機制，深度學習模型在一定程度上能夠學習市場的新變化，調整其預測策略，從而在動態的市場環境中保持競爭力。. ### 深度學習在量化交易中的主要應用場景. 1. **價格預測與方向判斷**：這是深度學習在量化交易中最直接也是最受關注的應用。利用歷史的價格時間序列數據（例如股票的每日開盤價、收盤價、最高價、最低價、成交量等），深度學習模型，特別是LSTM和Transformer，能夠預測未來的價格走勢或判斷其短期方向。例如，基於LSTM可以預測未來幾分鐘或幾天的收盤價，而CNN則可以分析K線圖的形態，識別出潛在的買入或賣出訊號，用於短期交易策略 。. 2. **高頻交易訊號分類**：在高頻交易（HFT）領域，數據量極其龐大且變化迅速。深度學習模型，如CNN和RNN，能夠處理海量的微觀市場結構數據（例如訂單簿的深度變化、買賣價差的波動、瞬時成交量等），對這些高頻資料進行實時分類，預測極短時間內的價格上漲或下跌訊號。這對於抓住轉瞬即逝的套利機會至關重要 。. 3. **因子強度預測與策略適應**：在多因子模型中，各個因子的有效性並非恆定不變。深度學習模型可以被用來預測某一特定因子（如價值、動量、低波動等）在未來一段時間內的預期收益或有效性強度 。這種預測有助於動態調整因子暴露，使得量化策略能夠更好地適應不斷變化的市場環境，提高策略的穩健性和收益。. 4. **風險預警與異常偵測**：風險管理是量化交易的基石。深度學習在這一領域的應用包括波動率預測、信用風險評估以及市場異常行為偵測。例如，自動編碼器（Autoencoder）模型能夠學習市場數據的正常模式，當偵測到與正常模式存在顯著偏差的數據時，便發出風險預警或識別出潛在的市場操縱、異常波動等，為投資組合提供實時的風險監控 。. 5. **資產組合最佳化**：除了單一資產的預測，深度學習也能應用於整個投資組合的構建與最佳化。通過預測不同資產之間的相關性、波動性以及預期收益，深度學習模型可以幫助構建具備更佳風險收益比的投資組合，實現動態資產配置。. 6. **另類數據分析**：隨著數據來源的多樣化，另類數據（如新聞情感、社交媒體情緒、衛星圖像、供應鏈數據等）在量化交易中的價值日益凸顯。自然語言處理（NLP）作為深度學習的一個重要分支，能夠從非結構化的文本數據中提取市場情緒、公司事件、行業趨勢等資訊。例如，通過分析新聞報導的情感傾向，可以判斷市場對特定事件的反應，進而指導交易決策。. ### 常見的深度學習模型及其特性. 1. **長短期記憶網路（LSTM）**：作為循環神經網路（RNN）的改進，LSTM特別擅長處理和預測時間序列數據。它內部的「門」機制使其能夠有效地解決傳統RNN的梯度消失/爆炸問題，從而能更好地捕捉金融時間序列中長期的依賴關係，非常適用於股價和期貨價格的預測 。. 2. **卷積神經網路（CNN）**：CNN最初主要應用於圖像識別領域。然而，在量化交易中，它可以將股價的K線圖、技術指標的走勢圖等「圖像化」輸入，通過卷積層自動提取局部特徵，進而捕捉出技術形態和趨勢模式。例如，CNN可以用來識別頭肩頂、雙底等經典技術圖形 。. 3. **Transformer**：Transformer模型以其強大的序列處理能力和並行計算優勢，在自然語言處理領域取得了巨大成功。近年來，它也被引入金融時間序列預測中，特別是在捕捉更長的序列依賴性和處理多變量時間序列方面，展現出強大潛力。其注意力機制（Attention Mechanism）使得模型能夠在預測時關注序列中最重要的部分 。. 4. **自動編碼器（Autoencoder）**：Autoencoder是一種無監督學習模型，其主要功能是學習數據的壓縮表示（特徵）。在量化交易中，它可用於數據降維、特徵學習以及異常檢測。例如，在風險管理模組中，Autoencoder可以通過重建誤差來判斷數據是否異常，從而識別出潛在的市場風險或詐欺行為 。. ### 挑戰與限制. 1. **資料品質與樣本數限制**：深度學習模型通常需要龐大且高品質的數據集進行訓練。然而，金融市場的數據往往存在噪音、缺失值，且優質的標註數據相對稀缺。對於某些較為稀有的市場事件或小市值股票，可用的歷史數據可能不足以支撐複雜深度模型的訓練，導致模型泛化能力不足 。. 2. **過度擬合風險**：金融市場具有高度的非平穩性和非決定性。過去的規律不一定能完美復現於未來。深度學習模型由於其複雜性，容易在訓練數據上表現出色，但卻可能「記住」歷史噪音而非真正的市場規律，導致在真實交易或樣本外數據上表現不佳，即發生過度擬合 。因此，嚴格的交叉驗證、樣本外測試和多重回溯測試至關重要。. 3. **解釋性不足（「黑盒子」問題）**：深度學習模型，特別是多層的神經網路，其內部運作機制複雜，難以直觀解釋模型為何做出某一特定決策。這對於需要透明度、可解釋性和合規性審查的金融行業來說是一個顯著的挑戰 。在監管日益嚴格的背景下，缺乏解釋性可能限制其在某些關鍵決策領域的應用。. 4. **部署與效能要求高**：在實時交易，特別是高頻交易策略中，模型的預測速度和執行延遲是決定成敗的關鍵。深度學習模型的訓練和推理往往需要大量的計算資源（如GPU），並對系統架構設計提出極高的要求，以確保低延遲的實時預測能力 。這對於中小型量化團隊來說可能構成較高的技術和成本門檻。. 5. **市場環境的非平穩性**：金融市場是一個動態演化的系統，市場結構、參與者行為和經濟環境都在不斷變化。這使得基於歷史數據訓練的模型難以完美適應未來的市場變化，需要持續的模型監測、再訓練和適應性調整。. ### 實踐建議與未來展望. 1. **輔助預測而非單一決策依據**：將深度學習模型作為生成交易訊號或預測趨勢的「輔助工具」，而非唯一的決策依據。可將其與傳統的量化策略或人工分析相結合，構建更為穩健的混合式策略 。. 2. **模組化策略架構**：將量化策略拆解為獨立的模組，例如「訊號產生模組」和「資金分配模組」。「訊號產生」可以由深度學習模型負責，而「資金分配」則可由傳統的最佳化方法負責。這種模組化設計能提升整個策略的彈性、可維護性和可解釋性 。. 3. **嚴格的驗證流程**：始終堅持嚴格的回溯測試（Backtesting）、樣本外驗證（Out-of-sample testing）和前向測試（Forward testing）。此外，應保留詳細的效能紀錄，並實施模型監控機制，確保模型在實際部署後的穩定性和有效性 。. 4. **探索可解釋性AI（XAI）**：隨著可解釋性AI技術的發展，未來將有更多工具幫助我們理解深度學習模型的決策過程，從而增強模型的透明度和可信賴度。. 展望未來，隨著數據量的持續增長、深度學習演算法的不斷創新以及計算能力的顯著提升，深度學習在量化交易中的應用將會更加深入和精細化。多模態學習（Multi-modal Learning）——結合數值、文本、圖像等多種類型數據——將會是重要的發展方向，以構建更為全面的市場視圖。同時，聯邦學習（Federated Learning）等保護數據隱私的技術，也可能在金融機構間的合作中發揮重要作用。儘管挑戰猶存，深度學習無疑正在重塑量化交易的格局，為投資者提供了前所未有的強大分析和決策工具。. 2025-08-01T07:43:17+00:00By Arthur|Categories: Documentation|0 Comments. ## Related Posts. #### 量化實戰全解析：突破數據極限，構建法人級的通用交易模型. 6 12 月, 2025 | 0 Comments. * 量化交易聖杯：從 Dollar Bars 到三重障礙法，破解數據取樣陷阱. #### 量化交易聖杯：從 Dollar Bars 到三重障礙法，破解數據取樣陷阱. 2 12 月, 2025 | 0 Comments. #### 量化交易必讀：詳解分數差分與滾動驗證，解決模型失憶與作弊. 29 11 月, 2025 | 0 Comments. #### 馴服金融怪獸：特徵工程的「三大流派」與關鍵轉換工具. 28 11 月, 2025 | 0 Comments. #### 為什麼你的模型在股市會失效？——揭開金融數據的三大「隱形陷阱」. 27 11 月, 2025 | 0 Comments. ## Leave A Comment 取消回覆.",
    "file_path": "unknown_source",
    "create_time": 1769004808,
    "update_time": 1769004808,
    "_id": "doc-4c95d5a15ed3f57bef9a360c159df34a"
  },
  "doc-858e73004375c211a45ff338fb81b219": {
    "content": "[DOC_ID: chunk-99997519]\n[领域: 金融学]\n如今，借助先进的GPU 和深度学习技术，交易员能够在搭载 Kinetica 数据库和NVIDIA GPU 的单一高强度计算平台上执行数据探索、模型开发/评分和模型消耗等繁重",
    "file_path": "unknown_source",
    "create_time": 1769004974,
    "update_time": 1769004974,
    "_id": "doc-858e73004375c211a45ff338fb81b219"
  },
  "doc-b7f8fbd8112b2a87e0edad2549687469": {
    "content": "[DOC_ID: chunk-f12dfcfe]\n[领域: 金融学]\n量化交易里能否用到机器学习、深度学习？个人是做量化投资的，之前是做中低频的量化，近两年转到了做高频，也就是日内的T0。天外有天，人外有人，以下的观点是从个人的...-雪球. 发布于 2022-11-27 22:07来自雪球. 来源：雪球App，作者： 基股传声，（https://xueqiu.com/1248905760/236421068）. 天外有天，人外有人，以下的观点是**从个人的研发路径得到的结论**，有问题欢迎讨论。. 首先，来聊一聊机器学习在中低频量化的应用。因为最开始是学的Barra那一套，线性模型。所以自然的想法，**用机器学习去给股票打分**，因子当然还是那一套，只不过省去了因子大类合成这些步骤，毕竟如果合成一下，不到10个因子，好像也不太用机器学习了。. 最后的结果，还是挺让人失望的，尝试了很多机器学习模型，最后失败了，**结论是所有的机器模型都跑不过传统的线性模型。**. 中间其实尝试过很多的方法，更换标签、回归问题变成分类问题、更多数据集、模型集成等等， 无一例外，跑赢不了最简单的线性模型。. 事后来看，个人认为2个原因，**在中低频信噪比太低了，另外就是数据还是太少**。比如放入周度的数据，一年大概40~50个时间点，一年3000个股票算（早期的股票数量少，大约估计这个数字），10年也就120万个点。这个数字对于机器学习太少了，更不要说深度学习了。. 后来转到日内的T0，**机器学习还是展现了比较大的优势**。毕竟，日内的因子上百个，谁也不会想着用线性模型去做。另外深度学习也比机器学习有更多的优势。这一块内容，我就不太方便细讲了。. 个人认为这一段机器学习能够强于线性模型，**一个是信噪比高一点**，毕竟很多人工交易员能够在这个尺度上做的非常的优秀，确定性强一点。另外就是数据量的大幅度增加，一个股票的tick数据，全天4800个点，1天3000只股票，1天的数据量是1440万。**如果是10天，这个数据量就上亿了。**. 这么多数据喂给机器学习还是能学点东西出来的，毕竟很多机会，**人肉眼都能发现。机器学习再学不出来，就是人工智障了。**. 最后，聊一聊机器学习。我一直认为，**机器学习能够把一件事情做好的前提是，人能够把这件事情也做的比较好。如果人都做不好这件事情，指望机器做好是不可能的。**. 比如，在中低频尺度上，没有哪一个主观的研究员、基金经理，做到高胜率、低回撤，**Barra那一套更多是从风险控制上去考虑的。人都做的颤颤巍巍的，机器在有限的外部信息下，能做的非常少。**. 再比如，现在大规模用机器学习、深度学习的互联网行业，很多是用来做推荐、反欺诈等等。**这个的前提是人能做的不错，当然我说的是针对单个样本。**比如一个工程师，看了看这个用户历史的购物记录、浏览页面、消费习惯、资金流水、亲朋好友的购物记录等等，**他个人是可以给这个用户一个比较好的推荐广告的。**. 之所以用机器学习、深度学习，是因为，**工程师数量太少了，用户数量是非常庞大的**。不可能每个用户，配置一个工程师。所以才会用这些算法，给用户做推荐，做到千人千面。假设我们的工程师、客服数量是无限的，那做出来的推荐，绝对比机器给的结果要好很多。. 另外，机器学习用到的特征，**也都是人工给标记和提炼的**（有听到过，用机器学习深度学习挖因子，但这个我还没用过）。以个人做量化的经验来看，**好的因子反而是更加重要的**。有效因子增加带来的改变，大于模型的性能提高。. __ 转发 4__ 讨论 51. ____- [x] 仅在正文下讨论Image 13. 勾选框可设置讨论内容显示范围。讨论内容会自动生成帖子并在站内推荐，勾选「仅在正文下讨论」功能后，讨论内容将仅在当前帖子下讨论区可见。. ### 全部讨论（51）. quant-computer2023-01-11 16:53·江苏. 17年毕业量化至18年，然后离职考虑转机器学习以后再回量化，阴差阳错转成深度学习（图像）至今，看以后能不能用上吧. __ 讨论__ 1. __ 投诉__ 屏蔽用户. angl17992022-11-29 15:27. __ 讨论__ 1. __ 投诉__ 屏蔽用户. 厚积薄发磨一剑2022-11-28 19:55. 噪音（无效数据）太多，而数据清洗预处理等需要专业经验，懂股票又懂机器学习的人太少，美股大涨大跌、周末假期效应、个股利好利空、大盘板块大涨大跌对个股的影响等等等等都会带来噪音，所以太难了. __ 2__ 1. __ 投诉__ 屏蔽用户. 敬畏常识2022-11-28 17:45. 股票的很多信息无法量化，会导致模型的输入失真，结果可想而知。机器学习只是知识的修饰器，本身并不会产生知识。. __ 1__ 1. __ 投诉__ 屏蔽用户. Panel_TraderImage 23: 达人认证2022-11-28 17:39. 线性模型和深度学习都算是机器学习的一个分支。小样本低频数据用线性模型，大样本高频数据用深度学习，介于两者之间的中频数据用介于两者之间的其它机器学习方法，可能比较合适一点。. __ 1__ 1. __ 投诉__ 屏蔽用户. 象牙山李宝库2022-11-28 11:12. > 有朋友在高盛就是搞量化交易的，高频交易海量数据对数学建模能力要求极高，他们是一个几十人团队在搞。 如果你说的是用Python写个程序抓数据做基础分析，那和machine learning没啥关系。. > 基本面量化投资已经兴起好多年了，无论是公募还是私募好多都在做，目前看超额收益还是比较高的。比如财务分析，过去一个研究员看一家上市公司最起码要看几个小时，现在使用量化数据分析，几分钟就出结果了。最前沿的已经开始“非标数据”的量化研究分析。举个简单例子：有些股票市盈率估值非常低，... __ 4__ 1. __ 投诉__ 屏蔽用户. 森林里的男人2022-11-28 11:02. 最后发现就不赚钱，![Image 28: [大笑]](https://assets.imedao.com/ugc/images/face_regular/v1/emoji_02_laughing.png? __ 1__ 1. __ 投诉__ 屏蔽用户. __ 1__ 1. __ 2__ 赞. __ 讨论__ 赞. __ 1__ 赞. __ 1__ 赞. __ 1__ 赞. 感谢分享![Image 44: [牛]](https://assets.imedao.com/ugc/images/face_regular/v1/emoji_07_wonderful.png? __ 1__ 赞. 感谢分享![Image 48: [牛]](https://assets.imedao.com/ugc/images/face_regular/v1/emoji_07_wonderful.png? v=1)![Image 49: [牛]](https://assets.imedao.com/ugc/images/face_regular/v1/emoji_07_wonderful.png? v=1)![Image 50: [牛]](https://assets.imedao.com/ugc/images/face_regular/v1/emoji_07_wonderful.png? > 感谢南哥过来资瓷~![Image 51: [笑]](https://assets.imedao.com/ugc/images/face/v1/emoji_01_smile.png? __ 1__ 赞. __ 1__ 赞. __ 2__ 赞. __ 1__ 赞. __ 讨论__ 赞. Image 64: 企业微信Image 65: 二维码.",
    "file_path": "unknown_source",
    "create_time": 1769005006,
    "update_time": 1769005006,
    "_id": "doc-b7f8fbd8112b2a87e0edad2549687469"
  },
  "doc-ead8239d6c7d35f7d27952face0dd715": {
    "content": "[DOC_ID: chunk-1d1aaa31]\n[领域: 金融学]\n本发明提出了一种基于深度学习的风控建模方法，包括如下步骤：S1，通过云端数据库获取用户数据，将用户数据区分为结构化数据和非结构化数据，并将用户数据进行初步筛选；S2，",
    "file_path": "unknown_source",
    "create_time": 1769005194,
    "update_time": 1769005194,
    "_id": "doc-ead8239d6c7d35f7d27952face0dd715"
  },
  "doc-6d22dc080f5fd339b89bf3f7732fc258": {
    "content": "[DOC_ID: chunk-9e5b3be3]\n[领域: 金融学]\n#### 模态框（Modal）标题. #### 模态框（Modal）标题. ISSN 2097-2326 (Online) ISSN 2096-9732 (Print) CN 10-1738/F. | | | | | --- | --- | | Deep Learning and Corporate Bond Credit Risk Fuwei JIANG, Bailin CHAI, Yihao LIN China Journal of Econometrics . DOI: 10.12012/CJoE2024-0092 | | |.",
    "file_path": "unknown_source",
    "create_time": 1769005220,
    "update_time": 1769005220,
    "_id": "doc-6d22dc080f5fd339b89bf3f7732fc258"
  },
  "doc-fe2c7c19ec682c0e709a26a55e0b8005": {
    "content": "[DOC_ID: chunk-6fafc19e]\n[领域: 金融学]\n传统的疾病风险预测主要基于 Cox 比例风险回归模型（简称 Cox 模型）及逻辑回归模型。例如，[Wang et al. 脑卒中预测模型的评估考虑了校准度（calibration）及区分度（discrimination）。校准度是指预测结果和实际结果的一致度，用 Hosmer-Lemeshow（H-L）统计量评价；区分度采用 c 统计，即受试者工作特征曲线（receiver operating characteristic curve，又称 ROC 曲线）下的面积（AUC）。脑卒中预测模型和脑卒中或死亡预测模型的 H-L 统计量分别为 7.6 和 6.5，脑卒中预测模型的 AUC 为 0.66，而脑卒中或死亡预测模型的 AUC 为 0.70。. 2010 年发表于 KDD 的文章 [Khosla et al. - σ)；建模时尝试了支持向量机（SVM）和基于边缘的删失回归方法。使用 L1 正则化逻辑回归进行特征选择，然后使用 SVM 进行预测，采用 10 倍交叉验证的平均测试 AUC 为 0.764，优于 L1 正则化 Cox 模型。将各种特征选择算法与预测算法相结合的平均显示，保守均值和基于边缘的删失回归相结合在 AUC 评价标准中能达到 0.777，为性能最佳的结果。. 近年来，深度学习技术飞速发展，对图像识别、语音识别、自然语言理解等多个领域产生了颠覆性的改变。对于电子病历数据分析方面，也已有一些研究利用深度学习方法来建立疾病风险预测模型，采用了 CNN 或 RNN 的模型。. 目前更多的人尝试用RNN（Recurrent Neural Network）的方法来分析电子病历中的临床事件之前的时序关系（Temporal Relation）。[Chio et.al 2016] 在心衰（HF，Heart Failure）的预测上率先使用了基于RNN的方法，基于3884个正例和28,903个负例数据，时间跨度从2000年5月，到2013年5月共3年的时间。针对单个临床事件的建模采用了自然语言理解中常用的one-hot向量的方式，把任何一个临床事件都表示成N维的向量，但向量的最后一位是事件发生时间距离预测时间的间隔，类似于一个时间戳（timestamp）。然后使用了GRU（Gated Recurrent Unit，门循环单元）从每个输入的临床事件向量计算相应的隐状态，在最终的隐状态上应用逻辑回归模型计算最后的HF风险概率。跟LR（Logistic Regression），SVM和KNN等多种经典回归或机器学习方法试验对比后发现，基于RNN方法的预测AUC有提高。. 预测建模的方法本身并没有太多的突破：除了 [Khosla et al. 深度学习方法变革了特征提取方法，但降低了可解释性：在特征选择时通过 CNN 或 RNN 的方法对原始特征进行多层的变换，把原始特征映射到新的空间中，提高分类的能力，但同时降低了模型的可解释性。. 2003] Wang TJ, Massaro JM, Levy D, et al. A risk score for predicting stroke or death in individuals with new-onset atrial fibrillation in the community: the Framingham Heart Study. 2010] Khosla A, Cao Y, Lin CC, Chiu HK, Hu J, Lee H. In: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, 2010 Jul 25 (pp. In: Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, 2011 Aug 21 (pp. Risk prediction with electronic health records: a deep learning approach.",
    "file_path": "unknown_source",
    "create_time": 1769005256,
    "update_time": 1769005256,
    "_id": "doc-fe2c7c19ec682c0e709a26a55e0b8005"
  },
  "doc-2103a1c2471cc6226897b2080204b309": {
    "content": "[DOC_ID: chunk-6a44d421]\n[领域: 金融学]\n## 作者. ## 什么是模型风险管理？. 例如，金融机构依靠一系列模型来进行定价、估值以及检测和防止欺诈和洗钱，以及开展其他金融服务。模型的使用往往会带来风险，这使得模型风险管理 (MRM) 成为企业必须考虑的一项重要因素。. 通过 Think 时事通讯，了解有关 AI、自动化、数据等方面最重要且最有趣的行业趋势。请参阅 IBM 隐私声明。. 您的订阅将以英语提供。每份时事通讯都包含取消订阅链接。您可以在此管理您的订阅或取消订阅。更多相关信息，请参阅我们的 IBM 隐私声明。. 另外，如果用于人工智能 (AI) 模型的训练数据集没有评估是否存在偏见，这些 AI 模型可能会产生反映并延续数据中固有偏见的结果。例如，求职者筛选系统可能会青睐男性或年轻的求职者，而医疗保健预测软件在优先考虑需要立即护理的患者时，可能会表现出种族偏见。. 这也是选择正确模型的关键所在。例如，尽管生成式 AI 是最新的技术，但它可能不太适合金融预测，因为其他成熟的模型可以用更少的工作量和更低的成本做到这一点。. 管理模型风险还需要遵循监管准则。例如，在美国，美联储和货币监理署 (OCC) 发布了 模型风险管理监管指南，作为 MRM 框架的基准。. ### 面向企业的生成式 AI 的兴起. ## 用于模型风险管理的 AI. AI 和机器学习也可应用于模型风险管理，尤其是在模型验证（如对市场模型进行压力测试）和实时模型监控期间。以下是模型风险管理中一些常用的机器学习算法和方法：. 模型风险管理软件可以帮助组织更有效地管理模型风险。它提供高级功能，例如模型清单以及跟踪和映射指标、模型和策略，以满足多项监管要求。其他模型风险管理工具还支持 AI 和机器学习模型管理，其功能包括模型监控和模型验证的自动化。. 播客 不仅仅只是聊天机器人：借助生成式 AI 构建真正实用的虚拟代理. 敬请收听，看一看虚拟代理随着在生成式 AI 的助力下变得更加迅速、准确，是否能够因此取代人类。. 深入了解 CEO 如何利用生成式 AI 和应用程序现代化来推动创新并保持竞争力。. 成功案例 IBM 为何采用 OpenPages 实现企业级 GRC. 了解 IBM CIO 组织如何实施 IBM OpenPages 来统一治理、风险与合规管理，简化审计流程，并提升跨业务部门的可视性。. 借助 IBM 利用丰富的数据和强大的 AI 技术来集成优化流程，从而实现业务运营转型。. IBM Cloud Pak for Business Automation. IBM Cloud Pak for Business Automation 是一套模块化的集成软件组件，用于运营管理和实现自动化。. 利用 IBM 行业领先的解决方案实现业务运营转型。通过智能工作流和自动化科技提高工作效率、敏捷性和创新。. 1 “SR 11-7: Guidance on Model Risk Management”, Federal Reserve, 4 April 2011. 2 “Structural causes of the global financial crisis: a critical assessment of the ‘new financial architecture’”, Cambridge Journal of Economics, 1 July 2009. 4 “Model risk – daring to open up the black box”, British Actuarial Journal, December 2015. 5 “Zillow’s home-buying debacle shows how hard it is to use AI to value real estate”, CNN, 9 November 2021.",
    "file_path": "unknown_source",
    "create_time": 1769005396,
    "update_time": 1769005396,
    "_id": "doc-2103a1c2471cc6226897b2080204b309"
  },
  "doc-dd790e22e66ff566c3fcba5e9ed475b3": {
    "content": "[DOC_ID: chunk-0ddba3b7]\n[领域: 金融学]\n訓練模型 與人類從經驗中學習的情況相若，機器學習演算法 會重複進行某項任務，每次都會略為調整其方法， 以期得出較佳結果。這個程序所需要巨大的運算能 力是一般桌面電腦無法應付的。有見及此，我們借 助金管局 「數碼化計劃」 下開發的內部數據科學實驗 室 （可視之為一台超級電腦） ，在數分鐘內就能完成 這項數據處理程序。 在訓練階段，機器學習演算法會從 「訓練組」 隨機抽 取觀察結果，並運用從輸入項得到的資訊構建一個 最能有效解釋輸出項的模型，然後以另一批從 「訓練 組」 隨機抽取的數據評估有關模型的表現。這個過程 會反覆進行，隨着每次重複這個程序，模型參數便 會被進一步微調。這個重複過程會一直持續至它無 法再提升模型的表現為止。 可供選擇的監督式機器學習演算法種類繁多，而每 種都各有優點與限制。本研究使用深度神經網路 （Deep Neural Network，簡稱 「DNN」 ） 來識別可能會 被降級的貸款。DNN是其中一種經常被用作應對現 實世界裏複雜問題 （例如偵測詐騙、圖像識別及自然 語言處理） 的先進機器學習技術。相比其他方法，雖 然DNN模型訓練需要(i)更大量的數據，以及(ii)更強 勁的運算能力，但同時它(i)通常表現較優勝，以及 (ii)可免卻耗時的手動抽取數據特徵過程。 第3頁 實證結果 剔除資料不全的貸款數據後，我們的數據集剩下 2,913,272 項觀察結果 （由2019 年4 月至2022 年 3月） 4。我們將數據集隨機分為兩部分，其中80% （2,330,617 項觀察結果） 為 「訓練組」 ，其餘20% （582,655項觀察結果） 為 「測試組」 。 經全面受訓後的模型表現會以 「測試組」 的數據進行 評估。就本研究而言，由於模型的任務是處理一個 二元分類問題 （貸款會否被降級） ，因此其表現可以 用稱為 「Area Under the Curve of Receiver Operating Characteristic」 （又名 「AUC」 ） 5 的指標來衡量。圖2 比較受訓DNN模型與基準模型的表現(Hosmer et al.",
    "file_path": "unknown_source",
    "create_time": 1769005534,
    "update_time": 1769005534,
    "_id": "doc-dd790e22e66ff566c3fcba5e9ed475b3"
  },
  "doc-d5073dc88c604f20a0eefda8a24a1214": {
    "content": "[DOC_ID: chunk-538751b1]\n[领域: 金融学]\n目前，金融行业对交易欺诈风险的侦测方式主要有两种：基于规. 则和基于机器学习算法。基于规则的方法是通过不断建立、更新. 基于交易行为特征的规则库，并在交易发生时，通过",
    "file_path": "unknown_source",
    "create_time": 1769005629,
    "update_time": 1769005629,
    "_id": "doc-d5073dc88c604f20a0eefda8a24a1214"
  },
  "doc-a6a52780a97b8eca9204ad8ed2f86840": {
    "content": "[DOC_ID: chunk-97d34885]\n[领域: 金融学]\n本发明公开了一种基于深度学习的欺诈应用检测方法，包括步骤：1)获取移动广告数据，进行预处理；2)提取结构数据和样本数据；3)基于结构数据构建图并获取图嵌入特征，基于样本",
    "file_path": "unknown_source",
    "create_time": 1769005662,
    "update_time": 1769005662,
    "_id": "doc-a6a52780a97b8eca9204ad8ed2f86840"
  },
  "doc-c00652dd3948d32546676fffc8064300": {
    "content": "[DOC_ID: chunk-86f2ceb0]\n[领域: 金融学]\nAI Agent风控模型是一种基于深度学习的实时欺诈检测算法实现。它通过分析用户行为、交易模式和历史数据，识别潜在的欺诈行为，从而帮助企业保护其资产和信誉。这种模型可以",
    "file_path": "unknown_source",
    "create_time": 1769005743,
    "update_time": 1769005743,
    "_id": "doc-c00652dd3948d32546676fffc8064300"
  },
  "doc-ac1ed9e88e425ab885199259d92315e5": {
    "content": "[DOC_ID: chunk-37527ca8]\n[领域: 金融学]\n在进行欺诈检测时，从机器学习转向深度学习可能会产生重大的业务影响。深度学习模型提高了检测欺诈活动的准确性，实现了实时检测并减少了误报。这些模型具有高度的可扩展性，",
    "file_path": "unknown_source",
    "create_time": 1769005798,
    "update_time": 1769005798,
    "_id": "doc-ac1ed9e88e425ab885199259d92315e5"
  },
  "doc-7ae4a8c47961536abb443d07e622440b": {
    "content": "[DOC_ID: chunk-f410bb2d]\n[领域: 金融学]\n本指南演示了一种基于深度学习图神经网络的端到端、近乎实时反欺诈系统。该蓝图架构使用深度图库（DGL）根据表格数据构造异构图，并训练图神经网络（GNN）模型以检测欺诈性",
    "file_path": "unknown_source",
    "create_time": 1769005847,
    "update_time": 1769005847,
    "_id": "doc-7ae4a8c47961536abb443d07e622440b"
  },
  "doc-b3378e1cd34422d214370ca24a5f17ef": {
    "content": "[DOC_ID: chunk-6810cdce]\n[领域: 金融学]\n利用图数据库支持实时反欺诈的检测. 本实施指南介绍了在Amazon Web Services (AWS) 云中部署基于深度学习图神经网络的实时反欺诈解决方案的架构注意事项和配置步骤。 它",
    "file_path": "unknown_source",
    "create_time": 1769005884,
    "update_time": 1769005884,
    "_id": "doc-b3378e1cd34422d214370ca24a5f17ef"
  },
  "doc-98217948cb9ad0bfdb25f3db0b7cd900": {
    "content": "[DOC_ID: chunk-3c401ea4]\n[领域: 金融学]\n针对新的深度学习欺诈侦测模型，三方工程师分别针对伪卡欺诈. 侦测，骗保检测等银行、保险业务常见应用场景，采用真实数据进. 行了多项仿真验证。GBDT→GRU→RF三明治结构欺诈侦",
    "file_path": "unknown_source",
    "create_time": 1769005937,
    "update_time": 1769005937,
    "_id": "doc-98217948cb9ad0bfdb25f3db0b7cd900"
  },
  "doc-ef7f8964a21d37b73c54e1e69f0b0005": {
    "content": "[DOC_ID: chunk-29976635]\n[领域: 金融学]\n本页面提供全球最准确、精炼的论文《Credit Card Fraud Detection: A Deep Learning Approach》摘要。通过Moonlight这款AI研究助手，您可以轻松快速地理解所阅读的所有论文。通过https://www.themoonlight.io/ 安装Chrome扩展，或直接在网页上传文件即可使用。Moonlight针对您的需求提供以下功能： - 文本解释： AI帮助您轻松理解复杂概念和段落。 - 图片解释： 一键解释图片、表格和公式。 - AI对话： 与AI互动，深入探讨论文内容。 - 智能引用： 无需跳转参考文献，即可查看引用论文的信息（标题、作者、摘要）。 - 翻译： 快速翻译陌生的单词、句子，甚至整页内容。 - 自动高亮： AI自动突出论文核心内容，帮助您迅速掌握原创性、方法和结果。 - 外部链接解释： AI分析外部来源并解释其与文档的关联性。 - 标记功能： 高亮重要句子并添加注释，创建个性化的研究笔记。 - 保存与分享： 将文档保存至个人库中，便于分享。 - 学术深度搜索： 根据已保存的文档推荐相关论文。. ## [论文评述] Credit Card Fraud Detection: A Deep Learning Approach. [# [论文评述] Credit Card Fraud Detection: A Deep Learning Approach](/file? url=https%3A%2F%2Farxiv.org%2Fpdf%2F2409.13406). 本文的主题是利用深度学习来检测信用卡欺诈，重点在于解决在金融交易过程中由于欺诈行为导致的高昂成本。文章首先概述了信用卡的广泛应用及其引发的欺诈问题，指出了传统欺诈检测系统面临的挑战，如概念漂移、类别不平衡和验证延迟。当前的许多欺诈检测系统依赖于人工智能、模糊逻辑和机器学习，但这些方法并未能有效解决所有的相关挑战。. ### 核心方法论. * 使用多层前馈神经网络（Multi-layer Feed-forward Neural Network）作为第一个深度学习模型。该模型通过将输入数据馈送到第一层，然后通过非线性变换将前一层的特征转化为更具体的特征，从而实现特征的学习层级化。. * 引入了深度自编码器（Deep Auto-Encoders）作为无监督学习的方法，通过在不使用标签的情况下学习正常用户的交易模式，从而有效识别出异常模式。这尤其适用于类别严重不平衡的信用卡欺诈数据。. * 采用自然启发的蝙蝠算法（Bat Algorithm）来优化深度学习模型的性能。这种算法模拟蝙蝠在捕猎时发出超声波的行为，用于特征选择和训练成本的降低。. ### 实验设计. **实验一：多层前馈神经网络**根据金融数据微调前馈神经网络，调整激活函数、迭代次数和隐藏层的大小，目标是为欺诈检测系统提供设计建议。通过使用验证集确定标准化方法及最佳激活函数。实验中发现Tanh函数在结合Z-score标准化时表现优于Logistic Sigmoid函数。. **实验二：深度自编码器**通过设计深度自编码器来检测信用卡数据集中的欺诈模式，训练过程中将数据划分为75%的训练集和25%的测试集，以重构法识别异常数据。损失函数采用均方误差（MSE）。. **实验三：性能优化**使用蝙蝠算法优化欺诈检测系统，检查训练成本和复杂性。该算法通过更新蝙蝠的位置和速度，寻找到全局最佳位置，进而选择更优特征。. **实验四：与其他算法比较**比较不同的Scikit-learn算法在分类欺诈模式方面的表现，包括AUC得分和混淆矩阵，利用过采样和欠采样方法处理类别不平衡，展示不同算法的优劣。. ### 实施与执行. 数据集使用Kaggle的信用卡欺诈检测数据集，共284807个实例，31个特征。数据标准化采用Z-score标准化和Min-max标准化，以不同的激活函数（如Tanh和Sigmoid）进行实验，确保选择合适的方法构建模型。. ### 结果与分析. 实验结果表明，深度学习模型在信用卡欺诈检测方面的准确率达到了96.21%。通过蝙蝠算法优化特征选择后，模型在AUC得分上更是超越了传统方法。同时，比较的不同算法也展示了深度学习在处理海量数据和非平衡类问题上的优势。. ### 结论. 提出的基于深度学习的欺诈检测方法能够显著提高性能，并且在没有足够标签数据的情况下依然有效。这样的系统可以实现更高程度的自动化，从而降低人工干预带来的时间和成本，同时减少误报率，增强对于欺诈行为的检测能力。尤其是在大数据和深度学习快速发展的背景下，本文的方法展现了未来在金融领域的广阔前景。. ## 免费AI PDF查看器 革新您的论文阅读方式. / CEO Younghyun Chung / Business Registration Number 271-86-02206. 6F, 11-8 Teheran-ro 77-gil, Gangnam-gu, Seoul, Republic of Korea, 06159. Contact 02-6925-6978 E-mail: moonlight@corca.ai. © 2026 Corca, Inc. All rights reserved.",
    "file_path": "unknown_source",
    "create_time": 1769005999,
    "update_time": 1769005999,
    "_id": "doc-ef7f8964a21d37b73c54e1e69f0b0005"
  },
  "doc-f8a385ef6ec16dbfeda21e87bfc82a5e": {
    "content": "[DOC_ID: chunk-6d20e98f]\n[领域: 金融学]\nDOI: 10.12677/csa.2025.1511295 基于动态时序图神经网络的信用卡欺诈检测 方法研究 李 峰1,2，李惠先2，陈 雪2，韩祝华3* 1河北金融学院河北省金融科技应用重点实验室，河北 保定 2河北金融学院金融科技学院，河北 保定 3河北金融学院统计与数据科学学院，河北 保定 收稿日期：2025年10月12日；录用日期：2025年11月12日；发布日期：2025年11月24日 摘 要 信用卡欺诈是金融科技领域的重大挑战。现有基于静态快照的图神经网络方法难以有效捕捉欺诈行为在 连续时间上的动态演化模式。 当前研究多将动态交易流离散化为静态图， 导致时序信息丢失和检测延迟。 本文提出一种新的动态时序图神经网络模型(DTGNN-FD)。该模型采用连续时间动态图范式，通过门控 记忆机制和时间感知注意力来持续更新用户表征，从而对流式交易进行实时风险建模。在两个真实金融 欺诈数据集上的实验表明，所提模型在F1-Score和Recall等关键指标上优于多种基线模型，表现出识别 复杂时序模式的潜力。本研究为实时信用卡欺诈检测提供了一种有效的解决方案。 关键词 欺诈检测，图神经网络，动态图，时序分析，循环记忆模块 Credit Card Fraud Detection Approach Based on a Dynamic Temporal Graph Neural Network Feng Li1,2, Huixian Li2, Xue Chen2, Zhuhua Han3* 1Hebei Key Laboratory of Financial Technology Application, Hebei Finance University, Baoding Hebei 2School of Financial Technology, Hebei Finance University, Baoding Hebei 3School of Statistics and Data Science, Hebei Finance University, Baoding Hebei Received: October 12, 2025; accepted: November 12, 2025; published: November 24, 2025 *通讯作者。 李峰 等 DOI: 10.12677/csa.2025.1511295 174 计算机科学与应用 Abstract Credit card fraud poses a significant challenge in the field of financial technology.",
    "file_path": "unknown_source",
    "create_time": 1769006205,
    "update_time": 1769006205,
    "_id": "doc-f8a385ef6ec16dbfeda21e87bfc82a5e"
  },
  "doc-fd544a0511ae2b1911f2ec7bdc2235ae": {
    "content": "[DOC_ID: chunk-01793f07]\n[领域: 语言学]\n直到2013 年，随着越来越多的研究者使用深度学习模型来处理自然语言，NNLM 模型才被重新发掘，并成为使用神经网络建模语言的经典范例。NNLM 模型的思路与统计语言模型保持一致",
    "file_path": "unknown_source",
    "create_time": 1769006379,
    "update_time": 1769006379,
    "_id": "doc-fd544a0511ae2b1911f2ec7bdc2235ae"
  },
  "doc-1eb9c8a9254475c3a6672347d5ac549f": {
    "content": "[DOC_ID: chunk-1f20b2ef]\n[领域: 语言学]\n由于神经网络各层之间、各个神经网络之间的“交流语言”为向量，所以深度学习工程师可以轻松地将多个神经网络组合起来，形成一种端到端的设计。比如之前谈到的情感分析案例中，",
    "file_path": "unknown_source",
    "create_time": 1769006396,
    "update_time": 1769006396,
    "_id": "doc-1eb9c8a9254475c3a6672347d5ac549f"
  },
  "doc-747e06575d3f3c246eac89670d12c86e": {
    "content": "[DOC_ID: chunk-0fdee808]\n[领域: 语言学]\n人与人之间需要交流。 出于人类这种基本需要，每天都有大量的书面文本产生。 比如，社交媒体、聊天应用、电子邮件、产品评论、新闻文章、 研究论文和书籍中的丰富文本， 使计算机能够理解它们以提供帮助或基于人类语言做出决策变得至关重要。. *自然语言处理*是指研究使用自然语言的计算机和人类之间的交互。 在实践中，使用自然语言处理技术来处理和分析文本数据是非常常见的， 例如 8.3节的语言模型 和 9.5节的机器翻译模型。. 要理解文本，我们可以从学习它的表示开始。 利用来自大型语料库的现有文本序列， *自监督学习*（self-supervised learning） 已被广泛用于预训练文本表示， 例如通过使用周围文本的其它部分来预测文本的隐藏部分。 通过这种方式，模型可以通过有监督地从*海量*文本数据中学习，而不需要*昂贵*的标签标注！. 本章我们将看到：当将每个单词或子词视为单个词元时， 可以在大型语料库上使用word2vec、GloVe或子词嵌入模型预先训练每个词元的词元。 经过预训练后，每个词元的表示可以是一个向量。 但是，无论上下文是什么，它都保持不变。 例如，“bank”（可以译作银行或者河岸）的向量表示在 “go to the bank to deposit some money”（去银行存点钱） 和“go to the bank to sit down”（去河岸坐下来）中是相同的。 因此，许多较新的预训练模型使相同词元的表示适应于不同的上下文， 其中包括基于Transformer编码器的更深的自监督模型BERT。 在本章中，我们将重点讨论如何预训练文本的这种表示， 如 图14.1中所强调的那样。. 图14.1 预训练好的文本表示可以放入各种深度学习架构，应用于不同自然语言处理任务（本章主要研究上游文本的预训练）¶. 图14.1显示了 预训练好的文本表示可以放入各种深度学习架构，应用于不同自然语言处理任务。 我们将在 15节中介绍它们。. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs） Next.",
    "file_path": "unknown_source",
    "create_time": 1769006413,
    "update_time": 1769006413,
    "_id": "doc-747e06575d3f3c246eac89670d12c86e"
  },
  "doc-afea3fd8f33940c9cfe692c11adfe3b5": {
    "content": "[DOC_ID: chunk-fa1d0700]\n[领域: 语言学]\n# 深度學習：自然語言處理的秘密武器. 自然語言處理 ( **Natural Language Processing** **，簡稱** **NLP** ) 正逐步改變人工智慧理解人類的想法與行為的方式，而其中深度學習更扮演著至關重要的角色。在此演變下，品牌與行銷應如何從中受益？. 有時，在嘗試建構人工智慧的過程中，才能體會到人類心智的複雜性。自然語言處理便是一個很好的例子，該技術致力於探討人工智慧如何辨別人類語言中的細微差異。. 以英文單字為例，「 plaster 」（灰泥，一種用於覆蓋牆面的塗料）和「 plasters 」（ OK 繃）二字間的差異也許看似簡單，但是對人工智慧來說，這中間的過程卻一點都不輕鬆。研究人員必須事先準備大量的定義、與「 plasters 」有關的語料以供人工智慧學習，才能有足夠的能力在真實情境中區辨消費者所搜尋的究竟是居家裝潢用的塗料，或是保護傷口用的急救產品。. Appier 的首席人工智慧科學家孫民指出，為了讓傳統方法發揮作用，「研究人員往往須明確定義每個詞彙的含義，以及詞彙間的關係。」傳統方法仰賴高度的人為干預來定義各種例外情況，然而這樣的作法會削弱人工智慧的運作效率。. 傳統方法雖然讓人工智慧具備基礎的語言處理能力，然而，其學習過程偏向採用「死記硬背」的方式。換言之，研究人員不但需要預先定義許多文意並且統整許多定義間的關係，在這些事務中，也需要大量人為介入處理。因此，透過傳統方法建構而成的人工智慧，往往缺乏商務應用方面的語言處理能力。. 人類的溝通方式要比單純死背複雜得多。語言本身具有高度「針對性」，而這種存在於字裡行間外的特定情境（例如作者與其書寫對象間的關係），會影響溝通時所選用的詞彙、句法、拼字，以及標點符號的呈現方式。. 孫民還表示：「語言隨著時間不斷演變，人們在正式文件中和網路上所使用的語言有著明顯差異。想讓傳統方法在現今的語言環境中發揮作用，除了得建構專門的書面英語、美式英語和澳式英語等知識庫外，還須擁有一個包含 Reddit 這類網站中常見用語的專門知識庫。然而，傳統方法在應用上並不具備這樣的可擴充性。」. 深度學習正好解決了這方面的問題。在自然語言處理領域中，深度學習機制讓人工智慧能夠透過觀察單詞或短句在段落中的使用方式，來理解這些語言的含義。也就是說，單詞或短句的意義以及字詞之間的關係，是直接從原始段落中習得，而非經由預先定義的文章內容，或是仰賴人力來進行闡釋或定義關係。這意味著當消費者搜尋「 clear plaster 」（透明 OK 繃）的時候，人工智慧能夠明白其所尋找的是急救類的物品，而不是裝潢用的材料；這樣的運作機制讓自然語言處理更貼近人類的學習模式。. 孫民指出，在電腦「學會每個獨立詞彙所代表的詞向量」前，深度學習機制仍須透過大量文本（語料庫）的分析來進行語言學習，而這些文本通常來自新聞網站、維基百科和 Reddit 網站上的評論。 詞向量 的概念指的是將相似的單詞映射到向量空間中，以觀察其彼此靠近的程度，並藉此判斷在同一上下文中的不同單詞，在語義上是否相近。. 詞向量的概念在關鍵字行銷上的作用尤其明顯。在單憑人力的情況下，行銷人通常需預先根據行銷目標，自行構思一份關鍵字清單。然而，在深度學習的幫助下，企業只需 在人工智慧中輸入一些「種子資料」， 「然後這些資料便能 在向量空間中尋找相似的關鍵字。 」孫民提到。. 假設有一家旅遊公司正在針對潛在顧客擬定關鍵字清單，他們可能會在清單中加入「度假」、「假期」、「班機」、「郵輪」、「短期旅遊」等關鍵字眼，這時精通自然語言處理的人工智慧工具還能從合適的上下文中，判定搜尋「宿霧」和「菲律賓」二字的使用者是否有意出外旅行。. 此外，建立在深度學習上的自然語言處理機制可以 更精準地進行情感分析 ，能根據關鍵字來判斷使用者在搜尋當下所抱持的是正面的或負面的感受。若使用者所搜尋的關鍵字是「宿霧」和「地震」，而不是「宿霧」和「潛水」，那麼人工智慧便會判定該使用者並非合適的旅遊行銷對象。孫民還表示：「透過自然語言分析，企業在消費者情感分析的精確度可以提高 10% 到 20% 。」. 基於深度學習的自然語言處理機制，有助於行銷人深入了解消費者的需求，從而形成更富價值的洞察。不但能協助企業擴大行銷活動所能觸及的對象，還能透過相關性更高的優惠訊息來抓住其目光。若行銷人一開始僅使用「旅遊」這個關鍵字來定義目標市場，那麼很有可能會錯失與搜尋「宿霧」和「潛水」二字的消費者互動的機會，但詞向量的應用卻可避免這類的情況發生。在掌握到消費者所使用的關鍵字後，企業便可針對特定地點推出相關的優惠方案，或是推薦東南亞其他潛水勝地的旅遊行程。. 儘管具備不少優勢，但目前的自然語言處理技術仍有不足之處，尤其是在文本生成方面。孫民坦言：「其在語言生成上的表現不如語言理解方面的表現穩定，有時會產出預想之外的內容，因此需要透過人力進行再確認的動作。」. 另一項挑戰與用來訓練人工智慧的資料來源有關，孫民補充道：「由於人工智慧是直接使用研究人員所提供的資料來進行學習，因此若資料本身有問題或者內容有誤，那麼人工智慧產出的結果也會跟著受到影響。」換句話說，這部分也需要藉由人力來進行資料來源的查證工作。. 即便如此，孫民仍充滿自信地表示，未來基於深度學習的自然語言處理機制有望在經過微調後，逐步降低人為干預的必要性。他也指出，該技術至目前為止「已成功為企業節省人力資源，並且使行銷工作更具可擴充性。」在接下來的數年內，人類將能更放心地讓聊天機器人處理複雜的查詢問題，以及推動進一步的行銷自動化工作。. 就現階段而言，基於深度學習的自然語言處理所能為企業和行銷部門提供的協助（精準識別更廣泛的目標受眾，並根據其需求提供相關性高的優惠訊息），證明了其確實是一項富有價值的投資。人工智慧接下來的發展，著實令人期待。. **想進一步了解深度學習如何幫助企業獲取顧客，以及改善與顧客間的互動關係？如須獲得更深入的洞察，歡迎下載Appier最新白皮書：「** **鎖定高價值應用程式使用者** **:** **運用深度學習提高獲取新客的行銷成效** **」。**. ### 超越一對一個人化：運用 AI 打造廣告創意素材，實現「千人千面」顧客旅程 ### Agentic AI? 比聊天機器人跟 Co-pilot 更會主動思考、計畫與行動的 AI 解決方案 ### 什麼是內容相關廣告 (Contextual Advertising)？. ## 訂閱 Appier 部落格. 一手掌握最新行銷科技趨勢、自動化行銷、產業趨勢、最佳實踐案例、以及 Appier 觀點。.",
    "file_path": "unknown_source",
    "create_time": 1769006475,
    "update_time": 1769006475,
    "_id": "doc-afea3fd8f33940c9cfe692c11adfe3b5"
  },
  "doc-38b1076024c94a070f64f72dcaea1102": {
    "content": "[DOC_ID: chunk-f7e042be]\n[领域: 语言学]\nNLP 预处理是指将原始文本准备好，以供程序或机器学习模型分析的过程。要将文本转换成深度学习模型更容易分析的格式，就必须对其进行NLP 预处理。 有几种NLP 预处理",
    "file_path": "unknown_source",
    "create_time": 1769006668,
    "update_time": 1769006668,
    "_id": "doc-38b1076024c94a070f64f72dcaea1102"
  },
  "doc-b572e7e95c9e5e07043de0b3cb587187": {
    "content": "[DOC_ID: chunk-fec3396a]\n[领域: 语言学]\n自然语言处理（NLP）结合计算语言学、预测性人工智能和深度学习模型处理人类语言。 计算语言学. 计算语言学是使用计算机和软件工具理解和构建人类语言模型的科学。研究",
    "file_path": "unknown_source",
    "create_time": 1769006691,
    "update_time": 1769006691,
    "_id": "doc-b572e7e95c9e5e07043de0b3cb587187"
  },
  "doc-7890c6c195439c2a8d6fe49741a8156c": {
    "content": "[DOC_ID: chunk-8f2d4779]\n[领域: 语言学]\n## 深度学习、机器学习与NLP的前世今生. 2 评论 16426 浏览 72 收藏 29 分钟. > 随着深度学习的发展，自然语言处理领域的难题也得到了不断突破，AlphaGo项目的主要负责人David Silver曾说“深度学习 （DL）+ 强化学习 （RL）= 人工智能（AI）”。目前深度学习在自然语言处理上主要有哪些应用？在工程实践中是否会有哪些瓶颈？. ## 一、为什么做文本挖掘. 简单来说：NLP的目的是让机器能够理解人类的语言，是人和机器进行交流的技术。它应用在我们生活中，像：智能问答、机器翻译、文本分类、文本摘要，这项技术在慢慢影响我们的生活。. NLP的发展历史非常之久，计算机发明之后，就有以机器翻译为开端做早期的NLP尝试，但早期做得不是很成功。直到上个世纪八十年代，大部分自然语言处理系统还是基于人工规则的方式，使用规则引擎或者规则系统来做问答、翻译等功能。. 第一次突破是上个世纪九十年代，有了统计机器学习的技术，并且建设了很多优质的语料库之后，统计模型使NLP技术有了较大的革新。接下来的发展基本还是基于这样传统的机器学习的技术，从2006年深度学习开始，包括现在图像上取得非常成功的进步之后，已经对NLP领域领域影响非常大。. 当年上小学时有一本书叫《字词句篇与达标训练》，里面讲了字、词、句、篇，我们开始学写字，词是最基础的一级，中文的一个字比英文的一个字母的语义要丰富的多，但表义能力仍然较差。所以中文一般的处理情况都是按照词级别，词级别的分析就有了中文分词、有了命名实体识别这样的层次来做底层处理。. 在这个底层处理之上是段落级别，是一句话、一段话、短的文本，对这个级别文本做法又对应了相关的技术，包括：依存文法分析、词位置分析、语义归一化、文本纠错等等功能。但是这个功能也是为它更上级的服务去服务的，达观称之为“篇章”级的应用。. 大部分同学平时做比赛、做项目关注的点最多是在“篇章”级的应用，底下这些中文分词等都已经有很好的工具了，不用再从头到尾去开发，只要关心上层的应用，把底下的工具用好，让它产生需要的Feature，来做分类、主题模型、文章建模，这种比较高层次的应用。. 所以，要做好NLP，包括我们公司在内，这三个级别的技术都是自己掌握的。但是如果个人学习使用是有权衡的。某个同学的某一个技术特别好也是OK的，因为现在开源工具，甚至商用工具有很好的效果。如果不要求精度特别高或者有特殊的要求，用这些工具一般是可以达到你的要求。. ## 二、为什么要用深度学习？. 深度学习的发展与应用要有一定的基础，上个世纪末互联网时代到来已经有大量的数据电子化，我们有海量的文章真是太多了。有这样的数据之后就要去算它，需要算法进步。以前这个数据量规模没法算，或者数据量太大算起来太慢。就算有更好的算法还是算得很慢时，就需要芯片的技术，尤其我们现在用并行计算GPU，这个加速对各种各样的算法尤其深度学习的算法影响速度非常大。. 所以一定要有这三个基础——数据、算法、芯片，在这三个核心基础上面做更高级的应用，涉及人的感官——听觉、视觉、语言这三个感官，语音的识别、计算机的视觉、自然语言的处理。. 很多同学会把深度学习和机器学习划等号，实际上它们不是等号。AI的概念非常大，比如：我们用的Knowledge Base知识数据库也是一种AI，它可能没有那么智能。机器学习是AI其中的一小块，而深度学习用又是机器学习中的一小块，我们常见的CNN、RNN都属于深度学习的范畴。. 同时，也做Logistics Regression知识图谱，因为知识图谱是NLP中一个很重要的应用，无论是生成知识图谱，还是用它做像问答等其他应用都是会用到的。. 比如：做一个分类的问题，**这两个分类问题唯一的区别就是特征工程的区别。**我们用经典的机器学习算法是上面这条路，输入数据后大家就开始（包括打比赛也）做各种各样的特征工程。有了这样的特征，我们还要根据TF-IDF、互信息、信息增益等各种各样的方式去算特征值，或对特征进行过滤排序。传统机器学习或经典机器学习90%的时间，都会花在特征工程上。. **而Deep learning颠覆了这个过程，不需要做特征工程。**需要各种各样的特征，比如：需要一些长时间依赖的特征，那可以用RNN、LSTM这些，让它有个序列的依赖；可以用局部的特征，用各种各样的N元语法模型，现在可以用CNN来提取局部的文本特征。. * 很多实际场景是挖掘出一个好的特征或者对我们系统贡献很大的特征，往往比选择算法影响还大。用基本的特征，它的算法差距不会特别大，最多也就10个点，主要还是特征工程这块，而深度学习很好的解决了这个问题。. 2. 预处理，预处理很重要，大家在工作中拿到的数据都是经过清洗过程的，“达观杯”算法大赛的数据是我们帮大家清洗过的。比赛中做到的字、词都是各种各样的ID，是预处理的一部分。. 4. 后处理，比如业务需要分类，分类最终的结果是通过不通过，这都是后处理的过程。. 5. 应用，应用的方向有文本分类、情感分析、中文分词、命名实体识别、机器翻译。. 几乎所有任务都可以拿Deep learning来做，它的适应性和它的广度非常好，例如：传统的机器学习做文本分类需要特定的算法，而这个算法不可能做命名实体识别的事情。. 在深度学习在NLP领域火起来之前，最有代表性的一个研究，对每个人影响最大的工作就是Word2Vec，把一个字、一个词变成向量来表示，这是对我们影响非常大的工作。. 在之前我们以词为单位，一个词的表示方式几乎都是one hot。 one hot序列有一个致命的缺点，你不能计算相似度，所有人算出来都是“0”，都是一样的，距离也都是一样的，因此它不能很好的表示词之间的关系。. 第一，这个词如果有1万维的话，1万维本来存储它就是一个非常稀疏的矩阵、而且很浪费，我们就可以把它变得更小，因为我们的Word2Vec里面一般的向量都在 512以内。. 这个维度的向量相对1万维来说已经是比较低维的空间，它里面存的是各种的浮点数，这个浮点数看起来这三个向量好像每个都不一样，但是实际去计算，发现这三个向量之间的相似度非常高，一个是相似度可以判断它的相似性，另外是判断它们的距离。. 威海、潍坊、枣庄这几个城市在空间上离得非常近，它们的数值也非常近。它对于我们实际工作的好处是增强了我们的泛化能力，这是一个很难做的事情。. * 第二，有了这样的表示之后可以做语义的计算，包括山东-威海约等于广东-佛山，两个向量之间是约等于的，语义的东西不太好解释，但是人知道这是怎么回事，语义相近就是Word2Vec最大的帮助。. 有了表示学习之后，下一步就是常见的各种网络结构，这些都是非常常见的，比如：CNN、GRU、RNN、Bi-LSTM。LSTM也是一种RNN，Bi-LSTM也是一种LSTM，只不过Bi是双向的LSTM，它可能学到前后上下文的特征和语义。. GRU的好处是比LSTM这种算法稍微简单，**所以在层次比较深的时候或者比较复杂的时候，用它这个单元的运算效率会高一点、快一点，但它实际精度可能稍微差一点。**所以模型那么多，怎么来选是很重要的，要根据大家的实践去看看怎么用。. 这张图中间的九宫格就是个卷积格，每个数字相当于一个过滤器。它做的事情对一个图像来说，是把九宫格和图像中对应的矩阵相乘，乘出来一个结果，得到卷积之后它就开始平移，平移的步长是可选择的，一般我们都是一步一步平移过去。. 是有意义的，它可能学到像直线、弯曲等特征，很简单的图形特点，然后它会得到一层。. 我们这只是一层，它在CNN里面尤其图像识别网络，大家都听过“大力出奇迹”，网络越深效果越好，因为它经过一层一层的学习，可以把每一层的特征进行浓缩。. 简单的像素没有任何的表义能力，到第一层浓缩之后它有一些点线的能力，再往上浓缩可能就有弧线的能力，再往上浓缩它越来越复杂，可以做到把一个像素这个没有意义的东西变成有意义的东西。可以它可以看成是一层层的过滤，选出最好的特征结果，这是卷积的原理。卷积不仅仅在图像里，在文本里用得也非常好。. 看下面这张图，它比传统的RNN多了一个所谓的细胞状态，我翻译成“细胞”，一般也叫“cell”，它多了一个存储长期信息的“cell”状态。. 看输入Ht-1和Xt，Ht-1是上一个时刻这个cell隐状态的输出，Xt是当前输入，它们两个通过这个函数计算后的输出是0-1之间的某一个值。. * 第二步，来了一些新的信息，不能只是把老的细胞状态更新，还要把新的信息添进去，通过这两个公式来添，第一个公式输出0-1的系数，第二个公式要选出量是多少。. 有了第一步和第二步之后就开始第三步细胞状态更新，第一步的输出0-1和Ct-1相乘决定上一时刻这个细胞状态留下多少。第二步算出来系数和信息量相乘决定留下多少新增信息，然后把上一步剩下的和这一步新增的加起来，做一个更新，这个更新就是现的cell状态值。. 现在单元的状态更新完了，下一步就要输出，这个输出有两个：第一个，对外是一样，还是隐层的输出Ht，这个输出和前面讲的RNN隐层输出是一样的，只是多了一步内部更新。决定留下多少老的信息，决定留下多少新的信息，再把老的信息和新的信息加起来就是最终的结果。. 长短期记忆网络可以把很长很远的语义通过Ct把信息记下来，而RNN本来就很擅长记忆这种比较近的信息，所以LSTM长短信息都能记下来，对后面特征的选择、模型的输出选择有很大的帮助。. ## 三、深度学习的具体应用. * **one to one：**图像分类，对于我们来说，图像就是二维矩阵，就是像素XY坐标的矩阵，输入之后经过神经网络的处理输出蓝色，这是分类问题。. * **one to many：**图像描述，最早看到这个应用觉得很神奇，一个图像进来了，它告诉我图像上有一个狗、一个猫站在车旁边，这就是一个图像描述的过程，它可以把图像变成很多输出，这就是one to many的问题。. * **many to one：**输入的是一个序列，文字等都是这样一个序列，这个序列输出之后做文本分类、情感分析，它最终都给出来这样一个结果，它们都属于“多到一”的过程。. * **many to many：**这有两张图，它们的区别是：第一张图红色输入的时候没有蓝色的输出，而是等第三个红色输入的时候蓝色开始输出，它是一个异步的序列到序列的问题，异步到一个序列问题常见的例子就是机器翻译。机器翻译是看到很多上下文才能决定开始怎么翻译，不能光看到China就翻译成中国，因为英文的表述和中文表述顺序有时候不同，需要看到上下文之后再去翻译。但是有异步就有同步，我们写代码经常异步和同步问题，其实这边也一样，序列到序列的同步关系就是我们经常见的，所有选手应该都知道的序列标注问题，序列标注问题的上面就是各种各样的应用。. 本次讲的是文本，所以我着重会讲many to one和many to many的过程。. “达观杯”算法大赛很多同学在用传统的方式，包括baseline来做，很多人吐槽baseline好像有点高。但是我们没有做特殊优化，这是最基础的版本，做出来很高说明传统的机器学习还是非常好的，不是Deep learning一统天下。. 传统的机器学习，需要构造特征，不同领域定制化程度很高，这个模型A领域用了，B领域几乎要从头再做一遍，没有办法把其他的特征迁移过来很好的使用。某些领域效果很好，某些领域另外一个算法很好，传统机器学习把各种各样的方式做以融合来提升效果。. **深度学习则可实现端到端，无需大量特征工程。**框架的通用性也很好，能满足多领域的需求，并且可以使用费监督语料训练字词向量提升效果。. 因为调参很麻烦，有时改了一下参数好很多，改了一个参数效果又下降了，有的算法能够对此有一定的解释，但不像传统机器学习能够解释得那么好。这两大帮派不能说完全谁战胜了谁，是相融相生的。. 它是一个单层的CNN，选择了几种类型的卷积，做一个feature map，然后用max-pooling取得每个map最大的特征作为最终的输出，结构非常简单，大家只要有一点深度学习的知识就可以。但是因为过于简单，而且CNN天生的缺陷是宽度有限，导致它会损失语义的问题。. Deep Pyramin CNN就是深度的CNN， CNN的特点就是结构简单。虽然有block N，但它每个block长得都是一样的，除了第一层，每一层就是一个pooling取一半，剩下是两个等宽度的卷积，输出250维，叠加好几层后就可能学到非常准的语义。. 这个模型的优点是非常符合人类的思维。Word级别的时候前面的套路都是一样的，做各种Embedding，在Embedding到下一层次，这个输到下一层sentence级别之前会加一层Attention，让它知道这句话里面哪一个词最重要，这像我们知道一句话中哪个词最重要。. 最终输出之前再加个Attention，这个Attenton去学这里面哪些句子最重要的。你可以简单的理解，它把我们输入的那么多文本，**也是经过了一层层的过滤，前面是通过卷积的过程，它现在是通过Attention的机制去找。**. 还有一个特别好的地方是学部分可解释，句子里哪些词最重要，它的蓝颜色就更深，它能找到语义级别哪个语义对分类贡献最大，这是这个网络很好的一点。. 包括前面讲的HNN、Deep Pyramin CNN，网上的实现跟论文是有一定差别的。所以大家要注意，我们关注的是它整体的网络结构，并不是每一点的百分之百的还原，我们不是它的复制者，而是它的使用者。所有的网络结构、参数甚至过程，只要大体的思想有了就OK。这两个是many to one在文本分类上用得很多的。. 序列标注就两个东西：第一个是定义标签体系。我们这边一般最常用BMES，简单一点的IO，复杂一点的BIO，BMES算是一个经典的方法，不多也不少，还有M1、M2、M3更复杂的一般都不太用。. 传统的CRF用起来效果不错，Deep learning也能够把这个事情做得很好。LSTM可以学习到很长的上下文，而且对识别非常有帮助。实际问题或者工业应用来说，我们要保证它的整体效果和复杂度的情况下，这边Bi-LSTM是一个非常好的方式，也是相对比较成熟的方式。. 我对这个模型结构的看待，它是一个深度学习和传统方式非常完美的结合。Bi-LSTM做特征工程，CRF做标签的输出。很多同学都试过，用纯的Bi-LSTM去写，最终输出标签之间没有序列依赖的关系。. 这是非常好的一篇论文，讲到了我们怎么能够把各种各样的level的信息用到，它这边是英文，所以有一个char级别的，先对char，通过RNN、CNN做一个Embedding。学习到char级别上的关系，char级别的关系合并之后是黄颜色那个字符的向量，然后它又把word级别的红颜色的词向量也加进去拼起来，还有两个是灰颜色的，灰颜色的是人工特征。就看大家自己怎么加，这是每个人的智慧。. 生成式摘要是很难的一个东西，它的训练集标注比我们标分词、标分类难得多，要有一篇文章，人得写出摘要，整理出好多这样的摘要。因为每个人写得不一样，包括评测的方式BLUE等，所以做摘要比较难。但是我们平时可以基于生成式文本的其他小应用。. 举个简单的例子：大家爬过一些新闻的网站，那么长的正文一般正文第一段把事情都说清楚了，然后有一个新闻的标题，我们可以用第一段作为输入，标题作为输出，做这样一个简单的通过新闻第一段可以写出新闻标题的功能，其实跟生成摘要的思想是一样的。唯一的差别是它加了注意力的机制，会发现它关注输出的哪些词对语义表达最有用，它会关注有用的信息，解码的时候就可以得到各种各样的序列、各种各样的值，用beam search找到最好的结果。. 引入注意机制，以前做不了这个事情，现在我们可以做这个事情。工业中用得比较多的是抽取式的摘要。简单来说，就是一篇文章中哪些句子比较重要，把它抽出来就可以了。. ## 四、文本挖掘的经验和思考. ### 实际工程中需要考虑的因素. ### **实际工程中运用深度学习挖掘文本的思考**. 深度学习能够克服传统模型的缺点，大家用CRF很多，但CFR有时拿不到太远的长的上下文，它比较关注左右邻居的状态，很远的状态对它影响不大。. 但是有些语义影响很大，比如：我们要抽“原告律师”、“被告律师”、“原告刘德华”，然后中间讲了一大堆，“委托律师张学友”，我们能抽取出来他是律师，但是如何知道他是原告律师？. 一定要看到刘德华前面三个有“原告”两个字，才知道他是原告律师。这时如果用深度学习LSTM的方式可以学到比较远的上下文特征，帮助你解决这个问题。. 4. 关注前沿技术，包括对抗网络、强化学习、迁移学习；. 本文由 @达观数据 原创发布于人人都是产品经理。未经许可，禁止转载.",
    "file_path": "unknown_source",
    "create_time": 1769006720,
    "update_time": 1769006720,
    "_id": "doc-7890c6c195439c2a8d6fe49741a8156c"
  },
  "doc-2143f772a74d69830b87fea511a6e614": {
    "content": "[DOC_ID: chunk-4de64ac3]\n[领域: 语言学]\n# 什么是 NLP（自然语言处理）？. ## 什么是 NLP？. 自然语言处理 (NLP) 是计算机科学和人工智能 (AI) 的一个子领域，它使用机器学习来使计算机能够理解人类语言并与之交流。. NLP 通过将计算语言学（基于规则的人类语言建模）与统计建模、机器学习 (ML) 和深度学习相结合，使计算机和数字设备能够识别、理解和生成文本和语音。. NLP 研究有助于推动生成式 AI 时代的到来，其涉及从大型语言模型 (LLM) 的交流技巧到图像生成模型理解请求的能力。NLP 已经成为许多人日常生活的一部分，为搜索引擎提供支持，通过语音命令、语音操作的 GPS 系统和智能手机上的问题解答数字助理来提示聊天机器人（如 Amazon 的 Alexa、Apple 的 Siri 和 Microsoft 的 Cortana）进行客户服务。. ## NLP 的优势. NLP 在完全或部分自动化任务（如客户支持、数据输入和文档处理）中特别有用。例如，由 NLP 提供支持的聊天机器人可以处理常规客户问询，从而让人工客服腾出时间处理更复杂的问题。在文档处理中，NLP 工具可以自动分类、提取关键信息并汇总内容，从而减少与手动数据处理相关的时间和错误。NLP 有助于语言翻译，将文本从一种语言转换为另一种语言，同时保留含义、上下文和细微差别。. NLP 通过从非结构化文本数据（如客户评论、社交媒体帖子和新闻文章）中提取洞察信息来增强数据分析。通过使用 文本挖掘 技术，NLP 可以发现大型数据集中不明显的模式、趋势和情绪。通过情感分析，可从文本中 提取主观特质， 比如态度、情感、讽刺、困惑、怀疑等。这通常用于将通信路由到系统或最有可能做出下一个响应的人员。. 利用 NLP，系统将能够理解用户查询背后的意图，从而提供更准确、与上下文更相关的搜索结果。由 NLP 提供支持的搜索引擎不仅仅依赖于关键字匹配，而是分析单词和短语的含义，即使查询模糊或复杂，也能够更轻松地找到相关信息。这有助于改善网络搜索、文档检索以及企业数据系统中的用户体验。. NLP 可以为高级语言模型提供支持，以便为各种用途生成类人文本。GPT-4 等预训练模型可根据用户提供的提示生成文章、报告、营销文案、产品描述，甚至是创意写作内容。NLP 提供支持的工具还可以协助自动执行多种任务，例如起草电子邮件、撰写社交媒体帖子或法律文书协作。NLP 可以理解上下文、语气和风格，从而确保生成的内容具有连贯性、相关性，并且与要传达的信息一致，从而节省内容创作花费的时间和精力，同时保持质量。. ## NLP 的方法. NLP 将计算语言学的强大功能与机器学习算法和深度学习相结合。计算语言学利用数据科学来分析语言和语音。它包括两种主要类型的分析：句法分析和语义分析。句法分析通过解析单词的语法并应用预先编程的语法规则来确定单词、短语或句子的含义。语义分析使用句法输出从单词中提取含义，并在句子结构中解释它们的含义。. ### 基于规则的 NLP. 统计 NLP 是较晚发展起来的，它能自动提取、分类和标记文本和语音数据的元素，然后为这些元素的每种可能含义分配统计可能性。这依赖于机器学习，能够进行复杂的语言学细分，如词性标注。. ### 深度学习 NLP. ## NLP 任务. 一些 NLP 任务通常有助于处理人类文本和语音数据，从而帮助计算机理解它所摄取的内容。其中一些任务包括：. ### 命名实体识别 (NER). ## NLP 的工作原理. NLP 文本预处理将原始文本转换成机器更容易理解的格式，为分析做好准备。首先进行标记化，这涉及将文本拆分为更小单位，如单词、句子或短语。这有助于将复杂的文本分解为可管理的部分。接下来，应用小写来标准化文本，通过将所有字符转换为小写，确保“Apple”和“apple”这样的词得到相同处理。停用词删除是另一个常见步骤，其中，“is”或“the”等常用词被过滤掉，因为它们不会为文本添加重要的含义。词干提取或词形还原将单词简化为它们的词根形式（例如，“running”变为“run”），通过对同一个单词的不同形式进行分组，可以更轻松地分析语言。此外，文本清理会删除可能使分析混乱的不需要的元素，例如标点符号、特殊字符和数字。. 特征提取是将原始文本转换为机器可以分析和解释的数字表示的过程。这涉及使用 Bag of Words 和 TF-IDF 等 NLP 技术将文本转换为结构化数据，这些技术可以对文档中单词的存在和重要性进行量化。更高级的方法包括 Word2Vec 或 GloVe 等词嵌入方法，它们将词语表示为连续空间中的密集向量，从而捕获词与词之间的语义关系。上下文嵌入通过考虑词语出现的上下文进一步增强了这一点，能够实现更丰富、更细微的表示。. 文本分析涉及通过各种计算技术从文本数据中解释和提取有意义的信息。该过程包括词性 (POS) 标注等任务，用于识别单词的语法角色，以及命名实体识别 (NER)，用于检测名称、位置和日期等特定实体。依赖关系解析用于分析单词之间的语法关系，以了解句子结构，而情感分析用于确定文本的情感基调，评估它是积极、消极还是中立。主题建模用于识别文本或整个文档语料库中的潜藏主题或话题。自然语言理解 (NLU) 是 NLP 的一个子集，侧重于分析句子背后的含义。利用 NLU，软件将能够在不同的句子中找到相似的含义或处理具有不同含义的单词。通过这些技术，NLP 文本分析可将非结构化文本转换为洞察分析。. 在上述各个过程中，一些不同的软件环境非常有用。例如，Natural Language Toolkit (NLTK) 是一套用 Python 编程语言编写、适用于英语的库和程序。它支持文本分类、标记化、词干提取、标注、解析和语义推理功能。TensorFlow 是一个用于机器学习和 AI 的免费开源软件库，可用于训练 NLP 应用程序的模型。如果有兴趣熟悉此类工具，相关教程和认证比比皆是。. ## NLP 的挑战. 即使是最先进的 NLP 模型，也不是完美的，就像人类语音容易出错一样。与任何 AI 技术一样，NLP 也有潜在的缺陷。人类语言充满了歧义，因此，程序员要编写能够准确确定文本或语音数据的预期含义的软件非常困难。人类学习语言可能需要数年时间，而且许多人从未停止过学习。但是，程序员必须教会由自然语言提供支持的应用程序识别和理解不规则语法现象，这样，他们的应用程序才能准确和有用。 相关风险可能包括：. 当人们说话时，他们的语言表达甚至肢体语言所表达的含义可能与字面的言辞完全不同。为了表达效果而夸大其词、为了强调重要性而强调单词或讽刺可能会被 NLP 混淆，从而使语义分析变得更加困难且不太可靠。. ## 各行业 NLP 用例. 新的医学见解和突破可能比许多医疗保健专业人员能获知的速度更快。基于 NLP 和 AI 的工具可以帮助加快对健康记录和医学研究论文的分析，从而做出更明智的医疗决策，或协助检测甚至预防疾病。. NLP 可以对理赔进行分析，通过寻找某些模式来确定需要关注的领域，并发现理赔处理中的低效问题，从而进一步优化处理方式和员工工作。. IBM® Granite 是我们开放式、性能优异、值得信赖的 AI 模型系列，专门为企业量身定制，并经过优化，可以帮助您扩展 AI 应用。深入了解语言、代码、时间序列和防护措施选项。. 我们对 2,000 家组织进行了调查，旨在了解他们的 AI 计划，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。. 利用 IBM 嵌入式 AI 增强您的应用程序.",
    "file_path": "unknown_source",
    "create_time": 1769007112,
    "update_time": 1769007112,
    "_id": "doc-2143f772a74d69830b87fea511a6e614"
  },
  "doc-3ac3b580cca11b5e1d0122810a6ec5f5": {
    "content": "[DOC_ID: chunk-f9b73650]\n[领域: 语言学]\n最新推荐文章于 2025-11-25 10:01:10 发布. 于 2021-09-15 11:07:26 发布. 上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add & Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。. 得到 8 个输出矩阵 Z1 到 Z8 之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个 Linear层，得到 Multi-Head Attention 最终的输出 Z。. 上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention, Add & Norm, Feed Forward, Add & Norm 组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add & Norm 和 Feed Forward 部分。. 通过上面描述的 Multi-Head Attention, Feed Forward, Add & Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵 X(n×d)，并输出一个矩阵 O(n×d)。通过多个 Encoder block 叠加就可以组成 Encoder。. 第一步：是 Decoder 的输入矩阵和 Mask 矩阵，输入矩阵包含 \"<Begin> I have a cat\" (0, 1, 2, 3, 4) 五个单词的表示向量，Mask 是一个 5×5 的矩阵。在 Mask 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。. 根据 Encoder 的输出 C计算得到 K, V，根据上一个 Decoder block 的输出 Z 计算 Q (如果是第一个 Decoder block 则使用输入矩阵 X 进行计算)，后续的计算方法与之前描述的一致。. *深度学习**算法*--*transformer* *算法*框架: embedding:词嵌入,也就是将词的输入转化为向量或者特征 positionencoding:添加位置信息 norm:批处理*(*BN*)* add:残差连接 Feed forward:全链接 *Transformer* 的*算法*核心主要集中在注意力模块:encoder 中的 self-attention以及decoder 中的 self-attention &&cross-attention... 李沐动手学*深度学习*V2-*transformer*和代码实现\\_李沐多头注意力代码-CSD...",
    "file_path": "unknown_source",
    "create_time": 1769007563,
    "update_time": 1769007563,
    "_id": "doc-3ac3b580cca11b5e1d0122810a6ec5f5"
  },
  "doc-778f83860f7db1907e0da6e0cb412723": {
    "content": "[DOC_ID: chunk-adc7557b]\n[领域: 语言学]\nTransformer是一个利用注意力机制来提高模型训练速度的模型。关于注意力机制可以参看这篇文章，trasnformer可以说是完全基于自注意力机制的一个深度学习模型，因为它适用",
    "file_path": "unknown_source",
    "create_time": 1769007718,
    "update_time": 1769007718,
    "_id": "doc-778f83860f7db1907e0da6e0cb412723"
  },
  "doc-e77bf0e5ecd6a42625cc947b20dfaca8": {
    "content": "[DOC_ID: chunk-69ff27f5]\n[领域: 语言学]\n这节课我们主要学习了Transformer模型。在学习时，我们主要应该关注两个核心机制：自注意力、多头注意力。搞懂Transformer的注意力机制后，我们基本就理解了",
    "file_path": "unknown_source",
    "create_time": 1769007742,
    "update_time": 1769007742,
    "_id": "doc-e77bf0e5ecd6a42625cc947b20dfaca8"
  },
  "doc-f11a177649a52b5177411e9ce9bb9885": {
    "content": "[DOC_ID: chunk-c082491d]\n[领域: 语言学]\nTransformer 是一種神經網路架構，可將輸入序列轉換或變更為輸出序列。Transformer 透過學習內容並追蹤序列元件之間的關係來做到這一點。例如，考慮這個輸入序列：「天空",
    "file_path": "unknown_source",
    "create_time": 1769007770,
    "update_time": 1769007770,
    "_id": "doc-f11a177649a52b5177411e9ce9bb9885"
  },
  "doc-ad8f11550a8a8807684e7ecc93a2a801": {
    "content": "[DOC_ID: chunk-338a909d]\n[领域: 语言学]\nLayer): \"\"\"transformer编码器块\"\"\" def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias = False,** kwargs): super(EncoderBlock, self). Encoder): \"\"\"Transformer编码器\"\"\" def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias = False,** kwargs): super(TransformerEncoder, self). add_module(\"block\" + str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias)) def forward(self, X, valid_lens,* args):# 因为位置编码值在-1和1之间， # 因此嵌入值乘以嵌入维度的平方根进行缩放， # 然后再与位置编码相加。 X = self. Encoder): \"\"\"Transformer编码器\"\"\" def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_hiddens, num_heads, num_layers, dropout, bias = False,** kwargs): super(). blks =[EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_hiddens, num_heads, dropout, bias) for _ in range(num_layers)] def call(self, X, valid_lens,** kwargs):# 因为位置编码值在-1和1之间， # 因此嵌入值乘以嵌入维度的平方根进行缩放， # 然后再与位置编码相加。 X = self. Encoder): \"\"\"transformer编码器\"\"\" def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias = False,** kwargs): super(TransformerEncoder, self). add_sublayer(str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias)) def forward(self, X, valid_lens,* args):# 因为位置编码值在-1和1之间， # 因此嵌入值乘以嵌入维度的平方根进行缩放， # 然后再与位置编码相加。 X = self.",
    "file_path": "unknown_source",
    "create_time": 1769007773,
    "update_time": 1769007773,
    "_id": "doc-ad8f11550a8a8807684e7ecc93a2a801"
  },
  "doc-0724c887b70d220c530dcfaee28003c1": {
    "content": "[DOC_ID: chunk-139ece35]\n[领域: 语言学]\n* English \"Transformer (deep learning) – 英语\"). * Simple English \"Transformer (machine learning model) – Simple English\"). * Српски / srpski \"Transformator (model mašinskog učenja) – 塞尔维亚语\"). | 范式 * 监督学习 * 無監督學習 * 線上機器學習 * 元学习&action=edit&redlink=1 \"元学习 (计算机科学)（页面不存在）\")（英语：Meta-learning (computer science) \"en:Meta-learning (computer science)\")） * 半监督学习 * 自监督学习 * 强化学习 * 基于规则的机器学习（英语：Rule-based machine learning） * 量子機器學習 |. | 与人类学习 * 主动学习&action=edit&redlink=1 \"主动学习 (机器学习)（页面不存在）\")（英语：Active learning (machine learning) \"en:Active learning (machine learning)\")） * 众包 * Human-in-the-loop（英语：Human-in-the-loop） |. | 模型诊断 * 学习曲线&action=edit&redlink=1 \"学习曲线 (机器学习)（页面不存在）\")（英语：Learning curve (machine learning) \"en:Learning curve (machine learning)\")） |. | 大会与出版物 * NeurIPS * ICML（英语：International Conference on Machine Learning） * ICLR * AAAI * IJCAI * CVPR * ML&action=edit&redlink=1 \"机器学习 (期刊)（页面不存在）\")（英语：Machine Learning (journal) \"en:Machine Learning (journal)\")） * JMLR（英语：Journal of Machine Learning Research） |. 6. **^** Chen, Lili; Lu, Kevin; Rajeswaran, Aravind; Lee, Kimin; Grover, Aditya; Laskin, Michael; Abbeel, Pieter; Srinivas, Aravind; Mordatch, Igor, Decision Transformer: Reinforcement Learning via Sequence Modeling, 2021-06-24, arXiv:2106.01345.",
    "file_path": "unknown_source",
    "create_time": 1769007844,
    "update_time": 1769007844,
    "_id": "doc-0724c887b70d220c530dcfaee28003c1"
  },
  "doc-07c6d2ca0b23fef78261afa6105b1fe0": {
    "content": "[DOC_ID: chunk-9263bcd9]\n[领域: 语言学]\nTransformers 是由Hugging Face 开发的一个NLP 包，支持加载目前绝大部分的预训练模型。随着BERT、GPT 等大规模语言模型的兴起，越来越多的公司和研究者采用Transformers 库",
    "file_path": "unknown_source",
    "create_time": 1769007997,
    "update_time": 1769007997,
    "_id": "doc-07c6d2ca0b23fef78261afa6105b1fe0"
  },
  "doc-93658308ac482857335fb970db2d1193": {
    "content": "[DOC_ID: chunk-a663405c]\n[领域: 语言学]\nSenior Staff Writer, AI Models. 转换器模型是一种神经网络架构，擅长处理顺序数据，最知名的是与大型语言模型 (LLM) 的关联。转换器模型在人工智能 (AI) 的其他领域（例如计算机视觉、语音识别和时间序列预测）也展现出卓越的性能。. 仅自回归解码器的 LLM，例如促成 OpenAI ChatGPT 发布的 GPT-3（生成式预训练转换器的缩写）模型，催化了生成式 AI 的现代纪元。. 转换器模型的核心功能是其自注意力机制，转换器模型从中获得了检测输入序列各部分之间关系（或依赖关系）的强大能力。与之前的 RNN 和 CNN 架构不同，转换器架构仅使用注意力层和标准前馈层。. 自注意力机制的优势——特别是转换器模型采用的多头注意力技术，使其性能超越了以往最先进的 RNN 和 CNN。. 加入我们世界级的专家小组——工程师、研究人员、产品负责人等将为您甄别 AI 领域的真知灼见，带来最新的 AI 资讯与深度解析。. 掌握注意力的数学概念或自注意力机制，对于理解转换器模型在许多领域的成功应用至关重要。注意力机制本质上是一种算法，旨在确定 AI 模型在特定时刻应“关注”数据序列的哪些部分。. 虽然*字符*（字母、数字或标点符号）是我们人类用来表示语言的基本单位，但 AI 模型使用的最小语言单位是*词元* 。每个词元都分配有一个 ID 号，这些 ID 号（而非单词甚至词元本身）就是 LLM 浏览其词汇“数据库”的途径。这种语言的*词元化*大大降低了处理文本所需的算力。. 使用*位置编码*时，模型会在词元进入注意力机制之前，根据每个词元的相对位置为其嵌入添加一个值向量。这两个词元越接近，它们的位置向量就越相似，因此，通过添加位置信息，它们的对齐分数就会越高。因此，该模型学会了更加关注附近的词元。. 添加位置信息后，每个更新的词元嵌入都会用来生成三个新向量。这些*查询、键*和*值*向量是通过将原始词元嵌入传递到第一个注意力层之前的三个并行前馈神经网络层中的每一个层来生成的。该线性层的每个并行子集都有一个独特的权重矩阵，通过对大量文本数据集进行自监督预训练来学习。. * 嵌入值乘以权重矩阵 WQ，得出*查询向量* (Q)，其具有 *dk* 维数. * 嵌入值乘以权重矩阵 WK，得出键向量 (K)，其维度同样为 *dk*. * 嵌入值乘以权重矩阵 WV，得出键向量 (K)，其维度为 *dk*. 在被馈送到第一个前馈层之前，每个原始输入词元嵌入被拆分为 *h* 个大小均匀的子集。每一个嵌入片段都被输入到 *h* 个由 *Q、K* 和 *V* 权重组成的并行矩阵之一，各矩阵分别被称为*查询*头、*键头*或*值头*。然后，每个并行的查询、键和值头三元组输出的向量会被输入下一个注意力层的相应子集，称为*注意力头*。. 转换器模型通常与 NLP 关联，最初是为机器翻译用例而开发的。值得一提的是，转换器架构催生了大语言模型 (LLM)，进而促成了生成式 AI 的诞生。. 公众最熟悉的大多数 LLM，从 OpenAI 的 GPT 系列和 Anthropic 的 Claude 模型等闭源模型，到包括 Meta Llama 或IBM® Granite 在内的开源模型，都是仅自回归解码器的 *LLM*。. 电子书 解锁生成式 AI + ML 的强大功能. 如何选择正确的方法来准备数据集和使用 AI 模型；如何使用模型选择框架来平衡性能要求与成本、风险、部署需求和利益相关者要求，以便为您的用例确定最适合的模型。. IBM Granite 是我们开放式、性能优异、值得信赖的 AI 模型系列，专门为企业量身定制，并经过优化，可以帮助您扩展 AI 应用程序。深入了解语言、代码、时间序列和护栏选项。. 报告 2024 年 AI 实际应用. 我们对 2,000 家组织进行了调查，旨在了解他们的 AI 计划，以发现哪些方法有效、哪些方法无效，以及如何才能取得领先。. 电子书 解锁生成式 AI + ML 的强大功能. 指南 面向 CEO 的生成式 AI 指南. 了解 CEOs 如何在生成式 AI 所能创造的价值与其所需的投资和带来的风险之间取得平衡。. 指南 让 AI 充分发挥作用：利用生成式 AI 提高投资回报率. 想要从 AI 投资中获得更好的回报吗？了解如何通过帮助您最优秀的人才构建和提供创新的新解决方案，在关键领域扩展生成式人工智能来推动变革。. 使用面向 AI 构建器的新一代企业级开发平台 IBM watsonx.ai，可以训练、验证、调整和部署生成式 AI、基础模型和机器学习功能。使用一小部分数据，即可在很短的时间内构建 AI 应用程序。. 通过增加 AI 重塑关键工作流程和运营，最大限度提升体验、实时决策和商业价值。. 一站式访问跨越 AI 开发生命周期的功能。利用用户友好型界面、工作流并访问行业标准 API 和 SDK，生成功能强大的 AI 解决方案。.",
    "file_path": "unknown_source",
    "create_time": 1769008030,
    "update_time": 1769008030,
    "_id": "doc-93658308ac482857335fb970db2d1193"
  },
  "doc-a6f32e4615a0137e965400eba7166c47": {
    "content": "[DOC_ID: chunk-6db04701]\n[领域: 语言学]\n【機器學習2021】Transformer (上) Hung-yi Lee 354000 subscribers 2878 likes 283126 views 26 Mar 2021 slides: https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/seq2seq_v9.pdf 125 comments",
    "file_path": "unknown_source",
    "create_time": 1769008235,
    "update_time": 1769008235,
    "_id": "doc-a6f32e4615a0137e965400eba7166c47"
  },
  "doc-4fcfe2fa03fc2203cba1b55a8668dc72": {
    "content": "[DOC_ID: chunk-26b58d63]\n[领域: 语言学]\n已于 2025-02-23 17:57:45 修改. CC 4.0 BY-SA版权. 版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。. 于 2025-02-23 17:51:48 首次发布. 人工智能（AI）、机器学习（ML）、深度学习（DL）和大语言模型（LLM）之间是**逐层包含且技术递进**的关系，具体如下：. 人工智能（AI） ⊃ 机器学习（ML） ⊃ 深度学习（DL） ⊃ 大语言模型（LLM）. * **人工智能（AI）**：是研究人造系统所展现出的智能行为的领域。其核心在于模拟、拓展并超越人类的智能，赋予计算机执行仅有人类才能完成的任务的能力，诸如语言解析、图像识别以及策略决策等。人工智能的研究范畴极为广泛，涵盖了机器学习、自然语言处理、计算机视觉以及机器人技术等多个分支。. * **机器学习（ML）**：是人工智能的一个重要分支，它专注于设计算法，使计算机能够通过数据分析来自我学习和进行决策或预测。机器学习算法能够识别数据中的模式，并根据这些模式做出预测或决策。. * **深度学习（DL）**：是机器学习的一个深化领域，它依托于神经网络，特别是深层神经网络，来进行数据处理。深度学习模型通过多层神经网络（或称深度）进行复杂的非线性转换，从而深入学习数据的高层次特征。深度学习在图像识别、语音识别以及自然语言处理等多个领域均取得了杰出的成果。. * **大语言模型（LLM）**：是深度学习技术的杰出应用之一，专为处理和生成自然语言文本而设计。这类模型通常包含庞大的参数规模，达到数十亿乃至数万亿，赋予其深刻理解和生成复杂语言结构的能力。通过海量文本数据的学习，大语言模型能够应对文本分类、情感分析、文本摘要、机器翻译以及对话生成等多样的语言任务。. #### 关系图. | 机器学习 | 结构化、小规模 | 预测房价、分类邮件 | 依赖人工特征工程 |. | 深度学习 | 非结构化、中大规模 | 图像识别、语音处理 | 自动特征提取 |. | 大语言模型 | 非结构化、超大规模 | 文本生成、代码补全 | 参数千亿级，自监督学习 |. * **大语言模型**：2020年后兴起，依赖Scaling Law（模型性能随数据量、参数规模提升）。. * **机器学习**：金融风控（逻辑回归）、推荐系统（协同过滤）。. * **深度学习**：医学影像识别（CNN）、语音助手（RNN）。. * **大语言模型**：智能客服（ChatGPT）、代码生成（GitHub Copilot）。. *LLM*S-*大语言模型**和*ai的*关系*？. *人工智能*（AI）：指的是由人制造出来的系统所表现出来的智能行为。它是一个广泛的领域，包括了*机器学习*、*深度学习*、*自然语言处理*、计算机视觉等多个子领域。*大语言模型*（*LLM*s）：是指使用大量数据训练的，能够理解*和*生成自然语言的模型。这些模型是*人工智能*在*自然语言处理*（NLP）领域的一种应用。总结来说，*大语言模型*是*人工智能*技术在*自然语言处理*领域的一种体现，它的研究*和*应用是推动*人工智能*发展的一个重要方面。随着技术的不断进步，*大语言模型*在AI领域的地位*和*作用也将越来越重要。. 大模型与*人工智能*的*关系**和*区别\\_*人工智能**和*大模型的*关系*. GPT是*LLM*模型的一种实现,而*LLM*模型是*人工智能*领域中的一个特定应用,*LLM*是NLP 中的一个重要组成部分,NLP是AIGC中一项很重要的技术。他们的*关系*可用如下图表示: *人工智能*是一个大的领域,应用范围更广泛,涵盖了除了*自然语言处理*之外的其他领域,如机器视觉、*机器学习*等。AIGC涉及到的领域*和*技术很广泛,其中很重要的一... ...与AI的*关系*全解析!\\_*大语言模型**和**人工智能*的*关系*. 简单科普一下,AI是*人工智能*,大模型是*大语言模型*(*LLM*)的缩写,大模型是AI领域的一个重要领域*和*分支。 ChatGPT爆火之前,提到AI模型一般指的是垂直模型,比如会做翻译的AI,会下围棋的AI,会对话的AI等。传统的AI都是这种只会做特定领域事情的模型,无法像人类一样,什么都会干。 如果AI什么都能干了,那叫做通用人工智... 一文捋清*人工智能**机器学习**深度学习*、大数据、数据分析、数据挖掘的*关系*. 作为一个不断发展的领域，*深度学习*继续推动机器所能实现的边界，正在进行的研究集中于提高模型的可解释性，解决伦理考虑，并将其适用性扩展到新的领域。从本质上讲，*人工智能*是一个总体概念，ML作为一个子集提供了学习能力，而DL，ML的一种特殊形式，利用深度神经网络来实现先进的学习*和*表示，共同推动了智能系统*和*技术的进化。*人工智能*的目的是让机器具备人的思维*和*意识。随着我们的成长，大量的数据通过视觉、听觉涌入大脑，使我们的神经网络连接，也就是神经元连接线上的权重发生变化，有些线权重增强了，有些线上的权重减弱了。. 一张图看懂AI、*机器学习*、*深度学习*与*大语言模型*的逻辑*关系*！. 本文系统梳理了AI、ML、RL、DL*和**LLM*的概念与层级*关系*。AI是让机器模拟人类智能的宏大目标；ML是实现AI的核心路径，让机器自动学习；RL是让机器自动提取数据特征；DL是多层神经网络实现的深度表征学习；*LLM*基于Transformer架构，是处理人类语言的杰出成果。文章通过层级*关系*图清晰展示了这些概念间的逻辑联系，帮助读者建立AI领域的知识框架。. 模型通过学习语言*之间*的对应*关系*,将句子翻译为中文:“你好,你怎么样?” 总之,语言在*大语言模型*中是核心,它不仅是模型的学习对象,也是推理过程中用于生成*和*理解的基础。通过训练,模型掌握了语言的使用规则*和*模式;在推理中,模型应用这些规则*和*模式来完成特定的任务。通过这种方式,语言成为了*大语言模型*理解*和*生成内容的关... *人工智能*-*机器学习*-*深度学习*-*大语言模型*的*关系*及其运行的三要素. 今天我们不用专业的术语,就用“工厂车间”来类比,把*人工智能*(AI,Artificial Intelligence)、*机器学习*(ML,Machine Learning)、*深度学习*(DL,Deep Learning)、*大语言模型*(*LLM*,Large Langeuage Model)的*关系*及其运行的三要素捋明白,争取让没有技术基础的你也能秒懂。. 【*语言模型*】探索AI模型、AI大模型、大模型、*大语言模型*与大数据模型的*关系*与协同. AI模型是*人工智能*领域的核心，它通过模拟人类智能的方式，使机器能够执行各种复杂的任务。AI模型涵盖了*机器学习*、*自然语言处理*、计算机视觉等多个方面，是实现*人工智能*功能的基础组件。AI模型的发展离不开数据的支持，通过对大量数据的分析*和*学习，AI模型能够不断提升自身的性能*和*准确性。. 在当今的技术讨论中，“*人工智能*”（AI）、“*机器学习*”（ML）、“*深度学习*”（DL）*和*“大模型”（*LLM*）等术语常被混用，以至于令人困惑。本文将简要梳理这些概念，帮助读者清晰理解它们的*关系*与区别。对于*人工智能*（AI）的定义，业界其实并没有一个统一的、公认的说法，我比较喜欢的是中国科学院院士谭铁牛在《求实》上发表的一篇文章里对*人工智能*的定义——“*人工智能*是研究开发能够模拟、延伸*和*扩展人类智能的理论、方法、技术及应用系统的一门新的技术科学，研究目的是促使智能机器会听（语音识别、机器翻译等）、会看（图像识别、文字. ...*机器学习*、*深度学习*与*大语言模型*的逻辑*关系*!\\_搜索一份思维导图,梳理... *大语言模型*是基于Transformer架构,参数规模极大的神经网络*语言模型*。通过在海量文本数据上进行预训练来理解*和*生成自然语言。 (到这里,其实没有太多透彻的研究结论了,每天都在快速的演进中) 总结一句话: *人工智能*(AI)是宏伟的目标,*机器学习*(ML)是实现它的核心路径,而*深度学习*(DL)则是这条路径上目前最强大的引擎,大... 一文彻底搞懂*大语言模型*、智能体与工作流\\_智能体 大模型 工作流-CSDN... *人工智能*正以前所未有的速度渗透到社会经济的各个层面,其发展范式也正从执行特定任务的“狭义AI”向具备更广泛认知与执行能力的“通用AI”迈进。在这一深刻变革中,*大语言模型*(*LLM*)、智能体(Agent)*和*工作流(Workflow)成为驱动智能化从“辅助工具”向“自主系统”演进的三个核心支柱。. *人工智能*是指通过模拟、延伸人类智能的机制，使机器能够执行需要智力的任务。这包括理解语言、学习、推理、问题解决等能力。*人工智能*系统通过算法*和*模型从大量数据中学习，并能够做出智能决策。*人工智能*的核心在于算法*和*模型，这些算法*和*模型能够处理、分析*和*解释数据，以模拟人类的智能行为。通过不断的学习*和*优化，*人工智能*系统能够逐渐提升其性能，以更好地完成各种任务。概念：*机器学习*是*人工智能*的一个重要分支，其核心思想是让计算机系统从数据中学习并提高性能，而无需明确地编程。. *人工智能*基于*深度学习*的大模型技术演进：从*机器学习*到AIGC的应用体系构建. 内容概要：本文系统介绍了*人工智能*的基本概念、发展历程、核心技术（*机器学习*与*深度学习*）、大模型与AIGC的兴起，以及*人工智能*的应用场景与未来挑战。文章从定义出发，阐述了*人工智能*的学科属性、智能维度*和*主要学派... 【必学收藏】AI与*大语言模型*到底是什么*关系*? 先谈谈,AI 、*机器学习*、*深度学习**和**大语言模型*的*关系*。 *人工智能*,这就像是我们作为老师的最终目标是制造出一个“像人一样聪明”的机器学生。不管是用死记硬背的方法,还是让它自己领悟,只要它表现得像人一样能看、能听、能思考,它就是AI。 *机器学习*,教这个学生的一种具体手段。以前我们是手把手告诉学生“如果看... ...大型*语言模型*对具身*人工智能*发展的推动与挑战\\_大模型对于具身智能的... 诸如GPT之类的大型*语言模型*是在大量文本数据集的基础上训练得到的*人工智能*系统,具备理解*和*生成人类语言的能力。最初,这些模型主要应用于写作*和*回答问题等任务,但随着技术的不断发展,它们正在逐渐演变为能够进行多模态通信、推理、规划以及解决问题的综合性系统。这种演变使得工程师能够突破传统限制,超越执行一些重复性任务的... （三）大模型/*人工智能*/*机器学习*/*深度学习*/NLP. 模型，简单来说，就是用来。它可以是实体的，也可以是虚拟的，目的是为了帮助我们更好地理解*和*预测所描述的对象。在生活中，模型无处不在，它们以各种形式存在，帮助我们解决各种问题。：模型就像是一个简化的“影子”或“替身”，它代表了真实世界中的某个东西或过程，但比真实的东西更简单、更容易理解。比如，你想造一个房子，但直接造个真房子太麻烦了，所以你会先造一个小的、简化的房子模型，这样你就可以更容易地看到房子的样子，并计划如何建造它。. 【*人工智能*领域】AI基础概念详解：涵盖*机器学习*、*深度学习*、NLP、Transformer及大模型应用. 内容概要：本文详细介绍了AI领域的基础概念，对比了传统编程与*机器学习*的区别，阐述了*自然语言处理*（NLP）、自然语言理解（NLU）、自然语言生成（NLG）的概念*和*应用场景。文章进一步讲解了监督学习、无监督学习*和*... 一文帮你看懂大模型行业黑话：AI、*机器学习*、大模型、*LLM*、Agent 都是啥*关系*？. 你可以根据我这个学习路线*和*系统资料，制定一套学习计划，只要你肯花时间沉下心去学习，它们一定能帮到你！. 【*人工智能*领域】全面教程与实战案例：涵盖基础概念、*机器学习*、*深度学习*、计算机视觉、*自然语言处理*及模型部署. 首先，阐述了AI的基本概念*和*发展历程，涵盖从萌芽期到*深度学习*期的各个阶段。接着，详细讲解了*机器学习*的基础知识，包括其三要素（数据、模型、评估）、主要类型（监督学习、无监督学习、强化学习）以及工作流程。... 一张图看懂：*人工智能*、*机器学习*、*深度学习**和*人工神经网络四个概念*之间*的相互*关系*. 在这个例子中，*人工智能*是驾驶员整体的能力，*机器学习*是他通过经验提升驾驶技巧的过程，*深度学习*是他应对复杂驾驶情境的能力，而神经网络是支持这些决策的具体技术工具。最外层代表*人工智能* (AI)，内部依次嵌套*机器学习* (ML)、*深度学习* (DL) *和*人工神经网络 (ANN)，展示了它们从广义到狭义的递进*关系*。这张图，展示了*人工智能*（AI）、*机器学习*（ML）、*深度学习*（DL）*和*人工神经网络（ANN）*之间*的相互*关系*。*人工智能*（AI）、*机器学习*（ML）、*深度学习*（DL）*和*人工神经网络（ANN）是相互关联的层次性概念。. 一句话理解 : AI、ML、DL、NLP、*LLM*. AI想造个聪明机器，ML教它自学成才，DL给它装了个超级大脑，NLP专攻语言课，*LLM*就是班里考满分的GPT学霸！：吞下全网万亿字文本，用千亿参数生成人类级回答（写代码、编小说、装心理医生）。：让机器像人类一样思考、决策（比如开车、下棋、对话）。：教机器听懂人话、说人话（翻译、写诗、客服对话）。：抖音推荐算法、支付宝信用评分、预测股票走势。模拟人脑，专攻复杂任务（如图像、语音、视频）。：扫地机器人、人脸识别、Siri语音助手。：ChatGPT、文心一言、通义千问。：感知环境、学习知识、规划行动。. 图解*深度学习* - *人工智能*、*机器学习**和**深度学习*. *机器学习*（Machine Learning，ML）\\*\\*是什么？ML是AI的一个分支，专注于让计算机通过数据学习并改进其性能。ML涉及训练算法以识别数据中的模式，并利用这些模式来做出预测或决策。ML可以分为监督学习、无监督学习*和*强化学习等多种类型。“一图 + 一句话”彻底搞懂什么是*机器学习*。“*机器学习*通过读取输入数据*和*答案，自动找出规则以完成任务，而非人类程序员编写规则，它是通过训练而非明确编程实现的。. *机器学习**和*大模型的*关系*，怎么入门. 总的来说,大模型是*机器学习*发展到一定阶段的产物,它们展示了*机器学习*的巨大潜力,同时也为*机器学习*的发展注入了新的动力。以往的*机器学习*主要关注特定任务的训练(如分类、回归等),而大模型引入了预训练-微调的范式。大模型,尤其是*自然语言处理*领域的大模型(如GPT-3、BERT等),是利用*机器学习*,特别是*深度学习*技术训练出来的。大模型的出现,展示了*机器学习*,特别是*深度学习*在处理复杂任务上的巨大潜力。大模型,尤其是预训练*语言模型*,具有强大的语言理解*和*生成能力,可以应用于问答、对话、摘要、翻译等多种*自然语言处理*任务。. 文章目录一：*人工智能*、*机器学习*、*深度学习*的*关系*二：*机器学习*1：*机器学习*的实现2：*机器学习*的方法论3：案例：牛顿第二定律4：确定模型参数5：模型结构介绍三：*深度学习*1:神经网络的基本概念2:*深度学习*的发展历程3:*深度学习*的研究*和*应用蓬勃发展4:*深度学习*改变了AI应用的研发模式5:实现了*深度学习*框架标准化. 一：*人工智能*、*机器学习*、*深度学习*的*关系*. *人工智能*：*人工智能*是研发用于模拟、延伸*和*扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学. AI黑话揭秘：一文打通AI、*机器学习*、大模型、*LLM*、Agent*之间*的*关系*. 本文就是来给你一次性讲清楚这些概念的*关系*——不说废话、不绕术语，直奔主题，让你看完后能真正搞明白：它们各自是啥，又是怎么串起来的。. 文章通过浙江大学*人工智能*教育教学研究中心的研究成果，展示了AI如何通过*深度学习**和*大模型来模拟人类的语言理解过程。 知识点一：语言的重要性 语言不仅帮助人类表达思想*和*情感，还使得人类能够以创造性的方式运用... * 工作时间 8:30-22:00.",
    "file_path": "unknown_source",
    "create_time": 1769008303,
    "update_time": 1769008303,
    "_id": "doc-4fcfe2fa03fc2203cba1b55a8668dc72"
  },
  "doc-3678971988880bb3960a90e15878444d": {
    "content": "[DOC_ID: chunk-581f76e1]\n[领域: 语言学]\n大语言模型（Large Language Models，LLM）是一种由包含数百亿以上权重的深度神经网络构建的语言模型，使用自监督学习方法通过大量无标记文本进行训练。",
    "file_path": "unknown_source",
    "create_time": 1769008615,
    "update_time": 1769008615,
    "_id": "doc-3678971988880bb3960a90e15878444d"
  },
  "doc-73a35fceff57d2bc7e2ac85fbd5203b4": {
    "content": "[DOC_ID: chunk-1db5dc88]\n[领域: 语言学]\n# 落痕的寒假. ## 今天也要加油鸭. # [[深度学习] 大模型学习1-大语言模型基础知识](https://www.cnblogs.com/luohenyueji/p/18644847 \"发布于 2024-12-31 22:21\"). 大语言模型（Large Language Model，LLM）是一类基于Transformer架构的深度学习模型，主要用于处理与自然语言相关的各种任务。简单来说，当用户输入文本时，模型会生成相应的回复或结果。它能够完成许多任务，如文本续写、分类、摘要、改写、翻译等。常见的LLM包括GPT、LLaMA等。本文将重点介绍LLM的基本原理和应用。详细内容可参考modelscope-classroom进行深入学习。. + 2.4 LLM量化、部署、优化. # 1 LLM基础知识. ## 1.1 LLM介绍. 随着LLM技术的飞速发展，Meta推出的LLaMA模型、Mistral AI发布的Mistral模型以及BigScience团队推出的BLOOM模型等多个开源LLM相继问世。这些模型在性能上已接近甚至媲美商业化LLM，进一步推动了LLM技术的广泛应用与创新。以下是几款代表性LLM系列的发展时间线，展现了这一领域的迅猛进步：. 到2024年底，在众多LLM中，闭源模型中表现最为出色的是GPT-4，而在开源模型中，LLama 3.3和LLama 3.2最为推荐。尽管LLama 3.2在各类基准测试中优于GPT-4，但在实际应用中，GPT-4的表现仍然更为卓越：. 2. 有监督微调（Supervised Fine-Tuning，SFT）：打造对话模型。. 3. 奖励模型训练（Reward Model Training）：培养能够评估回答的模型。. 4. 编码器-解码器注意力：解码器通过这个机制关注编码器的输出，帮助理解输入序列，从而生成更合适的输出。. * 主要用途： 仅编码器模型通常用于从输入数据中提取有用的特征信息，进行理解或表示学习。这些模型不需要生成输出，而是侧重于学习输入的上下文和表示。. * 应用： 这类模型在需要提取深度特征或做文本分类、情感分析等任务时非常有效。它们不涉及生成过程，而是通过理解和表示输入数据来完成任务。. * 主要用途： 解码器模型通常用于生成任务，尤其是序列生成任务，如文本生成、对话生成等。这类模型的目标是从给定的输入或上下文中生成连贯的输出。. * 应用： 解码器模型广泛应用于需要生成连续文本的任务，比如机器翻译、文本生成、代码生成等。. * 主要用途： 编码器-解码器模型适用于需要将一个输入序列映射到一个输出序列的任务，例如机器翻译、文本摘要、图像描述等。这种结构通常包含两个部分：编码器负责理解输入序列，解码器负责生成输出序列。. * 应用： 这类模型适用于任何需要将一个序列转换为另一个序列的任务，常见的应用场景包括机器翻译、摘要生成、对话生成等。. 1. 多模态大语言模型 (Multimodal Large Language Models). 构建LLM应用时，选择合适的方法整合专有和领域数据是关键一步，常用的方式包括提示词工程（Prompt Engineering）、模型训练与微调、以及检索增强生成（RAG，Retrieval-Augmented Generation）等。. 对于提示词工程的应用，Zero-Shot Learning（零样本学习）和Few-Shot Learning（少样本学习）是两种常见的提示词策略，它们在帮助用户获取所需内容时各有特点。以下是对这两种学习模式及其应用的进一步扩展说明：. 在Zero-Shot Learning模式下，用户提出任务时并不提供任何示例或样本，模型依赖于其预先学习的知识和语言能力直接生成答案。这样的模式适用于当用户需要一种快速、直接的生成内容方式，且不想提供或不需要提供额外的训练数据或背景信息。. 与Zero-Shot Learning不同，Few-Shot Learning是通过提供少量示例或样本来帮助模型更好地理解任务要求。通过这些示例，模型能够更准确地理解输入和输出之间的关系，从而提高生成内容的质量。此方式在某些情况下会更加精确，尤其是当用户有特定需求时，示例可以帮助引导模型产生更符合预期的回答。. ### 2.2.3 RAG. 检索增强生成（RAG, Retrieval-Augmented Generation）是一种融合LLM与外部知识库的技术。其核心理念是在LLM为用户提供答案时，首先通过从知识库中检索相关信息，并基于这些信息生成精确的回答，类似于对信息库进行快速查询以获得最佳解答。RAG的工作流程如下：. \\[q\\_{\\text{weight}} = \\text{round} \\left( \\frac{\\text{weight}}{\\text{scale}} \\right) \\]. 其中\\(q\\_{\\text{weight}}\\) 为量化后的权重，\\(\\text{weight}\\) 为量化前的权重，\\(\\text{scale}\\) 为缩放因子。可以看出，在进行量化时，通过缩放和取整操作，浮点数会丢失小数部分。在后续的计算或反量化过程中，由于无法完全恢复原浮点值，因此会出现一定的精度损失。. + llama.cpp/chatglm.cpp/qwen.cpp: 提供了高性能的C++实现，适合对性能有较高要求的场景。. 具体而言，对于输入向量\\(z\\)中的第\\(i\\)个元素，Softmax函数的定义为：. \\[\\text{Softmax}(z\\_i) = \\frac{e^{z\\_i}}{\\sum\\_{j} e^{z\\_j}} \\]. 其中，\\(e^{z\\_i}\\)表示\\(z\\_i\\)的指数，而分母是所有输入元素的指数和。这样的设计确保了较大值的输入元素对应的概率较高，而较小值的输入元素对应的概率较低，但不会完全为零。. \\[\\text{Softmax}(z\\_i, T) = \\frac{e^{z\\_i / T}}{\\sum\\_{j} e^{z\\_j / T}} \\]. 1. Static kv-cache and torch.compile. * Static kv-cache（键值缓存）是一种优化技术，旨在存储LLM在解码过程中的键值对，以避免重复计算，从而提高效率。与`torch.compile`结合使用时，通过预先分配kv-cache的大小，可以实现静态分配，进一步优化性能，可能带来最多4倍的速度提升。. * Prompt lookup decoding适用于输入和输出之间存在重叠词汇的任务（如摘要生成）。它通过在输入提示中进行字符串匹配，生成候选token序列，从而替代传统推测性解码中的草稿模型。. * Fine-Tuning with torch.compile and Padding-Free Data Collation：通过使用`torch.compile`进行微调，结合无填充数据整理技术，可以进一步提高模型运行效率。. * PyTorch scaled dot product attention：通过硬件加速和优化的计算图来加速模型的训练和推理。. * Llama 2: Open Foundation and Fine-Tuned Chat Models. * Prompt Engineering vs Fine-tuning vs RAG. posted @ 2024-12-31 22:21 落痕的寒假 阅读(2769) 评论(1) 收藏) 举报).",
    "file_path": "unknown_source",
    "create_time": 1769008641,
    "update_time": 1769008641,
    "_id": "doc-73a35fceff57d2bc7e2ac85fbd5203b4"
  },
  "doc-1a52d4be6b8f3e7719d553c603ce9c9f": {
    "content": "[DOC_ID: chunk-2f4801f3]\n[领域: 语言学]\n**数据准备。**收集、清理和整理用于训练 LLM 的原始数据。此步骤涉及数据清理（删除重复项和错误）、数据过滤（删除有偏见、淫亵或受版权保护的内容），以及词元化（将文本分解为模型可理解的单元）。. ### **LLM 和 Transformer**. Transformer 架构涉及数百万或数十亿个参数，这些参数使它能够捕捉复杂的语言模式和细微差别。事实上，“大语言模型”中的“大”字指的就是运行 LLM 所需的大量参数。. ### **LLM 和深度学习**. 引导 LLM 无监督学习过程的 Transformer 和参数都是一个更宽泛的结构——深度学习——的组成部分。深度学习是用来训练计算机以模拟人脑的算法来处理数据的人工智能技术。深度学习技术也称为深度神经学习或深度神经网络，旨在让计算机通过观察来学习、模仿人类获取知识的方式。. 最新的 LLM 可以理解和使用语言，这在过去是个人电脑所无法企及的。这类机器学习模型可以生成文本，归纳内容，以及进行翻译、重写、归类、分类和分析等。所有这些能力都为人类提供了一个强大的工具集，增强了我们的创造力，并且提高了解决难题的效率。. LLM 可以帮助补充或完全承担与语言相关的任务，如客户支持、数据分析和内容生成。这种自动化可以降低运维成本，同时为更具战略性的任务腾出人力资源。. LLM 可以快速扫描大量文本数据，使企业能够通过抓取社交媒体、评论和研究论文等来源，更好地了解市场趋势和客户反馈，这反过来又有助于为业务决策提供信息。. LLM 可帮助企业向客户提供高度个性化的内容，加强客户互动并改善用户体验。这可以表现为实施一个聊天机器人来提供全天候客户支持，根据特定用户角色定制营销信息，或者促进语言翻译和跨文化交流。. ## LLM 的挑战和局限. LLM 提示词往往复杂且不统一，处理海量数据通常需要大量计算资源与存储支持。llm-d 这类开源 AI 框架让开发人员可借助分布式推理等技术，满足 LLM 等复杂大型推理模型日益增长的需求。. 分布式推理和 llm-d 采用模块化架构，将推理任务分配到多台硬件设备上协同处理，助力 AI 工作负载高效运行，显著提升模型推理速度。. LLM 需要访问大量信息，有时包括客户信息或专有的商业数据。如果模型由第三方提供商进行部署或访问，那就必须特别谨慎小心。. 如果深度学习模型使用的训练数据存在统计学上的偏差，或者不能准确表示总体，则输出就可能存在缺陷。不幸的是，现有的人类偏见通常会传导到人工智能上，从而带来歧视性算法和偏见输出的风险。随着越来越多的企业利用 AI 来提高生产力和性能，至关重要的是制定相关策略，来尽量减少偏见。这首先需要确保在整个设计过程中秉持包容性的理念，并且要更深远地考虑所收集的数据是否代表足够的多样性。. ### LLM 的优势和局限. ### AI 使用中的监管和道德考量. * 许多 LLM 都具有“黑箱”特性，这会阻碍**透明度和可解释性**. #### AI 监管. AI 监管对于负责任地开发和监督 LLM 至关重要，它能确保模型运作符合企业组织的价值观和法律要求。随着 AI 法规的快速发展，企业组织必须优先确保遵守数据隐私法（如 GDPR 和 HIPAA）以及新的特定于 AI 的法规，这些法规通常要求对 AI 系统实施严格的风险管理、数据监管、人工监督以及强有力的 AI 系统安全防护。建立明确的问责框架也至关重要，明确由谁负责从开发到部署阶段的 LLM 性能和影响，并制定对关键决策至关重要的“人机回圈”策略。. ## 将 LLM 连接到外部数据源. * 检索增强生成（RAG）是一种通过整合所选知识源中的数据来扩充 LLM 知识库的架构。这些知识源可以是数据仓库、文本集合或既有文档。. * 代理式 AI 将自动化技术与 LLM 的创造力相结合。代理 AI 与工具的通信涉及编排，具体的流程或图表根据所使用的框架而有所不同。这种方法可以让 LLM 进行“推理”，并确定回答问题的最佳方式，例如判断能否利用现有信息回答当前问题，或者是否需要进行外部搜索。. ## LLM 与 SLM：语言模型对比. 进一步了解 LLM 与 SLM 的对比. 红帽 AI 提供对第三方模型库的访问权限，这些模型经过验证，可以在我们的平台上高效运行。这一套现成模型可以应用于容量指导规划的场景，帮助您针对特定领域的用例做出明智的决策。. ### 采用 LLM 的起点. 建议从红帽® 企业 Linux® AI 入手，它是我们的基础模型平台，您可以在该平台上针对企业应用开发、测试并运行 Granite 系列 LLM。借助 AI 平台，开发人员可以快速访问单个服务器环境，其中包含 LLM 和 AI 工具。它提供了调整模型和构建生成式 AI 应用所需的一切。. ## 开启企业 AI 之旅：新手指南. 了解模型上下文协议（MCP）如何将 AI 应用连接到外部数据源，助您构建更加智能的工作流。. ## AI/ML 相关资源. * 代理式 AI 与生成式 AI：有何区别. * SLM 与 LLM 的对比：什么是小语言模型？. * 一文了解什么是 AI 平台？有哪些类型，如何选择？. * AI/ML 用例有哪些？八大行业 AI 应用场景盘点.",
    "file_path": "unknown_source",
    "create_time": 1769008808,
    "update_time": 1769008808,
    "_id": "doc-1a52d4be6b8f3e7719d553c603ce9c9f"
  },
  "doc-1bb6bf3f22f6625482a3700d160f6e81": {
    "content": "[DOC_ID: chunk-a4d0b3a2]\n[领域: 语言学]\nNeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning (PDF). Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers. \"Ranking of Large Language Model (LLM) Cultural Bias\" --DIKWP Research Group International Standard Evaluation. \"Ranking of Large Language Model (LLM) Regional Bias\" --DIKWP Research Group International Standard Evaluation. \"The Large Language Model (LLM) Bias Evaluation (Age Bias)\" --DIKWP Research Group International Standard Evaluation. \"The Large Language Model (LLM) Bias Evaluation (Occupational Bias)\" --DIKWP Research Group International Standard Evaluation. | 概念 | * 代理人工智能（英语：Agentic AI） * 自编码器 + 变分自编码器 * 聊天机器人 + 列表 * 生成对抗网络 * GPT \"GPT (语言模型)\") * 幻觉 \"幻觉 (人工智能)\") + 模型崩溃（英语：Model collapse） * 大型语言模型 + 基础模型 + 推理语言模型 + 微调 \"微调 (深度学习)\") - 监督式微调（维基数据：Q118129371） + 检索增强生成 + 模型上下文协议 + BERT + 对话程式 + Ollama + 列表 * 机器学习 + 深度学习 + 强化学习 - RLHF + 监督学习 - 自监督学习 * 神经网络 * 提示工程 * Transformer架构 + 视觉transformer（英语：Vision transformer） * 向量数据库 * 词嵌入 |.",
    "file_path": "unknown_source",
    "create_time": 1769009100,
    "update_time": 1769009100,
    "_id": "doc-1bb6bf3f22f6625482a3700d160f6e81"
  },
  "doc-c73155a0054fe7510b458e0f7fe666b7": {
    "content": "[DOC_ID: chunk-9450a680]\n[领域: 语言学]\n# Search code, repositories, users, issues, pull requests... You signed in with another tab or window. You signed out in another tab or window. You switched accounts on another tab or window. ## 0x00 学习路径. + Prompt 工程、 RAG、Agent 等大模型应用开发范式。. ## 0x10 入门篇. * ChatGPT Prompt Engineering for Developers. + 虽然是 Prompt 工程，但是内容比较简单，适合入门者。. + OpenAI 官方 Quickstart 文档。以及 API Reference. * State of GPT：Andrej Karpathy 做的演示，极好的总结了 GPT 的训练和应用。 【必看】. * Deep Dive into LLMs like ChatGPT: Andrej Karpathy 最新的长达3小时的入门视频【必看】. ## 0x20 应用篇. * Building Systems with the ChatGPT API. * openai-cookbook：OpenAI 官方 Cookbook。. * Brex's Prompt Engineering Guide：Prompt 工程简介. ## 0x30 深入篇. ### 0x31 大模型技术基础方向. ### 0x32 大模型技术原理方向. ### 0x33 大模型训练微调方向. * Build a Large Language Model (From Scratch)：从零构建大模型。【必看】. ### 0x34 大模型数据工程方向. ### 0x35 大模型推理优化方向. ### 0x36 大模型应用方向. * A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks: Prompt 工程综述. * LLM Powered Autonomous Agents：Agent 早期的很不错的文章。. ## Issue actions. You can’t perform that action at this time.",
    "file_path": "unknown_source",
    "create_time": 1769009242,
    "update_time": 1769009242,
    "_id": "doc-c73155a0054fe7510b458e0f7fe666b7"
  },
  "doc-f394bd66e077e288f0b983860f06c440": {
    "content": "[DOC_ID: chunk-e8523e5c]\n[领域: 语言学]\n## 由先进的 Google AI 驱动的大语言模型. Google Cloud 将由 Google DeepMind 开发和测试的创新技术融入我们的企业级 AI 平台，使客户可以直接用于构建和提供生成式 AI 功能，无需准备，无需等待。. * Google Cloud 提供哪些 LLM 服务？. * LLM 在 Google Cloud 中是如何运作的？. Vertex AI 支持访问 Gemini，这是 Google DeepMind 推出的一个多模态模型。Gemini 能够理解几乎任何输入、组合不同类型的信息，还能生成几乎任何输出。在 Vertex AI with Gemini 中提供提示并执行测试，可使用文本、图片、视频或代码。利用 Gemini 的高级推理和先进的生成功能，开发者可以尝试使用示例提示，从而提取图片中的文本、将图片文本转换为 JSON，甚至可以针对上传的图片生成答案，以构建新一代 AI 应用。. 试用 Vertex AI Gemini API. Generative AI on Vertex AI：可让您访问 Google 的大型生成式 AI 模型，以便测试、调整和部署模型，并用于 AI 驱动的应用。. Vertex AI Agent Builder：允许开发者以开放的方式构建智能体，并为其部署企业级的控制功能。. Customer Engagement Suite with Google AI：智能联络中心解决方案，其中包括我们的对话式 AI 平台 Dialogflow，它同时具备基于意图的功能和 LLM 功能。. ### LLM 使用大量文本数据来训练神经网络。然后，此神经网络会用于生成文本、翻译文本或执行其他任务。用于训练神经网络的数据越多，它执行任务时的性能和准确性就越高。 Google Cloud 基于其 LLM 技术开发了多款产品，以满足不同的应用场景，您可以在下面的“常见用途”部分中进行探索。. * Vertex AI Agent Builder 文档. * Vertex AI Agent Builder 的价格. * Vertex AI Agent Builder 文档. * Vertex AI Agent Builder 的价格. ## 使用 Vertex AI LLM 处理并总结大型文档. ## 使用 Vertex AI LLM 处理并总结大型文档. ### 详细了解 Google Cloud 上的生成式 AI. ### 想要获得更多 Google Cloud AI 解决方案？. * 适用于 Google Cloud 的 Gemini. * SQL Server on Google Cloud. AI 赋能的助理，可在 Google Cloud 和 IDE 中使用。. * 适用于 Google Cloud 的 Gemini. * SQL Server on Google Cloud.",
    "file_path": "unknown_source",
    "create_time": 1769009336,
    "update_time": 1769009336,
    "_id": "doc-f394bd66e077e288f0b983860f06c440"
  },
  "doc-70cc6b4326bedb7c4b61745cce643075": {
    "content": "[DOC_ID: chunk-890ac9f3]\n[领域: 地球科学]\n摘要：. 遥感图像分析在国土资源管理、海洋监测等领域有着极为广阔的应用前景。深度学习技术已在图像处理领域取得突破性进展，然而，遥感图像固有的尺寸大、目标小而密集",
    "file_path": "unknown_source",
    "create_time": 1769009424,
    "update_time": 1769009424,
    "_id": "doc-70cc6b4326bedb7c4b61745cce643075"
  },
  "doc-81494149ccc19d39342ca4e7f5dc4668": {
    "content": "[DOC_ID: chunk-551b549b]\n[领域: 地球科学]\n本研究系统地综述了深度神经网络在遥感图像分类、目标检测、语义分割和变化检测等关键任务中的应用，分析了国内外在该领域的研究现状与进展。此外，本研究还探讨了多模态数据",
    "file_path": "unknown_source",
    "create_time": 1769009455,
    "update_time": 1769009455,
    "_id": "doc-81494149ccc19d39342ca4e7f5dc4668"
  },
  "doc-d03520804df199a76ce35edbbeca10f8": {
    "content": "[DOC_ID: chunk-af9389ef]\n[领域: 地球科学]\nImbal‐ anced learning-based automatic SAR images change detection by morphologically supervised PCA-Net. IEEE Geoscience and Remote Sensing Letters, 16(4): 554-558 [DOI: 10.1109/lgrs.2018.2878420] Wang Y H, Gao L R, Chen Z C and Zhang B.",
    "file_path": "unknown_source",
    "create_time": 1769009501,
    "update_time": 1769009501,
    "_id": "doc-d03520804df199a76ce35edbbeca10f8"
  },
  "doc-326a81b79c40b3a07062403d80063e34": {
    "content": "[DOC_ID: chunk-2d442d76]\n[领域: 地球科学]\n从尺度不变性、旋转不变性、复杂背景干扰、样本量少和多波段数据检测5个角度出发，总结了近几年基于深度学习的遥感图像目标检测方法。此外，对典型遥感图像目标的检测难点和",
    "file_path": "unknown_source",
    "create_time": 1769009546,
    "update_time": 1769009546,
    "_id": "doc-326a81b79c40b3a07062403d80063e34"
  },
  "doc-fb89c8462c112775ff8e6c43e7d050e8": {
    "content": "[DOC_ID: chunk-5e9e4efd]\n[领域: 地球科学]\nIn order to explore the feasibility of deep learning derived global features for satellite remote sensing image retrieval and positioning without auxiliary parameters, an evaluation system considering both effectiveness and efficiency is established, which quantifies the Precision@K, average ranking, feature extraction time, feature similarity calculation time, and hardware consumption. The results show that: (1) the global features extracted by deep learning models have higher effectiveness in satellite remote sensing image retrieval and positioning. Compared with local features, these models provide a new way for satellite remote sensing image retrieval and positioning; (2) based on the test datasets, the performance of DenseNet, ResNet-18, and VggNet is relatively better, and the precision@K of DenseNet is the highest, indicating the highest success rate. satellite remote sensing image/retrieval and location/deep learning/convolutional neural network/global features/image representation/effectiveness/efficiency{{custom_keyword}}/. Evaluation and Analysis of Deep Learning Global Representation Model for Satellite Remote Sensing Image Retrieval and Location[J].",
    "file_path": "unknown_source",
    "create_time": 1769009592,
    "update_time": 1769009592,
    "_id": "doc-fb89c8462c112775ff8e6c43e7d050e8"
  },
  "doc-f63d6eb86a6dff5d8cb2c08238a0e76e": {
    "content": "[DOC_ID: chunk-d428b834]\n[领域: 地球科学]\n## **lvxiangyang�ĸ��˲�������**. # ����PyTorch����ѧϰң��Ӱ������������Ŀ�����⡢�ָң��Ӱ����������ѧϰ�Ż�ʵ������Ӧ��. ���� 1398 ���Ķ� 2025-9-25 09:40 |���˷���:ң��|ϵͳ����:���бʼ�. ң��Ӱ�񲻶ϱ���������Ӧ���ڿ�����̽����׼ũҵ�����й滮����ҵ����������Ŀ��ʶ�����ֺ������С�. 1.����ѧϰ��ң��ͼ��ʶ���еķ�ʽ������. 2.��������ѧϰ����ʷ��չ���̣�������������ѧϰ��ң��Ӧ���е���ȱ��. 3.3.����ѧϰ������ѧϰ�������Ĵ�������. 6���ػ�������ȫ���Ӳ㣬�Լ������������ü���Ӧ���е�ע������. ��1����ͬ������������ʼ����ѧϰ�ʶԽ�����Ӱ��. ��2��ʹ��PyTorch��������粢ʵ��ң��ͼ�񳡾�����. **����������������ʵ����ң��Ӱ��Ŀ������**1.����ѧϰ�µ�ң��Ӱ��Ŀ����������֪ʶ2.Ŀ���������ݼ���ͼ���ͱ�ǩ��ʾ��ʽ3.Ŀ������ģ�͵�����������������ȷ�ʣ���ȷ�ʣ��ٻ��ʣ�mAP��4.two-stage�����ף�����ģ�Ϳ��ܣ�RCNN, Fast RCNN, Faster RCNN�ȿ��ܵ��ݱ��Ͳ���5. one-stage��һ�ף�����ģ�Ϳ��ܣ�SDD ��Yolo��ϵ��ģ��6.���м���ģ�ͷ�չС��. ��1��һ��������Faster-RCNN ģ����ʵ��ң��Ӱ����Ŀ������. ��4��ģ�͵Ĵ�����Ϻ�ѵ��. 1.����ѧϰ�µ�ң��Ӱ���ָ������Ļ�������. 2.FCN��SegNet��U-net��ģ�͵Ĳ���. 4.ң��Ӱ���ָ�������ͼ���ָ��Ĳ���. 5.��ң��Ӱ���ָ������е�ע������. ��2��ң��Ӱ�񻮷ֳ�Сͼ���Ĳ���. ��4����֤����ʹ�ù����е�ע������. **����ң��Ӱ������̽��������ѧϰ�Ż�����**1.���м�������ģ�ͽṹ���ݱ�ԭ��������AlexNet��VGG��googleNet��ResNet��DenseNet��ģ��2.��ģ���ݱ��н���ʵ��ѵ��ģ�͵ļ���3.�������ݵ��Ż�����4.����ģ�͵��Ż�����5.����ѵ�����̵��Ż�����6.���Լ����������Ż�����7.���Էָ��������Ż�����. 8.�ṩһЩ���õļ��⣬�ָ����ݼ��ı�ע����. ԭ�ģ�����PyTorch����ѧϰң��Ӱ������������Ŀ�����⡢�ָң��Ӱ����������ѧϰ�Ż�ʵ������Ӧ��. ��һƪ��������-���˻�-���桱ң�����ݿ���ʹ�ü����ﺬ��������ʵ�ַ���. ��һƪ�����˻�ң��ͼ��ƴ�Ӽ�����ʵ������Ӧ��. �ղ� IP: 111.225.70.\\*| �ȶ�|. #### 1 ����. ### �ò�������ע���û����� ��������¼ ���� (0 ������). ## ������. ## ȫ�����ߵ��������²���. * • ����PLUS-InVESTģ�͵���̬ϵͳ�������龰����ģ�������������Ż�������д��. * • ����ˮ��ֵģ������Visual modflow Flex. * • ����ɭ��ģ�ͺ��ļ���������Ӧ��. * • ����R���Եı�Ҷ˹����ģ�͵�ʵ������Ӧ��. * • ����R����lavaan�ṹ����ģ�ͣ�SEM��ʵ������Ӧ��. * • SWAPũҵģ�������Ʊ��������Է����������仯Ӱ��ʵ������Ӧ��. ## ȫ����ѡ���ĵ���. * • ��������ح��ˮ������ѧ����������ˮ�������е�TRP���嵰������. * • ��ѧ��2025��12��ʮ�Ѳ��İ񵥹�����. * • ��������ۭ���Ŷӣ����ز�״�������︳�ܿɳ�����Դ������ҽѧ�²��Ϸ�չ. Archiver|�ֻ���|**��ѧ��** ( ��ICP��07017567��-12 ). GMT+8, 2026-1-20 17:34.",
    "file_path": "unknown_source",
    "create_time": 1769009685,
    "update_time": 1769009685,
    "_id": "doc-f63d6eb86a6dff5d8cb2c08238a0e76e"
  },
  "doc-4a38cf117c062d1c34ec79abbff9e244": {
    "content": "[DOC_ID: chunk-36429fb7]\n[领域: 地球科学]\n随着地球观测卫星的增多，气候模型变得丰富起来，逐渐成为一个数据问题。 研究人员开始尝试把人工智能与气候学相结合，分析海量的气候数据，以期发现新的气候模型并提升天气",
    "file_path": "unknown_source",
    "create_time": 1769009874,
    "update_time": 1769009874,
    "_id": "doc-4a38cf117c062d1c34ec79abbff9e244"
  },
  "doc-babf4b9b036bdc4286b6ed4dc016cba8": {
    "content": "[DOC_ID: chunk-c1f894a4]\n[领域: 地球科学]\n通过将DFS-M和DFS-S模型对于不同时次的预报进行结合，可以得到效果更加均衡的预报。本研究可以为基于深度学习的天气气候预报方法的选择提供新的思路。 关键词: 天气气候预报",
    "file_path": "unknown_source",
    "create_time": 1769009910,
    "update_time": 1769009910,
    "_id": "doc-babf4b9b036bdc4286b6ed4dc016cba8"
  },
  "doc-b7e84880c36323dbbf1d92748718a1d1": {
    "content": "[DOC_ID: chunk-e2dc7886]\n[领域: 地球科学]\n# 机器学习模拟千年气候. ### Share this:. * Click to print (Opens in new window) Print. * Click to email a link to a friend (Opens in new window) Email. * Click to share on Bluesky (Opens in new window) Bluesky. * Click to share on Reddit (Opens in new window) Reddit. * Click to share on Facebook (Opens in new window) Facebook. * Click to share on LinkedIn (Opens in new window) LinkedIn. ##### Source: *AGU Advances*. *This is an authorized translation of an Eos article. 近年来，科学家们发现，基于机器学习的天气模型可以比传统模型更快地做出天气预测，且使用更少的能耗。然而，许多这些模型无法准确预测未来15天以上的天气，并且到第 60 天时就会开始模拟出不切实际的天气。. 深度学习地球系统模型（Deep Learning Earth System Model，简称DLESyM）建立在两个并行运行的神经网络上：一个模拟海洋，另一个模拟大气。在模式运行期间，对海洋状况的预测每四个模式日更新一次。由于大气条件演变得更快，对大气的预测每12个模式小时更新一次。. 该模型的创建者Cresswell-Clay 等人发现，DLESyM 与过去观测到的气候非常吻合，并能做出准确的短期预测。以地球当前的气候为基准，它还可以在不到 12 小时的计算时间内，准确模拟 1000 年周期内的气候和年际变化。它的性能通常与基于耦合模式比对计划第六阶段（CMIP6）的模型相当，甚至优于后者，CMIP6目前在计算气候研究中被广泛使用。. DLESyM 模型在模拟热带气旋和印度夏季季风方面优于 CMIP6 模型。它至少与 CMIP6 模型一样准确地捕捉了北半球大气“阻塞”事件的频率和空间分布，而这些事件可能导致极端天气。此外，该模型预测的风暴也非常真实。例如，在 1000 年模拟结束时（3016 年）生成的东北风暴的结构与 2018 年观测到的东北风暴非常相似。. 作者认为，DLESyM模型的主要优势在于，它比运行CMIP6 模型所需的计算成本要低得多，这使得它比传统模型更容易使用。(*AGU Advances*, , 2025). *This translation was made by* *Wiley**.* *本文翻译由**Wiley**提供。*. Read this article on WeChat. CC BY-NC-ND 3.0 Except where otherwise noted, images are subject to copyright. Any reuse without express permission from the copyright owner is prohibited. #### Features from AGU Publications. ## ALMA’s New View of the Solar System.",
    "file_path": "unknown_source",
    "create_time": 1769009939,
    "update_time": 1769009939,
    "_id": "doc-b7e84880c36323dbbf1d92748718a1d1"
  },
  "doc-ed300ee31268ac8e853d58a7e20740f0": {
    "content": "[DOC_ID: chunk-d665a7cc]\n[领域: 地球科学]\nMeteorological elements forecasting method based on deep learning[J]. Key words: meteorological forecast deep learning neural network long short term memory. | $ J(x) = {(x - {x\\_b})^{\\rm{T}}}{B^{ - 1}}(x - {x\\_b}) + \\sum\\limits\\_{i = 0}^n {{{({y\\_i} - {H\\_i}|{x\\_i}|)}^{\\rm{T}}}} R\\_i^{ - 1}({y\\_i} - {H\\_i}|{x\\_i}|) $ | (1) |. Monthly rainfall forecasting using one-dimensional deep convolutional neural network[J]. An ensemble of neural networks for weather forecasting[J]. Determination of input for artificial neural networks for flood forecasting using the copula entropy method[J]. Comparison of methods used for quantifying prediction interval in artificial neural network hydrologic models[J]. Artificial neural network modeling of rainfall-runoff process[J]. Deep neural network modeling for big data weather forecasting, information granularity, big data, and computational intelligence[M]. Hydrological early warning system based on a deep learning runoff model coupled with a meteorological forecast[J]. | [23] | SHI X J, SHI Z R, WAN H G, et al. Deep learning in neural networks: An overview[J].",
    "file_path": "unknown_source",
    "create_time": 1769010032,
    "update_time": 1769010032,
    "_id": "doc-ed300ee31268ac8e853d58a7e20740f0"
  },
  "doc-506b50fbb39065edcc65dbbbc31a2bd7": {
    "content": "[DOC_ID: chunk-26687d01]\n[领域: 地球科学]\n本发明公开了一种基于深度学习和数值天气预报的天气现象预报方法，包括：根据数值天气预报产品和天气现象观测数据，构建训练数据集；根据深度学习网络模型对所述训练数据集",
    "file_path": "unknown_source",
    "create_time": 1769010149,
    "update_time": 1769010149,
    "_id": "doc-506b50fbb39065edcc65dbbbc31a2bd7"
  },
  "doc-ffd1a702a5052372a15a2cc72ea7ca01": {
    "content": "[DOC_ID: chunk-53d34872]\n[领域: 地球科学]\n在气象预报应用领域，已有学者对深度学习方法. 的可解释性展开了深入讨论 ... . 基于深度学习改进数值天气预报模式和预报. 的研究及挑战. 气象科技进展，11（3",
    "file_path": "unknown_source",
    "create_time": 1769010183,
    "update_time": 1769010183,
    "_id": "doc-ffd1a702a5052372a15a2cc72ea7ca01"
  },
  "doc-a6b4a600bac8a52a4a59aa8136dc667e": {
    "content": "[DOC_ID: chunk-dd4154a7]\n[领域: 地球科学]\n研究结果表明，两个直接预报模型对整体预报时段的预报效果明显优于迭代预报模型，直接预报模型的RMSE比迭代预报模型低19%。随着预报时次的增加，迭代预报模型的预报误差累积",
    "file_path": "unknown_source",
    "create_time": 1769010208,
    "update_time": 1769010208,
    "_id": "doc-a6b4a600bac8a52a4a59aa8136dc667e"
  },
  "doc-333ff749c2940b520a06027eb3c17eac": {
    "content": "[DOC_ID: chunk-72f952a2]\n[领域: 地球科学]\n真实预测未来7天的天气气象数据，使用多种机器学习和深度学习LSTM算法，采集全国各省历史、实时数据（天气预报、极端天气预警、生活指数），可视化大屏，Hadoop、Spark、Hive",
    "file_path": "unknown_source",
    "create_time": 1769010233,
    "update_time": 1769010233,
    "_id": "doc-333ff749c2940b520a06027eb3c17eac"
  },
  "doc-5837f0d5dd6a7cba90da952aebd39e90": {
    "content": "[DOC_ID: chunk-01e65843]\n[领域: 地球科学]\nAARes-ConvLSTM 均方根误差 预报技巧 报告内容 智能气候预测的理念与案例 基于机器学习方法的中国降水预测 基于中尺度气象同化预报和深度学习的风速预报 光伏电站太阳辐射超短期预报 基于深度学习的北极地区格点化地表气温重建 气象人工智能未来发展思考 • 已有研究表明：极地缺测对研究全球温度变化存 在明显的影响 • 重建一套覆盖30°N以北的地表空气温度数据集 • 北极地区相对于中纬度观测十分有限 • 数据最少的地方出现在格陵兰岛和北冰洋部分区 域 研究目的 59 北极缺测情况以及其所带来的影响 1850年至今HadCRUT5观测覆盖情况 Huang et al. Res.: Atmos., 2021) 在北极地区仍存在不足： • 以上资料时间分辨率较低（月资料） • 未使用极地观测或对观测使用不充分 全球格点温度资料 60 基于器测观测的全球温度资料 序号 资料 起止时间 时间分辨率 陆地气温资料 海表面温度 参考文献 1 HadCRUT5 1850-2020 月 CRUTEM5 HadSST4 Morice et al. Res.: Atmos., 2021) 2 GISTEMP v4 1880-2020 月 GHCN v4 ERSST v5 Lenssen et al. Res.: Atmos., 2019) 3 NOAAGlobalTemp-Interim 1850-2020 月 GHCN v4 ERSST v5 Vose et al. 1850-2020 月 CRUTEM5 HadSST4 Kadow et al. Soc., 2014) 8 Vaccaro et al. 1850-2020 月 CRUTEM4 HadSST3 Vaccaro et al. Soc., 2016) 2000-2012 Most of the Arctic 30 km, L71 15 km, L71 Dahlgren et al. Clim., 1997) optimal interpolation Rigor et al.",
    "file_path": "unknown_source",
    "create_time": 1769010302,
    "update_time": 1769010302,
    "_id": "doc-5837f0d5dd6a7cba90da952aebd39e90"
  },
  "doc-44aa5ced4dbdd708f5b101c55c7bb894": {
    "content": "[DOC_ID: chunk-5fe70e58]\n[领域: 地球科学]\nZheng Y G, Zhu W J, Yao D, et al, 2016b. Zheng Y G, Zhou K H, Sheng J, et al, 2015. | Billet J, DeLisi M, Smith B G, et al, 1997. | Gagne II D J, McGovern A, Haupt S E, et al, 2017. | Kim M, Im J, Park H, et al, 2017a. | McGovern A, Elmore K L, Gagne D J, et al, 2017. | McGovern A, Lagerquist R, Gagne II D J, et al, 2019. | Mecikalski J R, Li X L, Carey L D, et al, 2013. | Mecikalski J R, Mackenzie W M Jr, Köenig M, et al, 2010a. | Mecikalski J R, Mackenzie W M Jr, König M, et al, 2010b. | Shi X J, Chen Z R, Wang H, et al, 2015. | Wang Y B, Long M S, Wang J M, et al, 2017. | Wilson J W, Feng Y R, Chen M, et al, 2010.",
    "file_path": "unknown_source",
    "create_time": 1769010527,
    "update_time": 1769010527,
    "_id": "doc-44aa5ced4dbdd708f5b101c55c7bb894"
  },
  "doc-ea37ab1462f7ff56b1e76b106d6df9dd": {
    "content": "[DOC_ID: chunk-79e9cb14]\n[领域: 哲学]\nDOI: 10.12677/acpp.2025.145279 深度学习的技术路径与哲学审视 杨 媛 华南师范大学马克思主义学院，广东 广州 收稿日期：2025年5月1日；录用日期：2025年5月23日；发布日期：2025年5月31日 摘 要 作为人工智能领域的核心技术之一，深度学习在近年来的研究和应用层面都取得了显著进展。本文从技 术演进、学习范式对深度学习进行哲学角度审视。首先梳理深度学习的发展脉络，然后深入讨论了深度 学习的学习范式，探索了深度学习面临的现实问题。深度学习作为一种强大的机器学习方法，引发了许 多哲学问题的关注与思考。通过对有关深度学习哲学思考的文献进行梳理，思考如何使用哲学对深度学 习技术进行指导，同时反思深度学习技术又如何影响现代哲学。 关键词 深度学习，人工智能，技术哲学 Technical Path and Philosophical Review of Deep Learning Yuan Yang School of Marxism, South China Normal University, Guangzhou Guangdong Received: May 1st, 2025; accepted: May 23rd, 2025; published: May 31st, 2025 Abstract As one of the core technologies in the field of artificial intelligence, deep learning has made sig-nificant progress in research and application in recent years. By combing the literature on philosophical thinking of deep learning, this paper considers how to use philosophy 杨媛 DOI: 10.12677/acpp.2025.145279 526 哲学进展 to guide deep learning technology and reflects on how deep learning technology affects modern philosophy.",
    "file_path": "unknown_source",
    "create_time": 1769010673,
    "update_time": 1769010673,
    "_id": "doc-ea37ab1462f7ff56b1e76b106d6df9dd"
  },
  "doc-520def31c8675778cae230b17d8f6b02": {
    "content": "[DOC_ID: chunk-90d802f3]\n[领域: 哲学]\nCameron J. Buckner的《From Deep Learning to Rational Machines》介绍了深度学习的哲学涵义。 从2015年开始，深度学习神经网络已经开始从根本上改变",
    "file_path": "unknown_source",
    "create_time": 1769010758,
    "update_time": 1769010758,
    "_id": "doc-520def31c8675778cae230b17d8f6b02"
  },
  "doc-56ba072dd517d40a45be43bb193491e8": {
    "content": "[DOC_ID: chunk-96b9853f]\n[领域: 哲学]\n# 张颖天 戴宁馨：人工智能时代的哲学思考. *张颖天 戴宁馨**2025年05月12日09:06**来源：光明日报*. **“哲学是世界观和方法论的统一。”无论是面对过去、现在，还是未来，哲学思考始终在探究人类认知的边界。随着人工智能时代的到来，科技与人类未来关系的认知成为哲学研究的重要视域，也促使哲学研究者们进一步反思人类的本质、人工智能的属性以及二者之间的关系。为此，本刊特组织三位青年学者围绕人工智能时代的哲学研究面临的挑战、哲学研究的不可替代性以及如何在时代中掘进等角度展开讨论，并邀请专家予以点评，以期启发更多学者对时代命题进行哲学反思与理性探究。**. **王 田** 中共北京市委党校〔北京行政学院〕哲学与文化教研部讲师. **主持人：人工智能的发展无疑给哲学研究带来诸多挑战，比如，在认知层面，能处理海量数据的算法模型对人的思维认知方式的挑战。请谈谈在进行哲学研究时，人的主体性创造将会面临哪些困难与挑战？**. 就哲学学者的个人研究实践来说，人工智能的发展，尤其是大型语言模型和深度学习算法的广泛应用，确实给我们带来了不少挑战。首先，人工智能在信息搜索、归纳整合、分析模式等方面展现出了惊人的能力。面对人工智能的高效输出，哲学研究者有可能对其产生过度的信任或依赖，陷入一种“原创焦虑”，进而丧失思维的自主性和原创性。其次，作为哲学研究者，当我们越来越依赖人工智能提供的现成答案时，就有可能被AI所预设的问题框架所牵制，难以发现问题之所以成为问题的深层原因和结构，最终有可能失去“深度求索”的能力。最后，哲学思考的典型图景是尝试进入一种“无人之境”的孤独旅程，这当然不是说哲学思考不需要与他人对话，而是说，哲学思考需要在自我思想世界具备独自探索的能力，但人工智能的便利可能导致哲学研究者在思考初期就引入大量的外部声音，从而扰乱思想主体酝酿生成新思想的过程。. 站在人类整体的角度，人工智能目前尚不足以真正威胁到人类在哲学领域的主体性创造。因为哲学思考的核心并不在于处理信息的效率，不在于对已有问题和答案的梳理与整合，而在于提出新问题或对于老问题进行重新界定，在于发现和质疑已有框架的种种预设，在于路径以及概念上的革新。哲学创造所需要的这些能力高度依赖于人类心智的独特性，并且这种独特性很难还原为能被清晰表述的规则系统或者算法。因此，除非未来的人工智能发展出了与人类相当的心智，否则哲学创造依然是属人的。. **赵立：**作为人类思想的产物，哲学的产生与发展无疑与人的创造性思维紧密相关。而人工智能作为对人类既往思想成果的高效技术整合，对当代哲学思考发起了巨大挑战。从人工智能时代的实践之维看，人工智能技术深度参与的全新世界图景要求我们必须适应其演变速度、改变深度和影响广度，在变动不居的实践地基上进行哲学研究与思考。但是，快速演化、不断迭代的人工智能技术使得时代要求愈发迫切而难以停留、时代特征愈发复杂而难以捕捉，因而“黄昏起飞的密涅瓦猫头鹰”既难以静心回望、系统沉思，也难以穿透现象、直达本质。从人工智能时代的思想之维看，现代科技的不断突破早已使得实证科学的思维方式深入人心，而人工智能的技术思维更是展现了其强大的应用能力，从而更为剧烈地冲击了哲学思考的意义与价值。因此，我们必须回应的问题是，人的思想活动在人工智能时代是否还有必要？还能有何作为？质言之，人的哲学思想在思考层次和思考结果上如何超出人工智能的技术逻辑，确证人类思考的独特主体价值。这是必须搞清楚、弄明白的核心问题。. **王田：**人工智能的迅猛发展使得传统哲学中作为理性核心的主体概念面临多重挑战。一是主动性和被动性的悖论。表面上看，人处于开启人机对话的主动地位、占据主导权，人工智能作为手段不过是被动地完成任务。实际上，人类若想高效获取信息必须以Prompt（提示词）来向AI表达需求，提示词越贴近人工智能语言，得到的结果就越精准。这样一来，占据主导地位的人的语言反而要适应机器语言，出现了主动性变为被动性的悖论，进而有可能出现因AI的广泛应用使得人类的思维和本质被控制、被异化。二是知识获取加速化和知识增量受限的悖论。AI能够通过输入指令迅速整理并生成无数结果，看似使得知识总量加速增长。但通过AI获取的是对已有知识的计算、推理和重组，人作为主体创造新知识的能力让位于获取旧知识，总体知识增量反而受限。并且，“哲学就是哲学史”，哲学研究尤其依赖对于哲学家与所处时代、思想发展史以及相互之间“历时态”和“共时态”对话的长期积累过程。当人类把认知“外包”给AI并习惯“加速”获取知识后，必然会挤压哲学积累的空间，使阅读、实践和反思等能力退化。三是人与机器的身份悖论。一般认为，AI是人的各种功能器官的外在工具性延伸，并不包含理性、感情和意志的精神意识，但AI在提供便利的同时也逐渐弱化了人的理性思考、发挥主观能动性等自主性活动，人反而成为只会发出提示语和接收推送信息的“机器”。当AI机器人索菲娅获得法律上的公民身份，哲学必须对“人是什么”以及人的本质进行深刻反思。. **王田：**人工智能带来的社会困境迫使哲学反思人工智能的安全性、可靠性和公平性问题。一是数据安全性研究。大数据技术凭借数据采集、海量存储及智能分析，可以对个体身份特征、行为轨迹和空间定位等隐私信息进行全息记录。在缺乏有效监管机制的情况下，个人信息可能被泄露，公民的隐私权益会遭到损害。二是信息可靠性研究。AI模型有时会生成表面上具有逻辑性且语法正确的文本，但其输出内容可能存在数据误用、语境误解、信息缺失、推理错误、无中生有等问题，还可能导致虚假信息传播，进一步扰乱公共秩序。三是数字偏见的公平性研究。由于接触AI的机会不均等、使用AI的能力不平等、与AI相融合的程度不平衡，“数字鸿沟”已成为不争的事实。这些都构成了哲学研究的新视域。. **魏犇群：**生成式人工智能的迅猛发展给哲学研究的视域带来了深刻变化。这些变化不仅体现在人工智能应用会激发出新的哲学问题，如人工智能的道德地位问题、智能驾驶的事故归责问题、数据偏见与隐私保护问题等。更为重要的是，人工智能的出现和发展本身就会改变甚至颠覆传统的哲学观念和视野。在这个意义上，人工智能可以被视为一个“哲学事件”。为什么这么说呢？. 首先，随着人工智能的不断进化，人与机器的区分会不停地遭遇挑战，这就倒逼我们重新理解一系列与之相关的基础概念，比如“意识”“理解”“能动性”“创造力”“语言”“道德”等，而对这些基础概念的深入探究正是哲学的任务。以“理解”这个概念为例，有一种传统观点认为，像AI这样的人造系统只会按照句法规则猜测下一个词，并不能真正理解词义，也不可能理解世界。然而，当我们看到当下生成式AI模型在博弈游戏中娴熟地制定策略，在交流时不仅对答如流，而且还能对外部现实做出出人意料的精妙推论，我们不禁怀疑，这样的AI自身是不是构建出了对于世界的表示系统，甚至已经能够理解到一部分世界结构了？而这又会促使我们思考，“理解”到底是什么意思？怎样的“理解”概念才会把人工智能从具有理解能力的事物中排除出去？也就是说，由于人工智能的出现，我们之前对于“理解”的理解已经不够用了。其次，在人工智能被普遍应用的时代，关于技术的深层思考会构成哲学研究中的一个主要领域。也就是说，哲学的内部版图将被重组。广义的“技术哲学”会逐渐成为与形而上学、知识论、伦理学等传统哲学领域并驾齐驱的新领域。此领域专门研究与技术相关的基础议题，比如“技术如何塑造我们的生活世界”以及“应该如何理解人与技术的关系”。随着技术哲学的发展，可以预见，哲学在未来会与科学产生更深的融合。最后，如果人工智能持续逼近人类各项能力的边界，一个古老的哲学问题会变得愈发紧迫：人是什么，或者，人之为人的根据是什么？哲学问题具有跨越时空的普遍性很大程度上是因为它们是用抽象而模糊的概念表达的，细究起来我们常常会发现，同一个哲学问题会在不同时代或者不同的社会环境中呈现出不同的形态。因此，当我们在人工智能时代问出“人是什么”的问题时，我们不是要回到人文主义的旧图景，而是要在人工智能技术的冲击下重新定义人的独特性和不可替代性。这将成为人工智能时代的哲学母题。. **赵立：**人工智能技术正在以一种不可扭转、不可抗拒的姿态塑造着我们的时代，使得哲学研究也要对变化的时代进行持续思考与回应。比如，在跨学科对话中重思人与技术的关系问题，以及深度思考人的社会存在，考察政治、经济与社会等诸多领域的新问题。人工智能时代人的社会生存必然遭遇新的情景、产生新的问题，例如，人工智能技术批量制造的真假难辨的信息、算法塑造的不平等鸿沟，等等。. **主持人：根据马克思劳动价值论，数据要素的价值依然凝结在人类劳动中。哲学作为人类智慧之学，哲学研究也意味着人类的思维劳动生产活动。那么，应如何把握人工智能技术的算法价值与哲学价值之间的张力？**. **魏犇群：**现如今，人工智能及其算法已经成为新型生产要素，可以为社会创造价值，因此有“算法价值”之说。按照马克思的理论，价值来自人类劳动的凝结。人工智能系统虽然尚不能被算作可以劳动的主体，但其算法模型的设计、训练、调试与优化，仍然凝结了人类的劳动。因此，目前的算法价值仍然是人类思维劳动的产物。但同样是人类思维的产物，哲学的价值与算法价值有何区别？至少就目前的状况来说，算法价值主要表现在信息处理上面，而哲学的价值则主要表现在批判性洞察与意义创造上面；算法价值往往体现为效率、规模、预测能力、提供现成方案等方面，而哲学的价值则体现为对问题背后的前提、框架的设定、基本概念的内涵及限度的反思。如果说目前的算法价值在于最终有望替代人类的“劳作”的话，哲学的价值则在于帮助人类在意识中把握自身的“劳动”，或者说“类本质”。因此，哲学价值无法被算法价值取代或者通约。. **王田：**人工智能技术的算法价值与哲学价值之间的张力，其理论本质是在人工智能时代对劳动价值论的再审视。人工智能作为突破性技术确实替代了一部分活劳动，产生了数字劳动新形态，改变了劳动的时空条件，并在这种显著的替代效应下进一步提高了资本有机构成、降低了单位商品价值量，使价值实现方式发生巨大转变。然而，这些并没有否定马克思劳动价值论中有关价值源泉唯一性、劳动者主体地位和资本属性等重要判断。劳动价值论的核心要义在于人的活劳动是创造价值的唯一源泉，马克思并不是基于单个具体劳动而是基于整体生产过程探寻价值的源泉。我们不能因无人化生产成为可能就忽略人工智能研发、分配、交换和消费各环节都需要劳动者参与的事实。人工智能本身就是劳动的产物，劳动者起到了不可替代的主体作用。面对人工智能非但无法消除资本的剥削性质，反而将剥削行为变得更加隐蔽的困境，唯有立足劳动解放论与人的自由全面发展，才能实现算法价值与哲学价值的辩证统一。. **赵立：**基于可计算认知模型形塑当代世界的人工智能技术，其关键工具就是高度数学形式化的算法。可以说，算法在某种程度上已经成为我们认识世界与改造世界的核心方式之一。因此，作为人类智慧的集体体现，算法以精确化、数字化、组织化的方式实现了对当代社会的整体建构，极大拓展了人类文明发展的可能空间。但我们更应重视、发掘哲学的价值，以免走向“唯算法”的技术决定论一端。历史地看，技术逻辑与哲学逻辑的争锋早已嵌入人类社会发展的各个阶段，但算法在人工智能时代的狂飙突进，已然在社会生活、伦理规范与思想意识等方面引发了诸多争议。对此，我们有必要重启技术逻辑与哲学逻辑的深层次对话，在人工智能时代实现两者的动态平衡。这种平衡当然不只是单纯消弭算法价值与哲学价值之间的张力，更在于将两者间的张力转化为人工智能时代发展的创造性势能。唯其如此，我们才能真正发挥人工智能技术的无穷潜能。. **主持人：当然，面对新兴智能技术，哲学研究也不能盲目转向，而要抽丝剥茧、聚焦真问题。请谈谈哲学思考的不可替代性体现在哪些方面？**. **赵立：**不同于人工智能技术背后的实证性与肯定性，哲学思考最为本质也最为核心的就是批判性反思。人工智能时代只是人类历史发展的一个阶段，虽然方兴未艾，但是各种问题已经或隐或显，我们在享受人工智能技术带来的诸多便利的同时，也必然遭遇甚至陷入技术引发的诸种困境。面对这些技术困境，我们只有基于哲学的批判性反思，全面观照人工智能技术的历史、当下与未来，才能用其利、避其害，走出一条通向未来的可行之途。但是，正像发展人工智能技术不能一哄而上一样，批判、反思人工智能技术同样不能一哄而上。哲学的批判性反思必须基于实践真知和理论思维，真正锚定需要回答的普遍性问题，进行真思考、找出真答案，在批判与反思中实现建构与超越，从而校准人工智能技术与哲学思考的发展方向。一言以蔽之，无论技术与时代发生何种变化，哲学总会以批判性反思的姿态在场并介入，也将在三者的动态关系中演化出新形态，迈向新阶段。. **魏犇群：**是的，哲学思考一定会与时代状况产生共振，但绝非盲目迎合时代潮流。哲学思考的不可替代性正是源于其本身的独特性。我认为主要体现在三个方面。第一，如果说人工智能擅长在既定的规则和框架下提供高效解答，哲学思考的特点则是提出深刻的问题。哲学思维不是在设定好的框架内运作，而是反思框架本身，揭示或质疑框架背后的预设；一旦发现自身对于那些预设的看法仍然受制于另一个更隐蔽的框架，哲学反思便会以那个更隐蔽的框架为对象，进而把握最深层的客观结构。这种自反性和批判性是哲学思考的灵魂，也是人工智能难以企及的能力。第二，如果说人工智能擅长探讨事实与功能，哲学思考则特别关注意义与价值。什么是正确的行动？人应该如何生活？人生的意义何在？这些古老的哲学问题追问的并不仅仅是如何高效达成既定目的，而是实质性的价值判断和意义建构。这些问题的解答必须依赖人类共同的经验、情感、担当和理性。在这一点上，人工智能无法取代哲学。第三，如果说人工智能擅长化繁为简，哲学思考则始终保持着对于复杂性和不可通约性的敏感。在普遍追求效率和最优解的今天，哲学常常提醒我们：世界和人的处境是复杂、矛盾、模糊甚至悲剧性的，有些问题没有确定的答案，有些价值之间存在内在冲突，有些体验无法还原为数据或模型。哲学思考的深刻有时候正体现在它愿意直面这些虽然不确定、不整齐、不可计算，也不可还原，却扎根于人类存在深处之物。这种深刻是人工智能难以把握到的。. **王田：**恩格斯认为：“对于已经从自然界和历史中被驱逐出去的哲学来说，要是还留下什么的话，那就只留下一个纯粹思想的领域：关于思维过程本身的规律的学说，即逻辑和辩证法。”哲学思考的不可替代性就在于，辩证思维方法既是科学思维方法的方法论前提，又能吸收、借鉴现代科学思维方法，并对其进行概括提升，从而不断开拓辩证法的思考空间。尽管AI基于海量数据训练能够展现出超越人类直觉的判断力，但其遵循的是数理逻辑，强调的是计算、因果和递归，以执行命令和完成任务为目的。这种思维模式既没有对外部世界的直观能力，也缺乏反思性和否定性等辩证思维的本质。哲学思考的目的并不是外在的，而是“认识你自己”，审视他人和他人的审视都无法替代这种反思性的自我审视。除此之外，哲学思考还能以思辨之力为改造世界的行动提供可能性条件：古希腊哲人对世界本原的探讨催生了早期科学与数学；近代启蒙思想家激发了人们对自由、平等与民主理念的追求；马克思主义揭示了人类社会的发展规律和未来走向，为“人类往何处去”提供了科学的路标指引。. **主持人：面向未来，我们有理由相信哲学的发展进步能助推人机协同的可持续发展。请谈谈如何发展人的哲学和人工智能的“哲学”？人工智能时代哲学研究思考的出发点和立足点是什么？**. **魏犇群：**在这里，“人工智能的哲学”有两个意思，一是关于人工智能的哲学，二是人工智能自己创造出来的“哲学”。按照第一个意思来理解，人工智能的哲学其核心任务是在人工智能不断模仿甚至超越人类智能的时代重新理解人的独特性，包括重新理解意识、理性、情感、道德、社会等。如此，人工智能的哲学将成为人类哲学的重要甚至主要组成部分。也就是说，人工智能的哲学属于人的哲学，其宗旨应该是以人文反思引导技术发展方向，其结果是人类在新技术的背景下加深对自身的理解。. **王田：**发展人工智能的“哲学”首先要确保技术创新由人类主导，而不是受资本逻辑和技术逻辑宰制。德国哲学家霍克海默曾指出，“对科学危机的理解依赖于一种有关当下社会境况的正确理论；因为作为社会功用的科学在当前反映着社会中的诸种矛盾”。我们不能离开运用人工智能的社会条件来谈论其正面和负面效应，人工智能产生副作用的主要原因在于应用它的外在环境不当。其次，在重新定位人与人工智能的关系时，不能放弃人类尺度，要确保人工智能为人类服务。最后，人工智能负载着伦理、文化、政治、经济等多重价值，这要求AI技术的发展符合人类价值观：既应当尊重人权、促进公平正义、服务于国家利益和人民福祉，也应当尊重文化差异、包容多样性文化，维护世界和平稳定和全球可持续发展。. **赵立：**以对世界的惊奇与追问为起点，哲学思考日益聚焦到认识人自身、关心人自身，人工智能时代的哲学研究更是要“基于人、围绕人、为了人”。进入现代社会以来，作为人类智慧产物的科学技术不断扩张其能力边界，人类在人工智能技术面前何以自处成为热门话题。作为人类的创造性产物，哲学思考有必要为人类运用科学技术“立法”，提供指引乃至牢牢掌控科学技术的发展方向与使用界限。. ##### 主管主办：全国哲学社会科学工作办公室 承办：人民网. ##### ©1999-2019 全国哲学社会科学工作办公室 版权所有 京ICP备12051030号-2.",
    "file_path": "unknown_source",
    "create_time": 1769010784,
    "update_time": 1769010784,
    "_id": "doc-56ba072dd517d40a45be43bb193491e8"
  }
}
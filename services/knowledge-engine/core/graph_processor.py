import os
import json
import networkx as nx

class GraphProcessor:
    """
    ä» LightRAG çš„è¾“å‡ºä¸­æå–å›¾è°±æ•°æ®å¹¶æ ‡æ³¨é¢†åŸŸ
    """
    
    def process_lightrag_output(
        self, 
        working_dir: str, 
        documents: list, 
        concept: str,
        chunk_mapping: dict
    ) -> dict:
        """
        è§£æ LightRAG è¾“å‡º,ç”Ÿæˆå‰ç«¯æ‰€éœ€çš„ JSON
        
        å‚æ•°:
        - working_dir: LightRAG å·¥ä½œç›®å½•
        - documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
        - concept: æ ¸å¿ƒæ¦‚å¿µ
        - chunk_mapping: {chunk_id: {doc_ids: [...], domains: [...]}}
        
        è¿”å›: {nodes: [...], edges: [...]}
        """
        graphml_path = os.path.join(working_dir, "graph_chunk_entity_relation.graphml")
        
        if not os.path.exists(graphml_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°å›¾è°±æ–‡ä»¶: {graphml_path}")
        
        # 1. åŠ è½½å®Œæ•´å›¾è°±
        G_full = nx.read_graphml(graphml_path)
        print(f"åŸå§‹å›¾è°±: {len(G_full.nodes())} èŠ‚ç‚¹, {len(G_full.edges())} è¾¹")
        
        # 2. ã€å‰ªæã€‘ä¿ç•™ä¸æ ¸å¿ƒæ¦‚å¿µè¿é€šçš„å­å›¾
        G_pruned = self._prune_graph(G_full, concept)
        print(f"å‰ªæå: {len(G_pruned.nodes())} èŠ‚ç‚¹, {len(G_pruned.edges())} è¾¹")
        
        # 3. è§£æèŠ‚ç‚¹ï¼ˆä½¿ç”¨å‰ªæåçš„å›¾ï¼‰
        nodes = self._extract_nodes(G_pruned, concept, chunk_mapping)
        
        # 4. è§£æè¾¹
        edges = self._extract_edges(G_pruned)
        
        return {
            "concept": concept,
            "nodes": nodes,
            "edges": edges,
            "total_nodes": len(nodes),
            "total_edges": len(edges)
        }
    
    def _prune_graph(self, G_full: nx.Graph, center_concept: str) -> nx.Graph:
        """
        å‰ªæå›¾è°±,ä¿ç•™ä¸æ ¸å¿ƒæ¦‚å¿µè¿é€šçš„é‡è¦å­å›¾
        
        ç­–ç•¥ :
        1. ä¿ç•™æ ¸å¿ƒæ¦‚å¿µèŠ‚ç‚¹åŠå…¶é‚»å±…
        2. ä¿ç•™é«˜åº¦æ•°èŠ‚ç‚¹ï¼ˆåº¦ > 1ï¼‰
        3. åªä¿ç•™ä¸æ ¸å¿ƒæ¦‚å¿µè¿é€šçš„èŠ‚ç‚¹
        
        å‚æ•°:
        - G_full: å®Œæ•´å›¾è°±
        - center_concept: æ ¸å¿ƒæ¦‚å¿µï¼ˆå¦‚ "ç†µ"ï¼‰
        
        è¿”å›: å‰ªæåçš„å­å›¾
        """
        # ========== é˜¶æ®µ 1: åˆæ­¥ç­›é€‰èŠ‚ç‚¹ ==========
        nodes_to_keep = set()
        
        # ç­–ç•¥ 1: ä¿ç•™æ ¸å¿ƒæ¦‚å¿µåŠå…¶ç›´æ¥é‚»å±…
        if center_concept in G_full.nodes():
            nodes_to_keep.add(center_concept)
            neighbors = list(G_full.neighbors(center_concept))
            nodes_to_keep.update(neighbors)
            print(f"æ ¸å¿ƒæ¦‚å¿µ '{center_concept}' + é‚»å±…: {len(neighbors) + 1} ä¸ª")
        else:
            print(f"æ ¸å¿ƒæ¦‚å¿µ '{center_concept}' ä¸åœ¨å›¾è°±ä¸­ï¼Œå°†ä¿ç•™æ‰€æœ‰é«˜åº¦æ•°èŠ‚ç‚¹")
        
        # ç­–ç•¥ 2: ä¿ç•™é«˜åº¦æ•°èŠ‚ç‚¹ï¼ˆåº¦ > 1ï¼‰
        high_degree = [n for n, d in G_full.degree() if d > 1]
        nodes_to_keep.update(high_degree)
        print(f"é«˜åº¦æ•°èŠ‚ç‚¹ (åº¦>1): {len(high_degree)} ä¸ª")
        
        # åˆæ­¥ç­›é€‰åçš„å­å›¾
        G_candidate = G_full.subgraph(nodes_to_keep).copy()
        
        # ========== é˜¶æ®µ 2: ã€æ ¸å¿ƒã€‘åªä¿ç•™ä¸ä¸­å¿ƒæ¦‚å¿µè¿é€šçš„èŠ‚ç‚¹ ==========
        if center_concept in G_candidate.nodes():
            # æ‰¾å‡ºä¸æ ¸å¿ƒæ¦‚å¿µåœ¨åŒä¸€è¿é€šåˆ†é‡ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
            connected_nodes = self._get_connected_component(G_candidate, center_concept)
            print(f"ğŸ”— ä¸ '{center_concept}' è¿é€šçš„èŠ‚ç‚¹: {len(connected_nodes)} ä¸ª")
            
            G_pruned = G_candidate.subgraph(connected_nodes).copy()
        else:
            # å¦‚æœæ ¸å¿ƒæ¦‚å¿µä¸å­˜åœ¨ï¼Œè¿”å›æœ€å¤§è¿é€šåˆ†é‡
            print(f"âš ï¸  ä½¿ç”¨æœ€å¤§è¿é€šåˆ†é‡")
            G_pruned = self._get_largest_component(G_candidate)
        
        # ========== å¯é€‰ï¼šè¿”å›å®Œæ•´å›¾è°± ==========
        # G_pruned = G_full.copy()
        
        return G_pruned
    
    def _get_connected_component(self, G: nx.Graph, node: str) -> set:
        """
        è·å–åŒ…å«æŒ‡å®šèŠ‚ç‚¹çš„è¿é€šåˆ†é‡
        
        å‚æ•°:
        - G: å›¾å¯¹è±¡
        - node: èŠ‚ç‚¹ ID
        
        è¿”å›: åŒ…å«è¯¥èŠ‚ç‚¹çš„è¿é€šåˆ†é‡ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        """
        # NetworkX æä¾›çš„è¿é€šåˆ†é‡æŸ¥æ‰¾
        for component in nx.connected_components(G):
            if node in component:
                return component
        
        return {node}  # å¦‚æœæœªæ‰¾åˆ°ï¼Œè‡³å°‘è¿”å›è‡ªå·±
    
    def _get_largest_component(self, G: nx.Graph) -> nx.Graph:
        """
        è·å–æœ€å¤§è¿é€šåˆ†é‡
        
        å‚æ•°:
        - G: å›¾å¯¹è±¡
        
        è¿”å›: æœ€å¤§è¿é€šåˆ†é‡çš„å­å›¾
        """
        if len(G.nodes()) == 0:
            return G
        
        # æ‰¾åˆ°æœ€å¤§çš„è¿é€šåˆ†é‡
        largest_cc = max(nx.connected_components(G), key=len)
        print(f"ğŸ“Š æœ€å¤§è¿é€šåˆ†é‡: {len(largest_cc)} èŠ‚ç‚¹")
        
        return G.subgraph(largest_cc).copy()
    
    def _extract_nodes(self, G: nx.Graph, concept: str, chunk_mapping: dict) -> list:
        """æå–èŠ‚ç‚¹ä¿¡æ¯"""
        nodes = []
        
        for node_id, node_data in G.nodes(data=True):
            # è§£æ source_id
            source_ids_raw = node_data.get('source_id', '')
            source_chunks = self._parse_source_ids(source_ids_raw)
            
            # åæŸ¥é¢†åŸŸï¼ˆæ”¯æŒå¤šé¢†åŸŸï¼‰
            domains = self._resolve_domains(source_chunks, chunk_mapping)
            
            # æ„å»ºèŠ‚ç‚¹
            node = {
                "id": node_id,
                "label": node_data.get('entity_name', node_id),
                "description": node_data.get('description', 'æš‚æ— æè¿°'),
                "domains": domains,
                "source_chunks": source_chunks,
                "size": G.degree(node_id) + 1  # å‰ç«¯å¯å‚è€ƒçš„èŠ‚ç‚¹å¤§å°ï¼Œ=deg+1
            }
            
            nodes.append(node)
        
        return nodes
    
    def _extract_edges(self, G: nx.Graph) -> list:
        """æå–è¾¹ä¿¡æ¯"""
        edges = []
        
        for source, target, edge_data in G.edges(data=True):
            edge = {
                "source": source,
                "target": target,
                "relation": edge_data.get('label', 'related'),
                "description": edge_data.get('description', '')
            }
            edges.append(edge)
        
        return edges
    
    def _parse_source_ids(self, source_ids_raw: str) -> list:
        """è§£æ source_id å­—ç¬¦ä¸²"""
        if not source_ids_raw:
            return []
        
        # å°è¯•å¤šç§åˆ†éš”ç¬¦
        separators = ['<SEP>', ',', '\n']
        chunk_ids = [source_ids_raw]
        
        for sep in separators:
            temp = []
            for part in chunk_ids:
                temp.extend([p.strip() for p in part.split(sep) if p.strip()])
            chunk_ids = temp
        
        return chunk_ids
    
    def _resolve_domains(self, chunk_ids: list, chunk_mapping: dict) -> list:
        """
        é€šè¿‡ Chunk ID åæŸ¥é¢†åŸŸï¼ˆæ”¯æŒå¤šé¢†åŸŸï¼‰
        
        è¿”å›: ['ç‰©ç†å­¦', 'ä¿¡æ¯è®º'] æˆ– ['è·¨å­¦ç§‘']
        """
        all_domains = set()
        
        for cid in chunk_ids:
            mapping_info = chunk_mapping.get(cid)
            if mapping_info:
                # æ·»åŠ è¯¥ chunk å…³è”çš„æ‰€æœ‰é¢†åŸŸ
                all_domains.update(mapping_info['domains'])
        
        domains_list = sorted(list(all_domains))  # æ’åºä¿è¯ä¸€è‡´æ€§
        
        # å¦‚æœè·¨è¶Šå¤šä¸ªé¢†åŸŸ,è¿”å›æ‰€æœ‰é¢†åŸŸï¼ˆå‰ç«¯å¯æ ¹æ®é•¿åº¦åˆ¤æ–­æ˜¯å¦è·¨å­¦ç§‘ï¼‰
        if len(domains_list) > 1:
            return domains_list  # å¦‚ ["ä¿¡æ¯è®º", "ç‰©ç†å­¦"]
        elif len(domains_list) == 1:
            return domains_list
        else:
            return ["æœªçŸ¥"]

graph_processor = GraphProcessor()